{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Engines Module Documentation","text":"<p>The <code>engines</code> module is a comprehensive framework designed to implement various machine learning algorithms and techniques for recommendation systems. It includes several submodules, each focusing on different aspects of recommendation and filtering.</p>"},{"location":"#submodules","title":"Submodules","text":"<ul> <li>contentFilterEngine: A versatile engine for content-based filtering, incorporating traditional machine learning algorithms, neural network models, and hybrid approaches.</li> <li>unionizedFilterEngine: Focuses on collaborative filtering techniques, including matrix factorization and neural network-based methods.</li> <li>hybrid: Combines multiple recommendation strategies to enhance accuracy and robustness.</li> <li>content_based: Implements content-based filtering techniques, leveraging item features for recommendations.</li> </ul>"},{"location":"#usage","title":"Usage","text":"<p>The <code>engines</code> module is designed to be modular and extensible, allowing developers to easily integrate and experiment with different algorithms and techniques. Each submodule can be imported and utilized independently, providing flexibility in building custom recommendation solutions.</p>"},{"location":"#example","title":"Example","text":"<pre><code>from engines.contentFilterEngine.traditional_ml_algorithms import TRA_LR\nmodel = TRA_LR()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n</code></pre>"},{"location":"#notes","title":"Notes","text":"<ul> <li>Ensure that all dependencies are installed and configured correctly before using the module.</li> <li>The module is intended for educational and research purposes and may require further optimization for production use.</li> </ul>"},{"location":"#project-layout","title":"Project Layout","text":"<pre><code>engines/\n    contentFilterEngine/\n    unionizedFilterEngine/\n    hybrid/\n    content_based/\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images, and other files.\n</code></pre> <p>For more detailed documentation on each submodule, refer to the respective markdown files in the <code>docs</code> directory.</p>"},{"location":"contentFilterEngine/__init__/","title":"Content Filter Engine","text":"<p>contentFilterEngine</p> <p>The <code>contentFilterEngine</code> module is a comprehensive framework designed to implement various machine learning algorithms and techniques for content filtering and recommendation systems. It encompasses a wide range of approaches, including traditional machine learning algorithms, neural network-based models, probabilistic statistical methods, and hybrid ensemble methods.</p> <p>Structure: - <code>traditional_ml_algorithms</code>: Implements classic machine learning algorithms such as Logistic Regression, SVM, Decision Trees, and more. - <code>nn_based_algorithms</code>: Contains neural network-based models like CNN, RNN, and Transformer, among others. - <code>probabilistic_statistical_methods</code>: Includes methods like LDA and LSA for statistical analysis and modeling. - <code>embedding_representation_learning</code>: Focuses on techniques like Word2Vec and Doc2Vec for learning vector representations of data. - <code>performance_scalability</code>: Addresses issues related to feature extraction and scalable algorithms. - <code>fairness_explainability</code>: Provides tools for ensuring fairness and explainability in machine learning models. - <code>learning_paradigms</code>: Explores advanced learning paradigms such as meta-learning, transfer learning, and zero-shot learning. - <code>other_approaches</code>: Covers additional methods like sentiment analysis and rule-based systems.</p> <p>Usage: The <code>contentFilterEngine</code> is designed to be modular and extensible, allowing developers to easily integrate and experiment with different algorithms and techniques. Each submodule can be imported and utilized independently, providing flexibility in building custom content filtering solutions.</p> Example <p>from engines.contentFilterEngine.traditional_ml_algorithms import TRA_LR model = TRA_LR() model.fit(X_train, y_train) predictions = model.predict(X_test)</p> <p>Note: - Ensure that all dependencies are installed and configured correctly before using the module. - The module is intended for educational and research purposes and may require further optimization for production use.</p>"},{"location":"contentFilterEngine/context_personalization/context_aware/","title":"Context Aware","text":""},{"location":"contentFilterEngine/context_personalization/context_aware/#engines.contentFilterEngine.context_personalization.context_aware.ContextAwareRecommender","title":"<code>ContextAwareRecommender</code>","text":"<p>A context-aware recommender system that adapts recommendations based on contextual factors.</p> <p>This recommender system incorporates contextual information to provide more relevant  recommendations by adjusting feature weights based on the current context. It considers various contextual factors such as time, location, user state, and other environmental  variables to modify the importance of different item features.</p> <p>Attributes:</p> Name Type Description <code>context_factors</code> <code>Dict[str, Dict]</code> <p>Mapping of context factors to their value-specific weights. Format: {     \"factor_name\": {         \"value\": {             \"feature\": weight         }     } }</p> <code>item_features</code> <code>Dict[int, Dict]</code> <p>Mapping of item IDs to their feature dictionaries. Format: {     item_id: {         \"feature_name\": value     } }</p> <code>feature_weights</code> <code>Dict[str, float]</code> <p>Current active feature weights based on context.</p> <p>Methods:</p> Name Description <code>update_context</code> <p>Updates the current context and recalculates feature weights.</p> <code>recommend</code> <p>Generates recommendations considering the current context.</p> <code>_initialize_feature_weights</code> <p>Initializes weights based on context.</p> <code>_encode_item_features</code> <p>Encodes and weights item features.</p> Source code in <code>engines/contentFilterEngine/context_personalization/context_aware.py</code> <pre><code>class ContextAwareRecommender:\n    \"\"\"\n    A context-aware recommender system that adapts recommendations based on contextual factors.\n\n    This recommender system incorporates contextual information to provide more relevant \n    recommendations by adjusting feature weights based on the current context. It considers\n    various contextual factors such as time, location, user state, and other environmental \n    variables to modify the importance of different item features.\n\n    Attributes:\n        context_factors (Dict[str, Dict]): Mapping of context factors to their value-specific weights.\n            Format: {\n                \"factor_name\": {\n                    \"value\": {\n                        \"feature\": weight\n                    }\n                }\n            }\n        item_features (Dict[int, Dict]): Mapping of item IDs to their feature dictionaries.\n            Format: {\n                item_id: {\n                    \"feature_name\": value\n                }\n            }\n        feature_weights (Dict[str, float]): Current active feature weights based on context.\n\n    Methods:\n        update_context: Updates the current context and recalculates feature weights.\n        recommend: Generates recommendations considering the current context.\n        _initialize_feature_weights: Initializes weights based on context.\n        _encode_item_features: Encodes and weights item features.\n    \"\"\"\n\n    def __init__(\n        self, \n        context_config_path: str, \n        item_features: Dict[int, Dict[str, Any]]\n    ):\n        \"\"\"\n        Initialize the context-aware recommender with a configuration file for context factors and item features.\n\n        Parameters:\n        - context_config_path (str): Path to the JSON configuration file for context factors and weights.\n        - item_features (dict): A dictionary of item features.\n        \"\"\"\n        self.context_factors = self._load_context_config(context_config_path)\n        self.item_features = item_features\n        self.user_profiles = defaultdict(lambda: defaultdict(float))\n        self.feature_weights = self._initialize_feature_weights()\n\n    def _load_context_config(self, config_path: str) -&gt; Dict[str, Any]:\n        \"\"\"\n        Load context factors and their configurations from a JSON file.\n\n        Parameters:\n        - config_path (str): Path to the JSON configuration file.\n\n        Returns:\n        - Dict[str, Any]: Configuration for context factors.\n        \"\"\"\n        if not os.path.exists(config_path):\n            raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n        with open(config_path, 'r') as file:\n            config = json.load(file)\n        return config\n\n    def _initialize_feature_weights(self, current_context: Optional[Dict[str, Any]] = None) -&gt; Dict[str, float]:\n        \"\"\"\n        Initialize and calculate feature weights based on the current context.\n\n        This method processes the current context to determine appropriate weights for different\n        item features. It combines weights from multiple context factors when they affect the\n        same feature.\n\n        Args:\n            current_context (Optional[Dict[str, Any]]): Dictionary containing current context\n                information. Keys are context factor names, values are the current factor values.\n                If None, uses default context.\n\n        Returns:\n            Dict[str, float]: Dictionary mapping feature names to their calculated weights\n                based on the current context.\n\n        Example:\n            &gt;&gt;&gt; context = {\"time\": \"evening\", \"location\": \"home\"}\n            &gt;&gt;&gt; recommender._initialize_feature_weights(context)\n            {\"genre_action\": 1.2, \"genre_drama\": 0.8, \"length\": 1.5}\n        \"\"\"\n        weights = {}\n        context = current_context or self.context_factors.get(\"default\", {})\n        for factor, value in context.items():\n            factor_config = self.context_factors.get(factor, {})\n            value_weights = factor_config.get(str(value), {})\n            for feature, weight in value_weights.items():\n                weights[feature] = weight\n        return weights\n\n    def _encode_item_features(self, item_id: int) -&gt; Dict[str, float]:\n        \"\"\"\n        Encode item features into a weighted feature vector based on current context.\n\n        This method transforms an item's raw features into a weighted feature representation,\n        applying the current context-dependent weights. It handles both categorical and\n        numerical features appropriately.\n\n        Args:\n            item_id (int): The unique identifier of the item to encode.\n\n        Returns:\n            Dict[str, float]: Dictionary containing the encoded and weighted features.\n                For categorical features: {feature_name_value: weight}\n                For numerical features: {feature_name: value * weight}\n\n        Example:\n            &gt;&gt;&gt; recommender._encode_item_features(123)\n            {\"genre_action\": 1.2, \"duration\": 90.5, \"rating\": 4.5}\n\n        Note:\n            - Categorical features are encoded as separate binary features\n            - Numerical features are scaled by their corresponding weights\n        \"\"\"\n        features = self.item_features.get(item_id, {})\n        encoded = {}\n        for feature, value in features.items():\n            key = f\"{feature}_{value}\" if isinstance(value, str) else feature\n            weight = self.feature_weights.get(key, 1.0)\n            if isinstance(value, str):\n                encoded[key] = weight\n            else:\n                encoded[feature] = value * weight\n        return encoded\n\n    def fit(self, data: Dict[int, List[int]]):\n        \"\"\"\n        Train the recommender system by building user profiles based on their interactions.\n\n        Parameters:\n        - data (dict): The data used for training the model, containing user interactions.\n        \"\"\"\n        for user_id, items in data.items():\n            for item_id in items:\n                encoded_features = self._encode_item_features(item_id)\n                for feature, value in encoded_features.items():\n                    self.user_profiles[user_id][feature] += value\n\n    def recommend(\n        self, \n        user_id: int, \n        context: Optional[Dict[str, Any]] = None, \n        top_n: int = 10\n    ) -&gt; List[int]:\n        \"\"\"\n        Generate top-N item recommendations for a given user considering context.\n\n        Parameters:\n        - user_id (int): The ID of the user.\n        - context (dict, optional): The current context to consider. If provided, updates context factors.\n        - top_n (int): The number of recommendations to generate.\n\n        Returns:\n        - List[int]: List of recommended item IDs.\n        \"\"\"\n        if context:\n            self.feature_weights = self._initialize_feature_weights(context)\n        else:\n            self.feature_weights = self._initialize_feature_weights()\n\n        user_profile = self.user_profiles.get(user_id, {})\n        if not user_profile:\n            return []\n\n        scores = {}\n        interacted_items = set()\n        # Collect all items the user has interacted with to exclude them from recommendations\n        interacted_items = user_profile.get('interacted_items', set())\n\n        for item_id, features in self.item_features.items():\n            if item_id in interacted_items:\n                continue\n            encoded_features = self._encode_item_features(item_id)\n            score = 0.0\n            for feature, value in encoded_features.items():\n                score += user_profile.get(feature, 0.0) * value\n            scores[item_id] = score\n\n        # Sort items based on the computed scores in descending order\n        ranked_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n\n        # Return the top-N item IDs\n        return [item_id for item_id, score in ranked_items[:top_n]]\n</code></pre>"},{"location":"contentFilterEngine/context_personalization/context_aware/#engines.contentFilterEngine.context_personalization.context_aware.ContextAwareRecommender.__init__","title":"<code>__init__(context_config_path, item_features)</code>","text":"<p>Initialize the context-aware recommender with a configuration file for context factors and item features.</p> <p>Parameters: - context_config_path (str): Path to the JSON configuration file for context factors and weights. - item_features (dict): A dictionary of item features.</p> Source code in <code>engines/contentFilterEngine/context_personalization/context_aware.py</code> <pre><code>def __init__(\n    self, \n    context_config_path: str, \n    item_features: Dict[int, Dict[str, Any]]\n):\n    \"\"\"\n    Initialize the context-aware recommender with a configuration file for context factors and item features.\n\n    Parameters:\n    - context_config_path (str): Path to the JSON configuration file for context factors and weights.\n    - item_features (dict): A dictionary of item features.\n    \"\"\"\n    self.context_factors = self._load_context_config(context_config_path)\n    self.item_features = item_features\n    self.user_profiles = defaultdict(lambda: defaultdict(float))\n    self.feature_weights = self._initialize_feature_weights()\n</code></pre>"},{"location":"contentFilterEngine/context_personalization/context_aware/#engines.contentFilterEngine.context_personalization.context_aware.ContextAwareRecommender.fit","title":"<code>fit(data)</code>","text":"<p>Train the recommender system by building user profiles based on their interactions.</p> <p>Parameters: - data (dict): The data used for training the model, containing user interactions.</p> Source code in <code>engines/contentFilterEngine/context_personalization/context_aware.py</code> <pre><code>def fit(self, data: Dict[int, List[int]]):\n    \"\"\"\n    Train the recommender system by building user profiles based on their interactions.\n\n    Parameters:\n    - data (dict): The data used for training the model, containing user interactions.\n    \"\"\"\n    for user_id, items in data.items():\n        for item_id in items:\n            encoded_features = self._encode_item_features(item_id)\n            for feature, value in encoded_features.items():\n                self.user_profiles[user_id][feature] += value\n</code></pre>"},{"location":"contentFilterEngine/context_personalization/context_aware/#engines.contentFilterEngine.context_personalization.context_aware.ContextAwareRecommender.recommend","title":"<code>recommend(user_id, context=None, top_n=10)</code>","text":"<p>Generate top-N item recommendations for a given user considering context.</p> <p>Parameters: - user_id (int): The ID of the user. - context (dict, optional): The current context to consider. If provided, updates context factors. - top_n (int): The number of recommendations to generate.</p> <p>Returns: - List[int]: List of recommended item IDs.</p> Source code in <code>engines/contentFilterEngine/context_personalization/context_aware.py</code> <pre><code>def recommend(\n    self, \n    user_id: int, \n    context: Optional[Dict[str, Any]] = None, \n    top_n: int = 10\n) -&gt; List[int]:\n    \"\"\"\n    Generate top-N item recommendations for a given user considering context.\n\n    Parameters:\n    - user_id (int): The ID of the user.\n    - context (dict, optional): The current context to consider. If provided, updates context factors.\n    - top_n (int): The number of recommendations to generate.\n\n    Returns:\n    - List[int]: List of recommended item IDs.\n    \"\"\"\n    if context:\n        self.feature_weights = self._initialize_feature_weights(context)\n    else:\n        self.feature_weights = self._initialize_feature_weights()\n\n    user_profile = self.user_profiles.get(user_id, {})\n    if not user_profile:\n        return []\n\n    scores = {}\n    interacted_items = set()\n    # Collect all items the user has interacted with to exclude them from recommendations\n    interacted_items = user_profile.get('interacted_items', set())\n\n    for item_id, features in self.item_features.items():\n        if item_id in interacted_items:\n            continue\n        encoded_features = self._encode_item_features(item_id)\n        score = 0.0\n        for feature, value in encoded_features.items():\n            score += user_profile.get(feature, 0.0) * value\n        scores[item_id] = score\n\n    # Sort items based on the computed scores in descending order\n    ranked_items = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n\n    # Return the top-N item IDs\n    return [item_id for item_id, score in ranked_items[:top_n]]\n</code></pre>"},{"location":"contentFilterEngine/context_personalization/item_profiling/","title":"Item Profiling","text":""},{"location":"contentFilterEngine/context_personalization/item_profiling/#engines.contentFilterEngine.context_personalization.item_profiling.ItemProfilingRecommender","title":"<code>ItemProfilingRecommender</code>","text":"<p>A recommender system that builds and maintains detailed item profiles for context-aware recommendations.</p> <p>This recommender system creates comprehensive profiles for items by analyzing their features, usage patterns, and performance across different contexts. It uses these profiles to make more accurate recommendations based on contextual similarities.</p> <p>Attributes:</p> Name Type Description <code>item_profiles</code> <code>Dict[int, Dict]</code> <p>Detailed profiles for each item containing: - Static features (inherent item characteristics) - Dynamic features (usage patterns, ratings) - Contextual performance metrics</p> <code>context_performance</code> <code>Dict[int, Dict]</code> <p>Performance metrics for items in different contexts</p> <code>feature_importance</code> <code>Dict[str, float]</code> <p>Learned importance of different features</p> <p>Methods:</p> Name Description <code>build_profile</code> <p>Creates or updates an item's profile</p> <code>update_context_performance</code> <p>Updates item performance metrics for specific contexts</p> <code>get_context_similarity</code> <p>Calculates similarity between contexts</p> <code>recommend</code> <p>Generates recommendations using item profiles and current context</p> Example <p>profiler = ItemProfilingRecommender() profiler.build_profile(item_id=123, features={...}, context_data={...}) recommendations = profiler.recommend(user_id=456, context={\"time\": \"evening\"})</p> Source code in <code>engines/contentFilterEngine/context_personalization/item_profiling.py</code> <pre><code>class ItemProfilingRecommender:\n    \"\"\"\n    A recommender system that builds and maintains detailed item profiles for context-aware recommendations.\n\n    This recommender system creates comprehensive profiles for items by analyzing their features,\n    usage patterns, and performance across different contexts. It uses these profiles to make\n    more accurate recommendations based on contextual similarities.\n\n    Attributes:\n        item_profiles (Dict[int, Dict]): Detailed profiles for each item containing:\n            - Static features (inherent item characteristics)\n            - Dynamic features (usage patterns, ratings)\n            - Contextual performance metrics\n        context_performance (Dict[int, Dict]): Performance metrics for items in different contexts\n        feature_importance (Dict[str, float]): Learned importance of different features\n\n    Methods:\n        build_profile: Creates or updates an item's profile\n        update_context_performance: Updates item performance metrics for specific contexts\n        get_context_similarity: Calculates similarity between contexts\n        recommend: Generates recommendations using item profiles and current context\n\n    Example:\n        &gt;&gt;&gt; profiler = ItemProfilingRecommender()\n        &gt;&gt;&gt; profiler.build_profile(item_id=123, features={...}, context_data={...})\n        &gt;&gt;&gt; recommendations = profiler.recommend(user_id=456, context={\"time\": \"evening\"})\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the item profiling recommender.\n        \"\"\"\n        self.item_profiles: Dict[int, Dict[str, Any]] = {}\n\n    def fit(self, data: Dict[int, List[int]], item_features: Dict[int, Dict[str, Any]]):\n        \"\"\"\n        Train the recommender system by building item profiles.\n\n        Parameters:\n        - data (dict): The data used for training the model, containing user interactions.\n        - item_features (dict): A dictionary mapping item IDs to their features.\n        \"\"\"\n        # Example: Count-based item profiling\n        for user_id, items in data.items():\n            for item_id in items:\n                if item_id not in self.item_profiles:\n                    self.item_profiles[item_id] = {}\n                for feature, value in item_features.get(item_id, {}).items():\n                    self.item_profiles[item_id][feature] = self.item_profiles[item_id].get(feature, 0) + 1\n\n    def recommend(self, query: str, top_n: int = 10) -&gt; List[int]:\n        \"\"\"\n        Recommend items based on the similarity of the query to the documents.\n\n        Parameters:\n        - query (str): The query text for which to generate recommendations.\n        - top_n (int): Number of top recommendations to return.\n\n        Returns:\n        - List[int]: List of recommended item indices.\n        \"\"\"\n        logger.info(\"Generating recommendations using LSA.\")\n        query_vec = self.transform([query])\n        doc_vecs = self.lsa_model.transform(self.vectorizer.transform(self.vectorizer.get_feature_names_out()))\n        similarity_scores = (doc_vecs @ query_vec.T).flatten()\n        top_indices = similarity_scores.argsort()[::-1][:top_n]\n        logger.info(f\"Top {top_n} recommendations generated using LSA.\")\n        return top_indices.tolist()\n</code></pre>"},{"location":"contentFilterEngine/context_personalization/item_profiling/#engines.contentFilterEngine.context_personalization.item_profiling.ItemProfilingRecommender.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the item profiling recommender.</p> Source code in <code>engines/contentFilterEngine/context_personalization/item_profiling.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the item profiling recommender.\n    \"\"\"\n    self.item_profiles: Dict[int, Dict[str, Any]] = {}\n</code></pre>"},{"location":"contentFilterEngine/context_personalization/item_profiling/#engines.contentFilterEngine.context_personalization.item_profiling.ItemProfilingRecommender.fit","title":"<code>fit(data, item_features)</code>","text":"<p>Train the recommender system by building item profiles.</p> <p>Parameters: - data (dict): The data used for training the model, containing user interactions. - item_features (dict): A dictionary mapping item IDs to their features.</p> Source code in <code>engines/contentFilterEngine/context_personalization/item_profiling.py</code> <pre><code>def fit(self, data: Dict[int, List[int]], item_features: Dict[int, Dict[str, Any]]):\n    \"\"\"\n    Train the recommender system by building item profiles.\n\n    Parameters:\n    - data (dict): The data used for training the model, containing user interactions.\n    - item_features (dict): A dictionary mapping item IDs to their features.\n    \"\"\"\n    # Example: Count-based item profiling\n    for user_id, items in data.items():\n        for item_id in items:\n            if item_id not in self.item_profiles:\n                self.item_profiles[item_id] = {}\n            for feature, value in item_features.get(item_id, {}).items():\n                self.item_profiles[item_id][feature] = self.item_profiles[item_id].get(feature, 0) + 1\n</code></pre>"},{"location":"contentFilterEngine/context_personalization/item_profiling/#engines.contentFilterEngine.context_personalization.item_profiling.ItemProfilingRecommender.recommend","title":"<code>recommend(query, top_n=10)</code>","text":"<p>Recommend items based on the similarity of the query to the documents.</p> <p>Parameters: - query (str): The query text for which to generate recommendations. - top_n (int): Number of top recommendations to return.</p> <p>Returns: - List[int]: List of recommended item indices.</p> Source code in <code>engines/contentFilterEngine/context_personalization/item_profiling.py</code> <pre><code>def recommend(self, query: str, top_n: int = 10) -&gt; List[int]:\n    \"\"\"\n    Recommend items based on the similarity of the query to the documents.\n\n    Parameters:\n    - query (str): The query text for which to generate recommendations.\n    - top_n (int): Number of top recommendations to return.\n\n    Returns:\n    - List[int]: List of recommended item indices.\n    \"\"\"\n    logger.info(\"Generating recommendations using LSA.\")\n    query_vec = self.transform([query])\n    doc_vecs = self.lsa_model.transform(self.vectorizer.transform(self.vectorizer.get_feature_names_out()))\n    similarity_scores = (doc_vecs @ query_vec.T).flatten()\n    top_indices = similarity_scores.argsort()[::-1][:top_n]\n    logger.info(f\"Top {top_n} recommendations generated using LSA.\")\n    return top_indices.tolist()\n</code></pre>"},{"location":"contentFilterEngine/context_personalization/user_profiling/","title":"User Profiling","text":""},{"location":"contentFilterEngine/context_personalization/user_profiling/#engines.contentFilterEngine.context_personalization.user_profiling.UserProfilingRecommender","title":"<code>UserProfilingRecommender</code>","text":"<p>A context-aware recommender system that builds and maintains user profiles for personalized recommendations.</p> <p>This recommender system creates and maintains detailed user profiles that capture user preferences, behavior patterns, and context-dependent interactions. It uses these profiles to generate personalized recommendations that adapt to different contexts.</p> <p>Attributes:</p> Name Type Description <code>user_profiles</code> <code>Dict[int, Dict]</code> <p>Detailed profiles for each user containing: - Preference vectors - Historical interactions - Context-dependent behavior patterns - Long-term and short-term interests</p> <code>context_preferences</code> <code>Dict[int, Dict]</code> <p>User preferences mapped to different contexts</p> <code>preference_evolution</code> <code>Dict[int, List]</code> <p>Temporal evolution of user preferences</p> <p>Methods:</p> Name Description <code>update_profile</code> <p>Updates user profile with new interaction data</p> <code>analyze_context_preferences</code> <p>Analyzes user preferences in different contexts</p> <code>detect_preference_shifts</code> <p>Identifies changes in user preferences over time</p> <code>recommend</code> <p>Generates personalized recommendations based on profile and context</p> Example <p>profiler = UserProfilingRecommender() profiler.update_profile(user_id=123, interaction_data={...}, context={...}) recommendations = profiler.recommend(user_id=123, context={\"location\": \"work\"})</p> Note <ul> <li>Profiles are automatically updated with each user interaction</li> <li>Supports both explicit (ratings) and implicit (behavior) feedback</li> <li>Implements drift detection for evolving user preferences</li> </ul> Source code in <code>engines/contentFilterEngine/context_personalization/user_profiling.py</code> <pre><code>class UserProfilingRecommender:\n    \"\"\"\n    A context-aware recommender system that builds and maintains user profiles for personalized recommendations.\n\n    This recommender system creates and maintains detailed user profiles that capture user preferences,\n    behavior patterns, and context-dependent interactions. It uses these profiles to generate\n    personalized recommendations that adapt to different contexts.\n\n    Attributes:\n        user_profiles (Dict[int, Dict]): Detailed profiles for each user containing:\n            - Preference vectors\n            - Historical interactions\n            - Context-dependent behavior patterns\n            - Long-term and short-term interests\n        context_preferences (Dict[int, Dict]): User preferences mapped to different contexts\n        preference_evolution (Dict[int, List]): Temporal evolution of user preferences\n\n    Methods:\n        update_profile: Updates user profile with new interaction data\n        analyze_context_preferences: Analyzes user preferences in different contexts\n        detect_preference_shifts: Identifies changes in user preferences over time\n        recommend: Generates personalized recommendations based on profile and context\n\n    Example:\n        &gt;&gt;&gt; profiler = UserProfilingRecommender()\n        &gt;&gt;&gt; profiler.update_profile(user_id=123, interaction_data={...}, context={...})\n        &gt;&gt;&gt; recommendations = profiler.recommend(user_id=123, context={\"location\": \"work\"})\n\n    Note:\n        - Profiles are automatically updated with each user interaction\n        - Supports both explicit (ratings) and implicit (behavior) feedback\n        - Implements drift detection for evolving user preferences\n    \"\"\"\n    def __init__(self, user_attributes: Optional[pd.DataFrame] = None):\n        \"\"\"\n        Initialize the user profiling recommender with optional user attributes.\n\n        Parameters:\n        - user_attributes (pd.DataFrame, optional): DataFrame containing user information.\n        \"\"\"\n        self.user_profiles: Dict[int, Dict[str, Any]] = {}\n        self.user_attributes = user_attributes\n\n    def fit(self, user_interactions: Dict[int, List[int]]):\n        \"\"\"\n        Build user profiles based on interactions and user attributes.\n\n        Parameters:\n        - user_interactions (dict): Dictionary mapping user IDs to lists of interacted item IDs.\n        \"\"\"\n        for user_id, items in user_interactions.items():\n            profile = {}\n\n            # Incorporate user attributes if available\n            if self.user_attributes is not None:\n                user_info = self.user_attributes[self.user_attributes['user_id'] == user_id]\n                if not user_info.empty:\n                    user_info = user_info.iloc[0]\n                    for col in self.user_attributes.columns:\n                        if col != 'user_id':\n                            profile[col] = user_info[col]\n\n            # Add interacted items\n            profile['interacted_items'] = set(items)\n            self.user_profiles[user_id] = profile\n\n    def recommend(self, user_id: int, all_items: set, top_n: int = 10) -&gt; List[int]:\n        \"\"\"\n        Generate top-N item recommendations for a given user based on their profile.\n\n        Parameters:\n        - user_id (int): The ID of the user.\n        - all_items (set): Set of all available item IDs.\n        - top_n (int): The number of recommendations to generate.\n\n        Returns:\n        - List[int]: List of recommended item IDs.\n        \"\"\"\n        if user_id not in self.user_profiles:\n            return []\n\n        interacted_items = self.user_profiles[user_id].get('interacted_items', set())\n        recommendations = list(all_items - interacted_items)\n        return recommendations[:top_n]\n</code></pre>"},{"location":"contentFilterEngine/context_personalization/user_profiling/#engines.contentFilterEngine.context_personalization.user_profiling.UserProfilingRecommender.__init__","title":"<code>__init__(user_attributes=None)</code>","text":"<p>Initialize the user profiling recommender with optional user attributes.</p> <p>Parameters: - user_attributes (pd.DataFrame, optional): DataFrame containing user information.</p> Source code in <code>engines/contentFilterEngine/context_personalization/user_profiling.py</code> <pre><code>def __init__(self, user_attributes: Optional[pd.DataFrame] = None):\n    \"\"\"\n    Initialize the user profiling recommender with optional user attributes.\n\n    Parameters:\n    - user_attributes (pd.DataFrame, optional): DataFrame containing user information.\n    \"\"\"\n    self.user_profiles: Dict[int, Dict[str, Any]] = {}\n    self.user_attributes = user_attributes\n</code></pre>"},{"location":"contentFilterEngine/context_personalization/user_profiling/#engines.contentFilterEngine.context_personalization.user_profiling.UserProfilingRecommender.fit","title":"<code>fit(user_interactions)</code>","text":"<p>Build user profiles based on interactions and user attributes.</p> <p>Parameters: - user_interactions (dict): Dictionary mapping user IDs to lists of interacted item IDs.</p> Source code in <code>engines/contentFilterEngine/context_personalization/user_profiling.py</code> <pre><code>def fit(self, user_interactions: Dict[int, List[int]]):\n    \"\"\"\n    Build user profiles based on interactions and user attributes.\n\n    Parameters:\n    - user_interactions (dict): Dictionary mapping user IDs to lists of interacted item IDs.\n    \"\"\"\n    for user_id, items in user_interactions.items():\n        profile = {}\n\n        # Incorporate user attributes if available\n        if self.user_attributes is not None:\n            user_info = self.user_attributes[self.user_attributes['user_id'] == user_id]\n            if not user_info.empty:\n                user_info = user_info.iloc[0]\n                for col in self.user_attributes.columns:\n                    if col != 'user_id':\n                        profile[col] = user_info[col]\n\n        # Add interacted items\n        profile['interacted_items'] = set(items)\n        self.user_profiles[user_id] = profile\n</code></pre>"},{"location":"contentFilterEngine/context_personalization/user_profiling/#engines.contentFilterEngine.context_personalization.user_profiling.UserProfilingRecommender.recommend","title":"<code>recommend(user_id, all_items, top_n=10)</code>","text":"<p>Generate top-N item recommendations for a given user based on their profile.</p> <p>Parameters: - user_id (int): The ID of the user. - all_items (set): Set of all available item IDs. - top_n (int): The number of recommendations to generate.</p> <p>Returns: - List[int]: List of recommended item IDs.</p> Source code in <code>engines/contentFilterEngine/context_personalization/user_profiling.py</code> <pre><code>def recommend(self, user_id: int, all_items: set, top_n: int = 10) -&gt; List[int]:\n    \"\"\"\n    Generate top-N item recommendations for a given user based on their profile.\n\n    Parameters:\n    - user_id (int): The ID of the user.\n    - all_items (set): Set of all available item IDs.\n    - top_n (int): The number of recommendations to generate.\n\n    Returns:\n    - List[int]: List of recommended item IDs.\n    \"\"\"\n    if user_id not in self.user_profiles:\n        return []\n\n    interacted_items = self.user_profiles[user_id].get('interacted_items', set())\n    recommendations = list(all_items - interacted_items)\n    return recommendations[:top_n]\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/doc2vec/","title":"doc2vec","text":""},{"location":"contentFilterEngine/embedding_representation_learning/doc2vec/#engines.contentFilterEngine.embedding_representation_learning.doc2vec.DOC2VEC","title":"<code>DOC2VEC</code>","text":"<p>A Doc2Vec model implementation for generating document embeddings.</p> <p>This class provides methods for training document embeddings, retrieving vectors, and managing model persistence. It's particularly useful for recommendation systems that need to understand document-level semantics.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Doc2Vec</code> <p>The underlying Gensim Doc2Vec model instance</p> <p>Methods:</p> Name Description <code>train</code> <p>Trains the Doc2Vec model on a corpus of documents</p> <code>get_embedding</code> <p>Retrieves the embedding vector for a specific document</p> <code>save_model</code> <p>Persists the trained model to disk</p> <code>load_model</code> <p>Loads a previously trained model from disk</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/doc2vec.py</code> <pre><code>class DOC2VEC:\n    \"\"\"\n    A Doc2Vec model implementation for generating document embeddings.\n\n    This class provides methods for training document embeddings, retrieving vectors,\n    and managing model persistence. It's particularly useful for recommendation systems\n    that need to understand document-level semantics.\n\n    Attributes:\n        model (Doc2Vec): The underlying Gensim Doc2Vec model instance\n\n    Methods:\n        train: Trains the Doc2Vec model on a corpus of documents\n        get_embedding: Retrieves the embedding vector for a specific document\n        save_model: Persists the trained model to disk\n        load_model: Loads a previously trained model from disk\n    \"\"\"\n\n    def __init__(self, vector_size: int = 100, window: int = 5, min_count: int = 1, \n                 workers: int = 4, epochs: int = 10):\n        \"\"\"\n        Initialize a new Doc2Vec model with specified parameters.\n\n        Args:\n            vector_size (int): Dimensionality of the feature vectors. Higher dimensions can capture\n                             more complex patterns but require more data and computation.\n            window (int): Maximum distance between the current and predicted word within a sentence.\n                         Larger windows capture broader context but may introduce noise.\n            min_count (int): Ignores all words with total frequency lower than this value.\n                           Helps reduce noise from rare words.\n            workers (int): Number of worker threads for training parallelization.\n                         More workers can speed up training on multicore systems.\n            epochs (int): Number of iterations over the corpus during training.\n                         More epochs can improve quality but increase training time.\n\n        Note:\n            The model is not trained upon initialization. Call train() with your corpus\n            to begin training.\n        \"\"\"\n        self.model = Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=workers, epochs=epochs)\n\n    def train(self, documents: List[List[str]]):\n        \"\"\"\n        Train the Doc2Vec model on a corpus of documents.\n\n        This method processes the input documents, builds a vocabulary, and trains\n        the model using the specified parameters from initialization.\n\n        Args:\n            documents (List[List[str]]): A list of tokenized documents where each document\n                                       is represented as a list of strings (tokens).\n\n        Example:\n            &gt;&gt;&gt; doc2vec = DOC2VEC()\n            &gt;&gt;&gt; docs = [['this', 'is', 'doc1'], ['this', 'is', 'doc2']]\n            &gt;&gt;&gt; doc2vec.train(docs)\n\n        Note:\n            - Documents should be preprocessed (tokenized, cleaned) before training\n            - Training time scales with corpus size and vector_size\n            - Progress can be monitored through Gensim's logging\n        \"\"\"\n        tagged_data = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(documents)]\n        self.model.build_vocab(tagged_data)\n        self.model.train(tagged_data, total_examples=self.model.corpus_count, epochs=self.model.epochs)\n\n    def get_embedding(self, doc_id: int) -&gt; List[float]:\n        \"\"\"\n        Retrieve the embedding vector for a specific document.\n\n        Args:\n            doc_id (int): The unique identifier of the document to embed.\n                         Must be within range of trained documents.\n\n        Returns:\n            List[float]: A dense vector representation of the document with\n                        dimensionality specified by vector_size.\n\n        Raises:\n            KeyError: If doc_id is not found in the trained model\n            RuntimeError: If called before training the model\n\n        Note:\n            The returned vector captures semantic properties of the document\n            and can be used for similarity calculations or as features for\n            downstream tasks.\n        \"\"\"\n        return self.model.dv[str(doc_id)].tolist()\n\n    def save_model(self, path: str):\n        \"\"\"\n        Save the trained Doc2Vec model.\n\n        Parameters:\n        - path (str): File path to save the model.\n        \"\"\"\n        self.model.save(path)\n\n    def load_model(self, path: str):\n        \"\"\"\n        Load a pre-trained Doc2Vec model.\n\n        Parameters:\n        - path (str): File path of the saved model.\n        \"\"\"\n        self.model = Doc2Vec.load(path)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/doc2vec/#engines.contentFilterEngine.embedding_representation_learning.doc2vec.DOC2VEC.__init__","title":"<code>__init__(vector_size=100, window=5, min_count=1, workers=4, epochs=10)</code>","text":"<p>Initialize a new Doc2Vec model with specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>vector_size</code> <code>int</code> <p>Dimensionality of the feature vectors. Higher dimensions can capture              more complex patterns but require more data and computation.</p> <code>100</code> <code>window</code> <code>int</code> <p>Maximum distance between the current and predicted word within a sentence.          Larger windows capture broader context but may introduce noise.</p> <code>5</code> <code>min_count</code> <code>int</code> <p>Ignores all words with total frequency lower than this value.            Helps reduce noise from rare words.</p> <code>1</code> <code>workers</code> <code>int</code> <p>Number of worker threads for training parallelization.          More workers can speed up training on multicore systems.</p> <code>4</code> <code>epochs</code> <code>int</code> <p>Number of iterations over the corpus during training.          More epochs can improve quality but increase training time.</p> <code>10</code> Note <p>The model is not trained upon initialization. Call train() with your corpus to begin training.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/doc2vec.py</code> <pre><code>def __init__(self, vector_size: int = 100, window: int = 5, min_count: int = 1, \n             workers: int = 4, epochs: int = 10):\n    \"\"\"\n    Initialize a new Doc2Vec model with specified parameters.\n\n    Args:\n        vector_size (int): Dimensionality of the feature vectors. Higher dimensions can capture\n                         more complex patterns but require more data and computation.\n        window (int): Maximum distance between the current and predicted word within a sentence.\n                     Larger windows capture broader context but may introduce noise.\n        min_count (int): Ignores all words with total frequency lower than this value.\n                       Helps reduce noise from rare words.\n        workers (int): Number of worker threads for training parallelization.\n                     More workers can speed up training on multicore systems.\n        epochs (int): Number of iterations over the corpus during training.\n                     More epochs can improve quality but increase training time.\n\n    Note:\n        The model is not trained upon initialization. Call train() with your corpus\n        to begin training.\n    \"\"\"\n    self.model = Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=workers, epochs=epochs)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/doc2vec/#engines.contentFilterEngine.embedding_representation_learning.doc2vec.DOC2VEC.get_embedding","title":"<code>get_embedding(doc_id)</code>","text":"<p>Retrieve the embedding vector for a specific document.</p> <p>Parameters:</p> Name Type Description Default <code>doc_id</code> <code>int</code> <p>The unique identifier of the document to embed.          Must be within range of trained documents.</p> required <p>Returns:</p> Type Description <code>List[float]</code> <p>List[float]: A dense vector representation of the document with         dimensionality specified by vector_size.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If doc_id is not found in the trained model</p> <code>RuntimeError</code> <p>If called before training the model</p> Note <p>The returned vector captures semantic properties of the document and can be used for similarity calculations or as features for downstream tasks.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/doc2vec.py</code> <pre><code>def get_embedding(self, doc_id: int) -&gt; List[float]:\n    \"\"\"\n    Retrieve the embedding vector for a specific document.\n\n    Args:\n        doc_id (int): The unique identifier of the document to embed.\n                     Must be within range of trained documents.\n\n    Returns:\n        List[float]: A dense vector representation of the document with\n                    dimensionality specified by vector_size.\n\n    Raises:\n        KeyError: If doc_id is not found in the trained model\n        RuntimeError: If called before training the model\n\n    Note:\n        The returned vector captures semantic properties of the document\n        and can be used for similarity calculations or as features for\n        downstream tasks.\n    \"\"\"\n    return self.model.dv[str(doc_id)].tolist()\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/doc2vec/#engines.contentFilterEngine.embedding_representation_learning.doc2vec.DOC2VEC.load_model","title":"<code>load_model(path)</code>","text":"<p>Load a pre-trained Doc2Vec model.</p> <p>Parameters: - path (str): File path of the saved model.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/doc2vec.py</code> <pre><code>def load_model(self, path: str):\n    \"\"\"\n    Load a pre-trained Doc2Vec model.\n\n    Parameters:\n    - path (str): File path of the saved model.\n    \"\"\"\n    self.model = Doc2Vec.load(path)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/doc2vec/#engines.contentFilterEngine.embedding_representation_learning.doc2vec.DOC2VEC.save_model","title":"<code>save_model(path)</code>","text":"<p>Save the trained Doc2Vec model.</p> <p>Parameters: - path (str): File path to save the model.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/doc2vec.py</code> <pre><code>def save_model(self, path: str):\n    \"\"\"\n    Save the trained Doc2Vec model.\n\n    Parameters:\n    - path (str): File path to save the model.\n    \"\"\"\n    self.model.save(path)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/doc2vec/#engines.contentFilterEngine.embedding_representation_learning.doc2vec.DOC2VEC.train","title":"<code>train(documents)</code>","text":"<p>Train the Doc2Vec model on a corpus of documents.</p> <p>This method processes the input documents, builds a vocabulary, and trains the model using the specified parameters from initialization.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[List[str]]</code> <p>A list of tokenized documents where each document                        is represented as a list of strings (tokens).</p> required Example <p>doc2vec = DOC2VEC() docs = [['this', 'is', 'doc1'], ['this', 'is', 'doc2']] doc2vec.train(docs)</p> Note <ul> <li>Documents should be preprocessed (tokenized, cleaned) before training</li> <li>Training time scales with corpus size and vector_size</li> <li>Progress can be monitored through Gensim's logging</li> </ul> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/doc2vec.py</code> <pre><code>def train(self, documents: List[List[str]]):\n    \"\"\"\n    Train the Doc2Vec model on a corpus of documents.\n\n    This method processes the input documents, builds a vocabulary, and trains\n    the model using the specified parameters from initialization.\n\n    Args:\n        documents (List[List[str]]): A list of tokenized documents where each document\n                                   is represented as a list of strings (tokens).\n\n    Example:\n        &gt;&gt;&gt; doc2vec = DOC2VEC()\n        &gt;&gt;&gt; docs = [['this', 'is', 'doc1'], ['this', 'is', 'doc2']]\n        &gt;&gt;&gt; doc2vec.train(docs)\n\n    Note:\n        - Documents should be preprocessed (tokenized, cleaned) before training\n        - Training time scales with corpus size and vector_size\n        - Progress can be monitored through Gensim's logging\n    \"\"\"\n    tagged_data = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(documents)]\n    self.model.build_vocab(tagged_data)\n    self.model.train(tagged_data, total_examples=self.model.corpus_count, epochs=self.model.epochs)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/personalized_embeddings/","title":"personalized_embedding","text":""},{"location":"contentFilterEngine/embedding_representation_learning/personalized_embeddings/#engines.contentFilterEngine.embedding_representation_learning.personalized_embeddings.PERSONALIZED_EMBEDDINGS","title":"<code>PERSONALIZED_EMBEDDINGS</code>","text":"<p>A unified embedding manager combining Word2Vec and Doc2Vec capabilities.</p> <p>This class provides a comprehensive interface for training and managing both word and document embeddings, making it suitable for personalized recommendation systems that need to understand both word-level and document-level semantics.</p> <p>Attributes:</p> Name Type Description <code>word2vec</code> <code>WORD2VEC</code> <p>Instance of the Word2Vec model for word embeddings</p> <code>doc2vec</code> <code>DOC2VEC</code> <p>Instance of the Doc2Vec model for document embeddings</p> <p>Methods:</p> Name Description <code>train_word2vec</code> <p>Trains the Word2Vec model on a corpus of sentences</p> <code>train_doc2vec</code> <p>Trains the Doc2Vec model on a corpus of documents</p> <code>get_word_embedding</code> <p>Retrieves word vectors</p> <code>get_doc_embedding</code> <p>Retrieves document vectors</p> <code>save_models</code> <p>Persists both models to disk</p> <code>load_models</code> <p>Loads both models from disk</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/personalized_embeddings.py</code> <pre><code>class PERSONALIZED_EMBEDDINGS:\n    \"\"\"\n    A unified embedding manager combining Word2Vec and Doc2Vec capabilities.\n\n    This class provides a comprehensive interface for training and managing both word\n    and document embeddings, making it suitable for personalized recommendation systems\n    that need to understand both word-level and document-level semantics.\n\n    Attributes:\n        word2vec (WORD2VEC): Instance of the Word2Vec model for word embeddings\n        doc2vec (DOC2VEC): Instance of the Doc2Vec model for document embeddings\n\n    Methods:\n        train_word2vec: Trains the Word2Vec model on a corpus of sentences\n        train_doc2vec: Trains the Doc2Vec model on a corpus of documents\n        get_word_embedding: Retrieves word vectors\n        get_doc_embedding: Retrieves document vectors\n        save_models: Persists both models to disk\n        load_models: Loads both models from disk\n    \"\"\"\n\n    def __init__(self, word2vec_params: Dict[str, Any] = None, doc2vec_params: Dict[str, Any] = None):\n        \"\"\"\n        Initialize both Word2Vec and Doc2Vec models with customizable parameters.\n\n        Args:\n            word2vec_params (Dict[str, Any], optional): Configuration parameters for Word2Vec model.\n                                                       Includes vector_size, window, min_count, workers.\n            doc2vec_params (Dict[str, Any], optional): Configuration parameters for Doc2Vec model.\n                                                      Includes vector_size, window, min_count, workers, epochs.\n\n        Note:\n            If no parameters are provided, models will be initialized with default values.\n            See individual model documentation for default parameter details.\n        \"\"\"\n        self.word2vec = WORD2VEC(**(word2vec_params if word2vec_params else {}))\n        self.doc2vec = DOC2VEC(**(doc2vec_params if doc2vec_params else {}))\n\n    def train_word2vec(self, sentences: List[List[str]], epochs: int = 10):\n        \"\"\"\n        Train the Word2Vec model.\n\n        Parameters:\n        - sentences (List[List[str]]): A list of tokenized sentences.\n        - epochs (int): Number of training iterations.\n        \"\"\"\n        self.word2vec.train(sentences, epochs=epochs)\n\n    def train_doc2vec(self, documents: List[List[str]]):\n        \"\"\"\n        Train the Doc2Vec model.\n\n        Parameters:\n        - documents (List[List[str]]): A list of tokenized documents.\n        \"\"\"\n        self.doc2vec.train(documents)\n\n    def get_word_embedding(self, word: str) -&gt; List[float]:\n        \"\"\"\n        Get the embedding vector for a given word.\n\n        Parameters:\n        - word (str): The word to retrieve the embedding for.\n\n        Returns:\n        - List[float]: The embedding vector.\n        \"\"\"\n        return self.word2vec.get_embedding(word)\n\n    def get_doc_embedding(self, doc_id: int) -&gt; List[float]:\n        \"\"\"\n        Get the embedding vector for a given document ID.\n\n        Parameters:\n        - doc_id (int): The document ID.\n\n        Returns:\n        - List[float]: The embedding vector.\n        \"\"\"\n        return self.doc2vec.get_embedding(doc_id)\n\n    def save_models(self, word2vec_path: str, doc2vec_path: str):\n        \"\"\"\n        Save both Word2Vec and Doc2Vec models.\n\n        Parameters:\n        - word2vec_path (str): File path to save the Word2Vec model.\n        - doc2vec_path (str): File path to save the Doc2Vec model.\n        \"\"\"\n        self.word2vec.save_model(word2vec_path)\n        self.doc2vec.save_model(doc2vec_path)\n\n    def load_models(self, word2vec_path: str, doc2vec_path: str):\n        \"\"\"\n        Load pre-trained Word2Vec and Doc2Vec models.\n\n        Parameters:\n        - word2vec_path (str): File path of the saved Word2Vec model.\n        - doc2vec_path (str): File path of the saved Doc2Vec model.\n        \"\"\"\n        self.word2vec.load_model(word2vec_path)\n        self.doc2vec.load_model(doc2vec_path)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/personalized_embeddings/#engines.contentFilterEngine.embedding_representation_learning.personalized_embeddings.PERSONALIZED_EMBEDDINGS.__init__","title":"<code>__init__(word2vec_params=None, doc2vec_params=None)</code>","text":"<p>Initialize both Word2Vec and Doc2Vec models with customizable parameters.</p> <p>Parameters:</p> Name Type Description Default <code>word2vec_params</code> <code>Dict[str, Any]</code> <p>Configuration parameters for Word2Vec model.                                        Includes vector_size, window, min_count, workers.</p> <code>None</code> <code>doc2vec_params</code> <code>Dict[str, Any]</code> <p>Configuration parameters for Doc2Vec model.                                       Includes vector_size, window, min_count, workers, epochs.</p> <code>None</code> Note <p>If no parameters are provided, models will be initialized with default values. See individual model documentation for default parameter details.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/personalized_embeddings.py</code> <pre><code>def __init__(self, word2vec_params: Dict[str, Any] = None, doc2vec_params: Dict[str, Any] = None):\n    \"\"\"\n    Initialize both Word2Vec and Doc2Vec models with customizable parameters.\n\n    Args:\n        word2vec_params (Dict[str, Any], optional): Configuration parameters for Word2Vec model.\n                                                   Includes vector_size, window, min_count, workers.\n        doc2vec_params (Dict[str, Any], optional): Configuration parameters for Doc2Vec model.\n                                                  Includes vector_size, window, min_count, workers, epochs.\n\n    Note:\n        If no parameters are provided, models will be initialized with default values.\n        See individual model documentation for default parameter details.\n    \"\"\"\n    self.word2vec = WORD2VEC(**(word2vec_params if word2vec_params else {}))\n    self.doc2vec = DOC2VEC(**(doc2vec_params if doc2vec_params else {}))\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/personalized_embeddings/#engines.contentFilterEngine.embedding_representation_learning.personalized_embeddings.PERSONALIZED_EMBEDDINGS.get_doc_embedding","title":"<code>get_doc_embedding(doc_id)</code>","text":"<p>Get the embedding vector for a given document ID.</p> <p>Parameters: - doc_id (int): The document ID.</p> <p>Returns: - List[float]: The embedding vector.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/personalized_embeddings.py</code> <pre><code>def get_doc_embedding(self, doc_id: int) -&gt; List[float]:\n    \"\"\"\n    Get the embedding vector for a given document ID.\n\n    Parameters:\n    - doc_id (int): The document ID.\n\n    Returns:\n    - List[float]: The embedding vector.\n    \"\"\"\n    return self.doc2vec.get_embedding(doc_id)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/personalized_embeddings/#engines.contentFilterEngine.embedding_representation_learning.personalized_embeddings.PERSONALIZED_EMBEDDINGS.get_word_embedding","title":"<code>get_word_embedding(word)</code>","text":"<p>Get the embedding vector for a given word.</p> <p>Parameters: - word (str): The word to retrieve the embedding for.</p> <p>Returns: - List[float]: The embedding vector.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/personalized_embeddings.py</code> <pre><code>def get_word_embedding(self, word: str) -&gt; List[float]:\n    \"\"\"\n    Get the embedding vector for a given word.\n\n    Parameters:\n    - word (str): The word to retrieve the embedding for.\n\n    Returns:\n    - List[float]: The embedding vector.\n    \"\"\"\n    return self.word2vec.get_embedding(word)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/personalized_embeddings/#engines.contentFilterEngine.embedding_representation_learning.personalized_embeddings.PERSONALIZED_EMBEDDINGS.load_models","title":"<code>load_models(word2vec_path, doc2vec_path)</code>","text":"<p>Load pre-trained Word2Vec and Doc2Vec models.</p> <p>Parameters: - word2vec_path (str): File path of the saved Word2Vec model. - doc2vec_path (str): File path of the saved Doc2Vec model.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/personalized_embeddings.py</code> <pre><code>def load_models(self, word2vec_path: str, doc2vec_path: str):\n    \"\"\"\n    Load pre-trained Word2Vec and Doc2Vec models.\n\n    Parameters:\n    - word2vec_path (str): File path of the saved Word2Vec model.\n    - doc2vec_path (str): File path of the saved Doc2Vec model.\n    \"\"\"\n    self.word2vec.load_model(word2vec_path)\n    self.doc2vec.load_model(doc2vec_path)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/personalized_embeddings/#engines.contentFilterEngine.embedding_representation_learning.personalized_embeddings.PERSONALIZED_EMBEDDINGS.save_models","title":"<code>save_models(word2vec_path, doc2vec_path)</code>","text":"<p>Save both Word2Vec and Doc2Vec models.</p> <p>Parameters: - word2vec_path (str): File path to save the Word2Vec model. - doc2vec_path (str): File path to save the Doc2Vec model.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/personalized_embeddings.py</code> <pre><code>def save_models(self, word2vec_path: str, doc2vec_path: str):\n    \"\"\"\n    Save both Word2Vec and Doc2Vec models.\n\n    Parameters:\n    - word2vec_path (str): File path to save the Word2Vec model.\n    - doc2vec_path (str): File path to save the Doc2Vec model.\n    \"\"\"\n    self.word2vec.save_model(word2vec_path)\n    self.doc2vec.save_model(doc2vec_path)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/personalized_embeddings/#engines.contentFilterEngine.embedding_representation_learning.personalized_embeddings.PERSONALIZED_EMBEDDINGS.train_doc2vec","title":"<code>train_doc2vec(documents)</code>","text":"<p>Train the Doc2Vec model.</p> <p>Parameters: - documents (List[List[str]]): A list of tokenized documents.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/personalized_embeddings.py</code> <pre><code>def train_doc2vec(self, documents: List[List[str]]):\n    \"\"\"\n    Train the Doc2Vec model.\n\n    Parameters:\n    - documents (List[List[str]]): A list of tokenized documents.\n    \"\"\"\n    self.doc2vec.train(documents)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/personalized_embeddings/#engines.contentFilterEngine.embedding_representation_learning.personalized_embeddings.PERSONALIZED_EMBEDDINGS.train_word2vec","title":"<code>train_word2vec(sentences, epochs=10)</code>","text":"<p>Train the Word2Vec model.</p> <p>Parameters: - sentences (List[List[str]]): A list of tokenized sentences. - epochs (int): Number of training iterations.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/personalized_embeddings.py</code> <pre><code>def train_word2vec(self, sentences: List[List[str]], epochs: int = 10):\n    \"\"\"\n    Train the Word2Vec model.\n\n    Parameters:\n    - sentences (List[List[str]]): A list of tokenized sentences.\n    - epochs (int): Number of training iterations.\n    \"\"\"\n    self.word2vec.train(sentences, epochs=epochs)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/word2vec/","title":"word2vec","text":""},{"location":"contentFilterEngine/embedding_representation_learning/word2vec/#engines.contentFilterEngine.embedding_representation_learning.word2vec.WORD2VEC","title":"<code>WORD2VEC</code>","text":"<p>A Word2Vec model implementation for generating word embeddings.</p> <p>This class provides methods for training word embeddings and managing model persistence. It's particularly useful for recommendation systems that need to understand word-level semantics.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Word2Vec</code> <p>The underlying Gensim Word2Vec model instance</p> <p>Methods:</p> Name Description <code>train</code> <p>Trains the Word2Vec model on a corpus of sentences</p> <code>get_embedding</code> <p>Retrieves the embedding vector for a specific word</p> <code>save_model</code> <p>Persists the trained model to disk</p> <code>load_model</code> <p>Loads a previously trained model from disk</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/word2vec.py</code> <pre><code>class WORD2VEC:\n    \"\"\"\n    A Word2Vec model implementation for generating word embeddings.\n\n    This class provides methods for training word embeddings and managing model\n    persistence. It's particularly useful for recommendation systems that need\n    to understand word-level semantics.\n\n    Attributes:\n        model (Word2Vec): The underlying Gensim Word2Vec model instance\n\n    Methods:\n        train: Trains the Word2Vec model on a corpus of sentences\n        get_embedding: Retrieves the embedding vector for a specific word\n        save_model: Persists the trained model to disk\n        load_model: Loads a previously trained model from disk\n    \"\"\"\n\n    def __init__(self, vector_size: int = 100, window: int = 5, min_count: int = 1, workers: int = 4):\n        \"\"\"\n        Initialize a new Word2Vec model with specified parameters.\n\n        Args:\n            vector_size (int): Dimensionality of the word vectors. Higher dimensions can capture\n                             more complex semantic relationships but require more data.\n            window (int): Maximum distance between the current and predicted word within a sentence.\n                         Larger windows consider broader context but may be noisier.\n            min_count (int): Ignores all words with total frequency lower than this value.\n                           Helps reduce noise from rare words.\n            workers (int): Number of worker threads for training parallelization.\n                         More workers can speed up training on multicore systems.\n\n        Note:\n            The model is not trained upon initialization. Call train() with your corpus\n            to begin training.\n        \"\"\"\n        self.model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n\n    def train(self, sentences: List[List[str]], epochs: int = 10):\n        \"\"\"\n        Train the Word2Vec model on a corpus of sentences.\n\n        Args:\n            sentences (List[List[str]]): A list of tokenized sentences where each sentence\n                                       is represented as a list of strings (tokens).\n            epochs (int): Number of iterations over the corpus during training.\n                         More epochs can improve quality but increase training time.\n\n        Note:\n            - Sentences should be preprocessed (tokenized, cleaned) before training\n            - Training time scales with corpus size and vector_size\n            - Progress can be monitored through Gensim's logging\n        \"\"\"\n        self.model.build_vocab(sentences)\n        self.model.train(sentences, total_examples=self.model.corpus_count, epochs=epochs)\n\n    def get_embedding(self, word: str) -&gt; List[float]:\n        \"\"\"\n        Get the embedding vector for a given word.\n\n        Parameters:\n        - word (str): The word to retrieve the embedding for.\n\n        Returns:\n        - List[float]: The embedding vector.\n        \"\"\"\n        if word in self.model.wv:\n            return self.model.wv[word].tolist()\n        else:\n            return [0.0] * self.model.vector_size\n\n    def save_model(self, path: str):\n        \"\"\"\n        Save the trained Word2Vec model.\n\n        Parameters:\n        - path (str): File path to save the model.\n        \"\"\"\n        self.model.save(path)\n\n    def load_model(self, path: str):\n        \"\"\"\n        Load a pre-trained Word2Vec model.\n\n        Parameters:\n        - path (str): File path of the saved model.\n        \"\"\"\n        self.model = Word2Vec.load(path)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/word2vec/#engines.contentFilterEngine.embedding_representation_learning.word2vec.WORD2VEC.__init__","title":"<code>__init__(vector_size=100, window=5, min_count=1, workers=4)</code>","text":"<p>Initialize a new Word2Vec model with specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>vector_size</code> <code>int</code> <p>Dimensionality of the word vectors. Higher dimensions can capture              more complex semantic relationships but require more data.</p> <code>100</code> <code>window</code> <code>int</code> <p>Maximum distance between the current and predicted word within a sentence.          Larger windows consider broader context but may be noisier.</p> <code>5</code> <code>min_count</code> <code>int</code> <p>Ignores all words with total frequency lower than this value.            Helps reduce noise from rare words.</p> <code>1</code> <code>workers</code> <code>int</code> <p>Number of worker threads for training parallelization.          More workers can speed up training on multicore systems.</p> <code>4</code> Note <p>The model is not trained upon initialization. Call train() with your corpus to begin training.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/word2vec.py</code> <pre><code>def __init__(self, vector_size: int = 100, window: int = 5, min_count: int = 1, workers: int = 4):\n    \"\"\"\n    Initialize a new Word2Vec model with specified parameters.\n\n    Args:\n        vector_size (int): Dimensionality of the word vectors. Higher dimensions can capture\n                         more complex semantic relationships but require more data.\n        window (int): Maximum distance between the current and predicted word within a sentence.\n                     Larger windows consider broader context but may be noisier.\n        min_count (int): Ignores all words with total frequency lower than this value.\n                       Helps reduce noise from rare words.\n        workers (int): Number of worker threads for training parallelization.\n                     More workers can speed up training on multicore systems.\n\n    Note:\n        The model is not trained upon initialization. Call train() with your corpus\n        to begin training.\n    \"\"\"\n    self.model = Word2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=workers)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/word2vec/#engines.contentFilterEngine.embedding_representation_learning.word2vec.WORD2VEC.get_embedding","title":"<code>get_embedding(word)</code>","text":"<p>Get the embedding vector for a given word.</p> <p>Parameters: - word (str): The word to retrieve the embedding for.</p> <p>Returns: - List[float]: The embedding vector.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/word2vec.py</code> <pre><code>def get_embedding(self, word: str) -&gt; List[float]:\n    \"\"\"\n    Get the embedding vector for a given word.\n\n    Parameters:\n    - word (str): The word to retrieve the embedding for.\n\n    Returns:\n    - List[float]: The embedding vector.\n    \"\"\"\n    if word in self.model.wv:\n        return self.model.wv[word].tolist()\n    else:\n        return [0.0] * self.model.vector_size\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/word2vec/#engines.contentFilterEngine.embedding_representation_learning.word2vec.WORD2VEC.load_model","title":"<code>load_model(path)</code>","text":"<p>Load a pre-trained Word2Vec model.</p> <p>Parameters: - path (str): File path of the saved model.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/word2vec.py</code> <pre><code>def load_model(self, path: str):\n    \"\"\"\n    Load a pre-trained Word2Vec model.\n\n    Parameters:\n    - path (str): File path of the saved model.\n    \"\"\"\n    self.model = Word2Vec.load(path)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/word2vec/#engines.contentFilterEngine.embedding_representation_learning.word2vec.WORD2VEC.save_model","title":"<code>save_model(path)</code>","text":"<p>Save the trained Word2Vec model.</p> <p>Parameters: - path (str): File path to save the model.</p> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/word2vec.py</code> <pre><code>def save_model(self, path: str):\n    \"\"\"\n    Save the trained Word2Vec model.\n\n    Parameters:\n    - path (str): File path to save the model.\n    \"\"\"\n    self.model.save(path)\n</code></pre>"},{"location":"contentFilterEngine/embedding_representation_learning/word2vec/#engines.contentFilterEngine.embedding_representation_learning.word2vec.WORD2VEC.train","title":"<code>train(sentences, epochs=10)</code>","text":"<p>Train the Word2Vec model on a corpus of sentences.</p> <p>Parameters:</p> Name Type Description Default <code>sentences</code> <code>List[List[str]]</code> <p>A list of tokenized sentences where each sentence                        is represented as a list of strings (tokens).</p> required <code>epochs</code> <code>int</code> <p>Number of iterations over the corpus during training.          More epochs can improve quality but increase training time.</p> <code>10</code> Note <ul> <li>Sentences should be preprocessed (tokenized, cleaned) before training</li> <li>Training time scales with corpus size and vector_size</li> <li>Progress can be monitored through Gensim's logging</li> </ul> Source code in <code>engines/contentFilterEngine/embedding_representation_learning/word2vec.py</code> <pre><code>def train(self, sentences: List[List[str]], epochs: int = 10):\n    \"\"\"\n    Train the Word2Vec model on a corpus of sentences.\n\n    Args:\n        sentences (List[List[str]]): A list of tokenized sentences where each sentence\n                                   is represented as a list of strings (tokens).\n        epochs (int): Number of iterations over the corpus during training.\n                     More epochs can improve quality but increase training time.\n\n    Note:\n        - Sentences should be preprocessed (tokenized, cleaned) before training\n        - Training time scales with corpus size and vector_size\n        - Progress can be monitored through Gensim's logging\n    \"\"\"\n    self.model.build_vocab(sentences)\n    self.model.train(sentences, total_examples=self.model.corpus_count, epochs=epochs)\n</code></pre>"},{"location":"contentFilterEngine/fairness_explainability/explainable/","title":"explainable","text":""},{"location":"contentFilterEngine/fairness_explainability/explainable/#engines.contentFilterEngine.fairness_explainability.explainable.EXPLAINABLE","title":"<code>EXPLAINABLE</code>","text":"Source code in <code>engines/contentFilterEngine/fairness_explainability/explainable.py</code> <pre><code>class EXPLAINABLE:\n    def __init__(self):\n        \"\"\"\n        Initialize the explainable module.\n\n        Attributes:\n            explanations (dict): A dictionary to store explanations with keys as \n            (user_id, item_id) tuples and values as explanation strings.\n        \"\"\"\n        self.explanations = {}\n\n    def generate_explanation(self, user_id: int, item_id: int, context: Optional[Dict[str, Any]] = None) -&gt; str:\n        \"\"\"\n        Generate an explanation for why a particular item was recommended to a user.\n\n        Parameters:\n            user_id (int): The ID of the user for whom the recommendation was made.\n            item_id (int): The ID of the recommended item.\n            context (dict, optional): Additional context in which the recommendation was made, \n            such as user preferences or item features.\n\n        Returns:\n            str: A textual explanation of the recommendation, detailing the factors that \n            influenced the recommendation decision.\n        \"\"\"\n        explanation = f\"Item {item_id} was recommended to User {user_id} because \"\n        if context:\n            explanation += f\"of the context {context} and \"\n        explanation += \"based on similar items and user preferences.\"\n        self.explanations[(user_id, item_id)] = explanation\n        return explanation\n\n    def get_explanation(self, user_id: int, item_id: int) -&gt; str:\n        \"\"\"\n        Retrieve a previously generated explanation for a recommendation.\n\n        Parameters:\n            user_id (int): The ID of the user for whom the recommendation was made.\n            item_id (int): The ID of the recommended item.\n\n        Returns:\n            str: The explanation of the recommendation if available, otherwise a default \n            message indicating that no explanation is available.\n        \"\"\"\n        return self.explanations.get((user_id, item_id), \"No explanation available.\")\n</code></pre>"},{"location":"contentFilterEngine/fairness_explainability/explainable/#engines.contentFilterEngine.fairness_explainability.explainable.EXPLAINABLE.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the explainable module.</p> <p>Attributes:</p> Name Type Description <code>explanations</code> <code>dict</code> <p>A dictionary to store explanations with keys as </p> Source code in <code>engines/contentFilterEngine/fairness_explainability/explainable.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the explainable module.\n\n    Attributes:\n        explanations (dict): A dictionary to store explanations with keys as \n        (user_id, item_id) tuples and values as explanation strings.\n    \"\"\"\n    self.explanations = {}\n</code></pre>"},{"location":"contentFilterEngine/fairness_explainability/explainable/#engines.contentFilterEngine.fairness_explainability.explainable.EXPLAINABLE.generate_explanation","title":"<code>generate_explanation(user_id, item_id, context=None)</code>","text":"<p>Generate an explanation for why a particular item was recommended to a user.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>int</code> <p>The ID of the user for whom the recommendation was made.</p> required <code>item_id</code> <code>int</code> <p>The ID of the recommended item.</p> required <code>context</code> <code>dict</code> <p>Additional context in which the recommendation was made, </p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A textual explanation of the recommendation, detailing the factors that </p> <code>str</code> <p>influenced the recommendation decision.</p> Source code in <code>engines/contentFilterEngine/fairness_explainability/explainable.py</code> <pre><code>def generate_explanation(self, user_id: int, item_id: int, context: Optional[Dict[str, Any]] = None) -&gt; str:\n    \"\"\"\n    Generate an explanation for why a particular item was recommended to a user.\n\n    Parameters:\n        user_id (int): The ID of the user for whom the recommendation was made.\n        item_id (int): The ID of the recommended item.\n        context (dict, optional): Additional context in which the recommendation was made, \n        such as user preferences or item features.\n\n    Returns:\n        str: A textual explanation of the recommendation, detailing the factors that \n        influenced the recommendation decision.\n    \"\"\"\n    explanation = f\"Item {item_id} was recommended to User {user_id} because \"\n    if context:\n        explanation += f\"of the context {context} and \"\n    explanation += \"based on similar items and user preferences.\"\n    self.explanations[(user_id, item_id)] = explanation\n    return explanation\n</code></pre>"},{"location":"contentFilterEngine/fairness_explainability/explainable/#engines.contentFilterEngine.fairness_explainability.explainable.EXPLAINABLE.get_explanation","title":"<code>get_explanation(user_id, item_id)</code>","text":"<p>Retrieve a previously generated explanation for a recommendation.</p> <p>Parameters:</p> Name Type Description Default <code>user_id</code> <code>int</code> <p>The ID of the user for whom the recommendation was made.</p> required <code>item_id</code> <code>int</code> <p>The ID of the recommended item.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The explanation of the recommendation if available, otherwise a default </p> <code>str</code> <p>message indicating that no explanation is available.</p> Source code in <code>engines/contentFilterEngine/fairness_explainability/explainable.py</code> <pre><code>def get_explanation(self, user_id: int, item_id: int) -&gt; str:\n    \"\"\"\n    Retrieve a previously generated explanation for a recommendation.\n\n    Parameters:\n        user_id (int): The ID of the user for whom the recommendation was made.\n        item_id (int): The ID of the recommended item.\n\n    Returns:\n        str: The explanation of the recommendation if available, otherwise a default \n        message indicating that no explanation is available.\n    \"\"\"\n    return self.explanations.get((user_id, item_id), \"No explanation available.\")\n</code></pre>"},{"location":"contentFilterEngine/fairness_explainability/fairness_aware/","title":"fairness aware","text":""},{"location":"contentFilterEngine/fairness_explainability/fairness_aware/#engines.contentFilterEngine.fairness_explainability.fairness_aware.FAIRNESS_AWARE","title":"<code>FAIRNESS_AWARE</code>","text":"Source code in <code>engines/contentFilterEngine/fairness_explainability/fairness_aware.py</code> <pre><code>class FAIRNESS_AWARE:\n    def __init__(self):\n        \"\"\"\n        Initialize the fairness-aware module.\n\n        Attributes:\n            fairness_metrics (dict): A dictionary to store calculated fairness metrics, \n            such as the distribution of recommendations across different user demographics.\n        \"\"\"\n        self.fairness_metrics = {}\n\n    def evaluate_fairness(self, recommendations: Dict[int, List[int]], user_attributes: pd.DataFrame) -&gt; Dict[str, float]:\n        \"\"\"\n        Evaluate the fairness of the recommendations across different user groups.\n\n        Parameters:\n            recommendations (dict): A dictionary mapping user IDs to lists of recommended item IDs.\n            user_attributes (pd.DataFrame): A DataFrame containing user demographic information, \n            such as age, gender, and other relevant attributes.\n\n        Returns:\n            dict: A dictionary of fairness metrics, providing insights into how recommendations \n            are distributed across different user groups. For example, it may include the \n            distribution of recommendations by gender or age group.\n        \"\"\"\n        # Example: Calculate the distribution of recommendations across gender\n        gender_distribution = user_attributes['gender'].value_counts(normalize=True).to_dict()\n        self.fairness_metrics['gender_distribution'] = gender_distribution\n        return self.fairness_metrics\n\n    def ensure_fairness(self, recommendations: Dict[int, List[int]], user_attributes: pd.DataFrame) -&gt; Dict[int, List[int]]:\n        \"\"\"\n        Adjust recommendations to ensure fairness across user groups.\n\n        Parameters:\n            recommendations (dict): A dictionary mapping user IDs to lists of recommended item IDs.\n            user_attributes (pd.DataFrame): A DataFrame containing user demographic information, \n            such as age, gender, and other relevant attributes.\n\n        Returns:\n            dict: Adjusted recommendations ensuring fairness, potentially modifying the original \n            recommendations to achieve a more balanced distribution across user groups.\n        \"\"\"\n        # Placeholder: Implement logic to adjust recommendations for fairness\n        return recommendations\n</code></pre>"},{"location":"contentFilterEngine/fairness_explainability/fairness_aware/#engines.contentFilterEngine.fairness_explainability.fairness_aware.FAIRNESS_AWARE.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the fairness-aware module.</p> <p>Attributes:</p> Name Type Description <code>fairness_metrics</code> <code>dict</code> <p>A dictionary to store calculated fairness metrics, </p> Source code in <code>engines/contentFilterEngine/fairness_explainability/fairness_aware.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the fairness-aware module.\n\n    Attributes:\n        fairness_metrics (dict): A dictionary to store calculated fairness metrics, \n        such as the distribution of recommendations across different user demographics.\n    \"\"\"\n    self.fairness_metrics = {}\n</code></pre>"},{"location":"contentFilterEngine/fairness_explainability/fairness_aware/#engines.contentFilterEngine.fairness_explainability.fairness_aware.FAIRNESS_AWARE.ensure_fairness","title":"<code>ensure_fairness(recommendations, user_attributes)</code>","text":"<p>Adjust recommendations to ensure fairness across user groups.</p> <p>Parameters:</p> Name Type Description Default <code>recommendations</code> <code>dict</code> <p>A dictionary mapping user IDs to lists of recommended item IDs.</p> required <code>user_attributes</code> <code>DataFrame</code> <p>A DataFrame containing user demographic information, </p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[int, List[int]]</code> <p>Adjusted recommendations ensuring fairness, potentially modifying the original </p> <code>Dict[int, List[int]]</code> <p>recommendations to achieve a more balanced distribution across user groups.</p> Source code in <code>engines/contentFilterEngine/fairness_explainability/fairness_aware.py</code> <pre><code>def ensure_fairness(self, recommendations: Dict[int, List[int]], user_attributes: pd.DataFrame) -&gt; Dict[int, List[int]]:\n    \"\"\"\n    Adjust recommendations to ensure fairness across user groups.\n\n    Parameters:\n        recommendations (dict): A dictionary mapping user IDs to lists of recommended item IDs.\n        user_attributes (pd.DataFrame): A DataFrame containing user demographic information, \n        such as age, gender, and other relevant attributes.\n\n    Returns:\n        dict: Adjusted recommendations ensuring fairness, potentially modifying the original \n        recommendations to achieve a more balanced distribution across user groups.\n    \"\"\"\n    # Placeholder: Implement logic to adjust recommendations for fairness\n    return recommendations\n</code></pre>"},{"location":"contentFilterEngine/fairness_explainability/fairness_aware/#engines.contentFilterEngine.fairness_explainability.fairness_aware.FAIRNESS_AWARE.evaluate_fairness","title":"<code>evaluate_fairness(recommendations, user_attributes)</code>","text":"<p>Evaluate the fairness of the recommendations across different user groups.</p> <p>Parameters:</p> Name Type Description Default <code>recommendations</code> <code>dict</code> <p>A dictionary mapping user IDs to lists of recommended item IDs.</p> required <code>user_attributes</code> <code>DataFrame</code> <p>A DataFrame containing user demographic information, </p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, float]</code> <p>A dictionary of fairness metrics, providing insights into how recommendations </p> <code>Dict[str, float]</code> <p>are distributed across different user groups. For example, it may include the </p> <code>Dict[str, float]</code> <p>distribution of recommendations by gender or age group.</p> Source code in <code>engines/contentFilterEngine/fairness_explainability/fairness_aware.py</code> <pre><code>def evaluate_fairness(self, recommendations: Dict[int, List[int]], user_attributes: pd.DataFrame) -&gt; Dict[str, float]:\n    \"\"\"\n    Evaluate the fairness of the recommendations across different user groups.\n\n    Parameters:\n        recommendations (dict): A dictionary mapping user IDs to lists of recommended item IDs.\n        user_attributes (pd.DataFrame): A DataFrame containing user demographic information, \n        such as age, gender, and other relevant attributes.\n\n    Returns:\n        dict: A dictionary of fairness metrics, providing insights into how recommendations \n        are distributed across different user groups. For example, it may include the \n        distribution of recommendations by gender or age group.\n    \"\"\"\n    # Example: Calculate the distribution of recommendations across gender\n    gender_distribution = user_attributes['gender'].value_counts(normalize=True).to_dict()\n    self.fairness_metrics['gender_distribution'] = gender_distribution\n    return self.fairness_metrics\n</code></pre>"},{"location":"contentFilterEngine/fairness_explainability/privacy_preserving/","title":"privacy preserving","text":""},{"location":"contentFilterEngine/fairness_explainability/privacy_preserving/#engines.contentFilterEngine.fairness_explainability.privacy_preserving.PRIVACY_PRESERVING","title":"<code>PRIVACY_PRESERVING</code>","text":"Source code in <code>engines/contentFilterEngine/fairness_explainability/privacy_preserving.py</code> <pre><code>class PRIVACY_PRESERVING:\n    def __init__(self):\n        \"\"\"\n        Initialize the privacy-preserving module.\n        \"\"\"\n        self.anonymized_data = {}\n\n    def anonymize_data(self, user_data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Anonymize user data to preserve privacy.\n\n        Parameters:\n        - user_data (pd.DataFrame): DataFrame containing user information.\n\n        Returns:\n        - pd.DataFrame: Anonymized user data.\n        \"\"\"\n        # Example: Remove identifiable information\n        anonymized_data = user_data.drop(columns=['user_id', 'zip_code'])\n        self.anonymized_data = anonymized_data\n        return anonymized_data\n\n    def apply_differential_privacy(self, data: pd.DataFrame, epsilon: float) -&gt; pd.DataFrame:\n        \"\"\"\n        Apply differential privacy to the data.\n\n        Parameters:\n        - data (pd.DataFrame): DataFrame containing data to be privatized.\n        - epsilon (float): Privacy budget parameter.\n\n        Returns:\n        - pd.DataFrame: Data with differential privacy applied.\n        \"\"\"\n        # Placeholder: Implement differential privacy logic\n        return data\n</code></pre>"},{"location":"contentFilterEngine/fairness_explainability/privacy_preserving/#engines.contentFilterEngine.fairness_explainability.privacy_preserving.PRIVACY_PRESERVING.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the privacy-preserving module.</p> Source code in <code>engines/contentFilterEngine/fairness_explainability/privacy_preserving.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the privacy-preserving module.\n    \"\"\"\n    self.anonymized_data = {}\n</code></pre>"},{"location":"contentFilterEngine/fairness_explainability/privacy_preserving/#engines.contentFilterEngine.fairness_explainability.privacy_preserving.PRIVACY_PRESERVING.anonymize_data","title":"<code>anonymize_data(user_data)</code>","text":"<p>Anonymize user data to preserve privacy.</p> <p>Parameters: - user_data (pd.DataFrame): DataFrame containing user information.</p> <p>Returns: - pd.DataFrame: Anonymized user data.</p> Source code in <code>engines/contentFilterEngine/fairness_explainability/privacy_preserving.py</code> <pre><code>def anonymize_data(self, user_data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Anonymize user data to preserve privacy.\n\n    Parameters:\n    - user_data (pd.DataFrame): DataFrame containing user information.\n\n    Returns:\n    - pd.DataFrame: Anonymized user data.\n    \"\"\"\n    # Example: Remove identifiable information\n    anonymized_data = user_data.drop(columns=['user_id', 'zip_code'])\n    self.anonymized_data = anonymized_data\n    return anonymized_data\n</code></pre>"},{"location":"contentFilterEngine/fairness_explainability/privacy_preserving/#engines.contentFilterEngine.fairness_explainability.privacy_preserving.PRIVACY_PRESERVING.apply_differential_privacy","title":"<code>apply_differential_privacy(data, epsilon)</code>","text":"<p>Apply differential privacy to the data.</p> <p>Parameters: - data (pd.DataFrame): DataFrame containing data to be privatized. - epsilon (float): Privacy budget parameter.</p> <p>Returns: - pd.DataFrame: Data with differential privacy applied.</p> Source code in <code>engines/contentFilterEngine/fairness_explainability/privacy_preserving.py</code> <pre><code>def apply_differential_privacy(self, data: pd.DataFrame, epsilon: float) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply differential privacy to the data.\n\n    Parameters:\n    - data (pd.DataFrame): DataFrame containing data to be privatized.\n    - epsilon (float): Privacy budget parameter.\n\n    Returns:\n    - pd.DataFrame: Data with differential privacy applied.\n    \"\"\"\n    # Placeholder: Implement differential privacy logic\n    return data\n</code></pre>"},{"location":"contentFilterEngine/graph_based_algorithms/gnn/","title":"GNN","text":""},{"location":"contentFilterEngine/graph_based_algorithms/gnn/#engines.contentFilterEngine.graph_based_algorithms.gnn.GNN","title":"<code>GNN</code>","text":"<p>GNN Class</p> <p>This class implements Graph Neural Networks (GNNs) for recommendation systems. GNNs are a type of neural network designed to operate on graph-structured data, capturing complex relationships between nodes through message passing and aggregation.</p> <p>Attributes:</p> Name Type Description <code>num_layers</code> <code>int</code> <p>Number of layers in the GNN.</p> <code>hidden_dim</code> <code>int</code> <p>Dimensionality of hidden layers.</p> <code>learning_rate</code> <code>float</code> <p>Learning rate for training the GNN.</p> <code>epochs</code> <code>int</code> <p>Number of training epochs.</p> <code>graph</code> <code>Graph</code> <p>The graph structure representing users and items.</p> <p>Methods:</p> Name Description <code>build_model</code> <p>Constructs the GNN model architecture, defining layers and operations for message passing and node aggregation.</p> <code>train</code> <p>Trains the GNN model on the provided data, optimizing node embeddings for recommendation tasks.</p> <code>recommend</code> <p>Generates top-N recommendations for a given user by leveraging learned node embeddings and graph structure.</p> <code>evaluate</code> <p>Evaluates the performance of the GNN model on test data, providing metrics such as accuracy and precision.</p> Source code in <code>engines/contentFilterEngine/graph_based_algorithms/gnn.py</code> <pre><code>class GNN:\n    \"\"\"\n    GNN Class\n\n    This class implements Graph Neural Networks (GNNs) for recommendation systems.\n    GNNs are a type of neural network designed to operate on graph-structured data,\n    capturing complex relationships between nodes through message passing and aggregation.\n\n    Attributes:\n        num_layers (int): Number of layers in the GNN.\n        hidden_dim (int): Dimensionality of hidden layers.\n        learning_rate (float): Learning rate for training the GNN.\n        epochs (int): Number of training epochs.\n        graph (Graph): The graph structure representing users and items.\n\n    Methods:\n        build_model():\n            Constructs the GNN model architecture, defining layers and operations for\n            message passing and node aggregation.\n\n        train(data):\n            Trains the GNN model on the provided data, optimizing node embeddings for\n            recommendation tasks.\n\n        recommend(user_id, top_n=10):\n            Generates top-N recommendations for a given user by leveraging learned node\n            embeddings and graph structure.\n\n        evaluate(test_data):\n            Evaluates the performance of the GNN model on test data, providing metrics\n            such as accuracy and precision.\n    \"\"\"\n    def __init__(self):\n        self.graph = None\n\n    def load_graph(self, file_path):\n        \"\"\"\n        Load a graph from a file.\n        \"\"\"\n        self.graph = nx.read_adjlist(file_path, delimiter=',', nodetype=int)\n        return self.graph\n\n    def visualize_graph(self, recommended_nodes=None, top_nodes=None, node_labels=None):\n        \"\"\"\n        Visualize the graph using the draw_graph function.\n        \"\"\"\n        if self.graph is not None:\n            # Use a simpler layout for visualization\n            pos = nx.circular_layout(self.graph)\n            draw_graph(self.graph, pos=pos, \n                       top_nodes=top_nodes, \n                       recommended_nodes=recommended_nodes, \n                       node_labels=node_labels)\n        else:\n            print(\"Graph not loaded. Please load a graph first.\")\n</code></pre>"},{"location":"contentFilterEngine/graph_based_algorithms/gnn/#engines.contentFilterEngine.graph_based_algorithms.gnn.GNN.load_graph","title":"<code>load_graph(file_path)</code>","text":"<p>Load a graph from a file.</p> Source code in <code>engines/contentFilterEngine/graph_based_algorithms/gnn.py</code> <pre><code>def load_graph(self, file_path):\n    \"\"\"\n    Load a graph from a file.\n    \"\"\"\n    self.graph = nx.read_adjlist(file_path, delimiter=',', nodetype=int)\n    return self.graph\n</code></pre>"},{"location":"contentFilterEngine/graph_based_algorithms/gnn/#engines.contentFilterEngine.graph_based_algorithms.gnn.GNN.visualize_graph","title":"<code>visualize_graph(recommended_nodes=None, top_nodes=None, node_labels=None)</code>","text":"<p>Visualize the graph using the draw_graph function.</p> Source code in <code>engines/contentFilterEngine/graph_based_algorithms/gnn.py</code> <pre><code>def visualize_graph(self, recommended_nodes=None, top_nodes=None, node_labels=None):\n    \"\"\"\n    Visualize the graph using the draw_graph function.\n    \"\"\"\n    if self.graph is not None:\n        # Use a simpler layout for visualization\n        pos = nx.circular_layout(self.graph)\n        draw_graph(self.graph, pos=pos, \n                   top_nodes=top_nodes, \n                   recommended_nodes=recommended_nodes, \n                   node_labels=node_labels)\n    else:\n        print(\"Graph not loaded. Please load a graph first.\")\n</code></pre>"},{"location":"contentFilterEngine/graph_based_algorithms/graph_filtering/","title":"Graph Filtering","text":""},{"location":"contentFilterEngine/graph_based_algorithms/graph_filtering/#engines.contentFilterEngine.graph_based_algorithms.graph_filtering.GraphFiltering","title":"<code>GraphFiltering</code>","text":"<p>GraphFiltering Class</p> <p>This class implements graph-based filtering techniques for recommendation systems. Graph-based filtering leverages the structure of a graph to make recommendations by analyzing the relationships between users and items.</p> <p>Attributes:</p> Name Type Description <code>graph</code> <code>Graph</code> <p>The graph structure representing users and items.</p> <code>similarity_metric</code> <code>str</code> <p>The metric used to calculate similarity between nodes.</p> <code>damping_factor</code> <code>float</code> <p>The damping factor used in algorithms like PageRank.</p> <code>max_iterations</code> <code>int</code> <p>Maximum number of iterations for convergence in iterative algorithms.</p> <p>Methods:</p> Name Description <code>build_graph</code> <p>Constructs the graph from the given data, where nodes represent users and items, and edges represent interactions or similarities.</p> <code>compute_similarity</code> <p>Computes similarity scores between nodes using the specified similarity metric.</p> <code>recommend</code> <p>Generates top-N recommendations for a given user by analyzing the graph structure.</p> <code>update_graph</code> <p>Updates the graph with new interactions or changes in the data, allowing for dynamic recommendations.</p> Source code in <code>engines/contentFilterEngine/graph_based_algorithms/graph_filtering.py</code> <pre><code>class GraphFiltering:\n    \"\"\"\n    GraphFiltering Class\n\n    This class implements graph-based filtering techniques for recommendation systems.\n    Graph-based filtering leverages the structure of a graph to make recommendations by\n    analyzing the relationships between users and items.\n\n    Attributes:\n        graph (Graph): The graph structure representing users and items.\n        similarity_metric (str): The metric used to calculate similarity between nodes.\n        damping_factor (float): The damping factor used in algorithms like PageRank.\n        max_iterations (int): Maximum number of iterations for convergence in iterative algorithms.\n\n    Methods:\n        build_graph(data):\n            Constructs the graph from the given data, where nodes represent users and items,\n            and edges represent interactions or similarities.\n\n        compute_similarity():\n            Computes similarity scores between nodes using the specified similarity metric.\n\n        recommend(user_id, top_n=10):\n            Generates top-N recommendations for a given user by analyzing the graph structure.\n\n        update_graph(new_data):\n            Updates the graph with new interactions or changes in the data, allowing for\n            dynamic recommendations.\n    \"\"\"\n    def __init__(self):\n        self.graph = None\n\n    def generate_graph(self, num_people, file_path=\"graph_dataset.csv\", seed=None):\n        \"\"\"\n        Generate a random graph and save it to a file.\n        \"\"\"\n        self.graph = generate_random_graph(num_people, file_path, seed)\n        return self.graph\n\n    def generate_large_graph(self, num_people, file_path=\"large_random_graph.csv\", seed=None):\n        \"\"\"\n        Generate a large random graph using multiprocessing.\n        \"\"\"\n        self.graph = generate_large_random_graph(num_people, file_path, seed)\n        return self.graph\n\n    def scale_and_save(self, input_file, output_dir, num_matrices):\n        \"\"\"\n        Scale and save matrices to the specified directory.\n        \"\"\"\n        scale_and_save_matrices(input_file, output_dir, num_matrices)\n</code></pre>"},{"location":"contentFilterEngine/graph_based_algorithms/graph_filtering/#engines.contentFilterEngine.graph_based_algorithms.graph_filtering.GraphFiltering.generate_graph","title":"<code>generate_graph(num_people, file_path='graph_dataset.csv', seed=None)</code>","text":"<p>Generate a random graph and save it to a file.</p> Source code in <code>engines/contentFilterEngine/graph_based_algorithms/graph_filtering.py</code> <pre><code>def generate_graph(self, num_people, file_path=\"graph_dataset.csv\", seed=None):\n    \"\"\"\n    Generate a random graph and save it to a file.\n    \"\"\"\n    self.graph = generate_random_graph(num_people, file_path, seed)\n    return self.graph\n</code></pre>"},{"location":"contentFilterEngine/graph_based_algorithms/graph_filtering/#engines.contentFilterEngine.graph_based_algorithms.graph_filtering.GraphFiltering.generate_large_graph","title":"<code>generate_large_graph(num_people, file_path='large_random_graph.csv', seed=None)</code>","text":"<p>Generate a large random graph using multiprocessing.</p> Source code in <code>engines/contentFilterEngine/graph_based_algorithms/graph_filtering.py</code> <pre><code>def generate_large_graph(self, num_people, file_path=\"large_random_graph.csv\", seed=None):\n    \"\"\"\n    Generate a large random graph using multiprocessing.\n    \"\"\"\n    self.graph = generate_large_random_graph(num_people, file_path, seed)\n    return self.graph\n</code></pre>"},{"location":"contentFilterEngine/graph_based_algorithms/graph_filtering/#engines.contentFilterEngine.graph_based_algorithms.graph_filtering.GraphFiltering.scale_and_save","title":"<code>scale_and_save(input_file, output_dir, num_matrices)</code>","text":"<p>Scale and save matrices to the specified directory.</p> Source code in <code>engines/contentFilterEngine/graph_based_algorithms/graph_filtering.py</code> <pre><code>def scale_and_save(self, input_file, output_dir, num_matrices):\n    \"\"\"\n    Scale and save matrices to the specified directory.\n    \"\"\"\n    scale_and_save_matrices(input_file, output_dir, num_matrices)\n</code></pre>"},{"location":"contentFilterEngine/graph_based_algorithms/semantic_models/","title":"Semantic Models","text":""},{"location":"contentFilterEngine/graph_based_algorithms/semantic_models/#engines.contentFilterEngine.graph_based_algorithms.semantic_models.SemanticModels","title":"<code>SemanticModels</code>","text":"<p>SemanticModels Class</p> <p>This class implements semantic models for graph-based recommendation systems. Semantic models enhance traditional graph-based methods by incorporating semantic information, such as item content or user profiles, into the recommendation process.</p> <p>Attributes:</p> Name Type Description <code>semantic_graph</code> <code>Graph</code> <p>The graph structure enriched with semantic information.</p> <code>embedding_dim</code> <code>int</code> <p>Dimensionality of semantic embeddings.</p> <code>similarity_threshold</code> <code>float</code> <p>Threshold for determining similarity between nodes.</p> <code>max_iterations</code> <code>int</code> <p>Maximum number of iterations for convergence in semantic algorithms.</p> <p>Methods:</p> Name Description <code>build_semantic_graph</code> <p>Constructs a semantic graph by integrating semantic information into the existing graph structure, enhancing node representations.</p> <code>compute_semantic_similarity</code> <p>Computes similarity scores between nodes using semantic information, improving recommendation accuracy.</p> <code>recommend</code> <p>Generates top-N recommendations for a given user by analyzing the semantic graph structure and node similarities.</p> <code>update_semantic_info</code> <p>Updates the semantic information in the graph, allowing for dynamic and context-aware recommendations.</p> Source code in <code>engines/contentFilterEngine/graph_based_algorithms/semantic_models.py</code> <pre><code>class SemanticModels:\n    \"\"\"\n    SemanticModels Class\n\n    This class implements semantic models for graph-based recommendation systems.\n    Semantic models enhance traditional graph-based methods by incorporating semantic\n    information, such as item content or user profiles, into the recommendation process.\n\n    Attributes:\n        semantic_graph (Graph): The graph structure enriched with semantic information.\n        embedding_dim (int): Dimensionality of semantic embeddings.\n        similarity_threshold (float): Threshold for determining similarity between nodes.\n        max_iterations (int): Maximum number of iterations for convergence in semantic algorithms.\n\n    Methods:\n        build_semantic_graph(data, semantic_info):\n            Constructs a semantic graph by integrating semantic information into the\n            existing graph structure, enhancing node representations.\n\n        compute_semantic_similarity():\n            Computes similarity scores between nodes using semantic information, improving\n            recommendation accuracy.\n\n        recommend(user_id, top_n=10):\n            Generates top-N recommendations for a given user by analyzing the semantic\n            graph structure and node similarities.\n\n        update_semantic_info(new_semantic_data):\n            Updates the semantic information in the graph, allowing for dynamic and\n            context-aware recommendations.\n    \"\"\"\n    def __init__(self):\n        self.graph = []\n\n    def set_graph(self, graph):\n        \"\"\"\n        Set the graph for semantic models.\n        \"\"\"\n        self.graph = graph\n\n    def find_optimal_path(self, start_city):\n        \"\"\"\n        Find the optimal path using the run_optimal_path function.\n        \"\"\"\n        run_optimal_path(self.graph, start_city)\n</code></pre>"},{"location":"contentFilterEngine/graph_based_algorithms/semantic_models/#engines.contentFilterEngine.graph_based_algorithms.semantic_models.SemanticModels.find_optimal_path","title":"<code>find_optimal_path(start_city)</code>","text":"<p>Find the optimal path using the run_optimal_path function.</p> Source code in <code>engines/contentFilterEngine/graph_based_algorithms/semantic_models.py</code> <pre><code>def find_optimal_path(self, start_city):\n    \"\"\"\n    Find the optimal path using the run_optimal_path function.\n    \"\"\"\n    run_optimal_path(self.graph, start_city)\n</code></pre>"},{"location":"contentFilterEngine/graph_based_algorithms/semantic_models/#engines.contentFilterEngine.graph_based_algorithms.semantic_models.SemanticModels.set_graph","title":"<code>set_graph(graph)</code>","text":"<p>Set the graph for semantic models.</p> Source code in <code>engines/contentFilterEngine/graph_based_algorithms/semantic_models.py</code> <pre><code>def set_graph(self, graph):\n    \"\"\"\n    Set the graph for semantic models.\n    \"\"\"\n    self.graph = graph\n</code></pre>"},{"location":"contentFilterEngine/hybrid_ensemble_methods/attention_mechanisms/","title":"Attention Mechanisms","text":"<p>Attention Mechanisms for Hybrid Recommendation Systems</p> <p>This module provides implementations of attention mechanisms tailored for hybrid recommendation systems. Attention mechanisms allow the model to focus on relevant parts of the input data, improving the quality of recommendations by considering contextual and sequential information.</p> <p>Key Features: - Implements self-attention and multi-head attention mechanisms. - Enhances hybrid models by integrating attention layers. - Supports attention-based feature extraction and representation learning.</p> <ul> <li>ATTENTION_MECHANISMS: Core class for implementing attention mechanisms in hybrid   recommendation systems.</li> </ul> <p>Usage: Instantiate the ATTENTION_MECHANISMS class to integrate attention layers into your hybrid recommendation model. Use the provided methods to train and apply attention mechanisms to your data.</p> Example <p>attention_model = ATTENTION_MECHANISMS() attention_model.train(user_item_matrix, attention_features) enhanced_recommendations = attention_model.recommend(user_id, top_n=10)</p>"},{"location":"contentFilterEngine/hybrid_ensemble_methods/ensemble_methods/","title":"Hybrid Ensemble","text":"<p>Ensemble Methods for Hybrid Recommendation Systems</p> <p>This module implements ensemble methods that combine multiple recommendation models to improve overall system performance. Ensemble methods leverage the diversity of different models to enhance prediction accuracy and robustness.</p> <p>Key Features: - Supports various ensemble strategies, including bagging, boosting, and stacking. - Combines outputs from multiple models to generate final recommendations. - Provides flexibility in model selection and ensemble configuration.</p> <ul> <li>ENSEMBLE_METHODS: Main class implementing ensemble techniques for hybrid   recommendation systems.</li> </ul> <p>Usage: Create an instance of the ENSEMBLE_METHODS class to configure and apply ensemble techniques to your recommendation models. Use the provided methods to train and generate ensemble-based recommendations.</p> Example <p>ensemble_model = ENSEMBLE_METHODS() ensemble_model.train(models_list, user_item_matrix) final_recommendations = ensemble_model.recommend(user_id, top_n=10)</p>"},{"location":"contentFilterEngine/hybrid_ensemble_methods/hybrid_collaborative/","title":"Hybrid Collaborative","text":"<p>Hybrid Collaborative Filtering Module</p> <p>This module implements hybrid collaborative filtering techniques that combine multiple recommendation strategies to improve accuracy and robustness. Hybrid methods leverage the strengths of different algorithms, such as collaborative filtering, content-based filtering, and others, to provide more personalized recommendations.</p> <p>Key Features: - Combines collaborative and content-based filtering methods. - Utilizes ensemble techniques to enhance recommendation performance. - Supports various hybridization strategies, including weighted, switching, and mixed   hybrid approaches.</p> <p>Classes: - HYBRID_COLLABORATIVE: Main class implementing hybrid collaborative filtering logic.</p> <p>Usage: To use this module, instantiate the HYBRID_COLLABORATIVE class and call its methods to train and generate recommendations based on your dataset.</p> Example <p>hybrid_cf = HYBRID_COLLABORATIVE() hybrid_cf.train(user_item_matrix, content_features) recommendations = hybrid_cf.recommend(user_id, top_n=10)</p>"},{"location":"contentFilterEngine/learning_paradigms/few_shot/","title":"Few Shot Learning","text":""},{"location":"contentFilterEngine/learning_paradigms/few_shot/#engines.contentFilterEngine.learning_paradigms.few_shot.FewShotLearner","title":"<code>FewShotLearner</code>","text":"Source code in <code>engines/contentFilterEngine/learning_paradigms/few_shot.py</code> <pre><code>class FewShotLearner:\n    def __init__(self, model, data_loader, criterion, optimizer, num_epochs):\n        \"\"\"\n        Initializes the FewShotLearner with the given model, data loader, criterion, optimizer, and number of epochs.\n\n        Parameters:\n            model (torch.nn.Module): The model to be trained and used for predictions.\n            data_loader (torch.utils.data.DataLoader): DataLoader providing the training data.\n            criterion (torch.nn.Module): Loss function used to evaluate the model's performance.\n            optimizer (torch.optim.Optimizer): Optimization algorithm used to update model weights.\n            num_epochs (int): Number of epochs to train the model.\n        \"\"\"\n        self.model = model\n        self.data_loader = data_loader\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.num_epochs = num_epochs\n\n    def train(self):\n        \"\"\"\n        Trains the model using the few-shot learning approach.\n\n        This method iterates over the data provided by the data_loader for a specified number of epochs,\n        updating the model's weights using the optimizer and evaluating its performance using the criterion.\n        \"\"\"\n        print(\"Training with Few-Shot Learning...\")\n        train_model(self.model, self.data_loader, self.criterion, self.optimizer, self.num_epochs)\n\n    def predict(self, graph, node_index, top_k=5, threshold=0.5):\n        \"\"\"\n        Predicts the top-k items for a given node in a graph using the trained model.\n\n        Parameters:\n            graph (torch.Tensor): The graph data structure containing nodes and edges.\n            node_index (int): The index of the node for which predictions are to be made.\n            top_k (int, optional): The number of top items to return. Defaults to 5.\n            threshold (float, optional): The threshold for prediction confidence. Defaults to 0.5.\n\n        Returns:\n            List[int]: A list of indices representing the top-k predicted items.\n        \"\"\"\n        print(\"Predicting with Few-Shot Learning...\")\n        return predict(self.model, graph, node_index, top_k, threshold)\n</code></pre>"},{"location":"contentFilterEngine/learning_paradigms/few_shot/#engines.contentFilterEngine.learning_paradigms.few_shot.FewShotLearner.__init__","title":"<code>__init__(model, data_loader, criterion, optimizer, num_epochs)</code>","text":"<p>Initializes the FewShotLearner with the given model, data loader, criterion, optimizer, and number of epochs.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be trained and used for predictions.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader providing the training data.</p> required <code>criterion</code> <code>Module</code> <p>Loss function used to evaluate the model's performance.</p> required <code>optimizer</code> <code>Optimizer</code> <p>Optimization algorithm used to update model weights.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs to train the model.</p> required Source code in <code>engines/contentFilterEngine/learning_paradigms/few_shot.py</code> <pre><code>def __init__(self, model, data_loader, criterion, optimizer, num_epochs):\n    \"\"\"\n    Initializes the FewShotLearner with the given model, data loader, criterion, optimizer, and number of epochs.\n\n    Parameters:\n        model (torch.nn.Module): The model to be trained and used for predictions.\n        data_loader (torch.utils.data.DataLoader): DataLoader providing the training data.\n        criterion (torch.nn.Module): Loss function used to evaluate the model's performance.\n        optimizer (torch.optim.Optimizer): Optimization algorithm used to update model weights.\n        num_epochs (int): Number of epochs to train the model.\n    \"\"\"\n    self.model = model\n    self.data_loader = data_loader\n    self.criterion = criterion\n    self.optimizer = optimizer\n    self.num_epochs = num_epochs\n</code></pre>"},{"location":"contentFilterEngine/learning_paradigms/few_shot/#engines.contentFilterEngine.learning_paradigms.few_shot.FewShotLearner.predict","title":"<code>predict(graph, node_index, top_k=5, threshold=0.5)</code>","text":"<p>Predicts the top-k items for a given node in a graph using the trained model.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Tensor</code> <p>The graph data structure containing nodes and edges.</p> required <code>node_index</code> <code>int</code> <p>The index of the node for which predictions are to be made.</p> required <code>top_k</code> <code>int</code> <p>The number of top items to return. Defaults to 5.</p> <code>5</code> <code>threshold</code> <code>float</code> <p>The threshold for prediction confidence. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <p>List[int]: A list of indices representing the top-k predicted items.</p> Source code in <code>engines/contentFilterEngine/learning_paradigms/few_shot.py</code> <pre><code>def predict(self, graph, node_index, top_k=5, threshold=0.5):\n    \"\"\"\n    Predicts the top-k items for a given node in a graph using the trained model.\n\n    Parameters:\n        graph (torch.Tensor): The graph data structure containing nodes and edges.\n        node_index (int): The index of the node for which predictions are to be made.\n        top_k (int, optional): The number of top items to return. Defaults to 5.\n        threshold (float, optional): The threshold for prediction confidence. Defaults to 0.5.\n\n    Returns:\n        List[int]: A list of indices representing the top-k predicted items.\n    \"\"\"\n    print(\"Predicting with Few-Shot Learning...\")\n    return predict(self.model, graph, node_index, top_k, threshold)\n</code></pre>"},{"location":"contentFilterEngine/learning_paradigms/few_shot/#engines.contentFilterEngine.learning_paradigms.few_shot.FewShotLearner.train","title":"<code>train()</code>","text":"<p>Trains the model using the few-shot learning approach.</p> <p>This method iterates over the data provided by the data_loader for a specified number of epochs, updating the model's weights using the optimizer and evaluating its performance using the criterion.</p> Source code in <code>engines/contentFilterEngine/learning_paradigms/few_shot.py</code> <pre><code>def train(self):\n    \"\"\"\n    Trains the model using the few-shot learning approach.\n\n    This method iterates over the data provided by the data_loader for a specified number of epochs,\n    updating the model's weights using the optimizer and evaluating its performance using the criterion.\n    \"\"\"\n    print(\"Training with Few-Shot Learning...\")\n    train_model(self.model, self.data_loader, self.criterion, self.optimizer, self.num_epochs)\n</code></pre>"},{"location":"contentFilterEngine/learning_paradigms/meta_learning/","title":"Meta Learning","text":""},{"location":"contentFilterEngine/learning_paradigms/meta_learning/#engines.contentFilterEngine.learning_paradigms.meta_learning.MetaLearner","title":"<code>MetaLearner</code>","text":"Source code in <code>engines/contentFilterEngine/learning_paradigms/meta_learning.py</code> <pre><code>class MetaLearner:\n    def __init__(self, model, data_loader, criterion, optimizer, num_epochs):\n        \"\"\"\n        Initializes the MetaLearner with the given model, data loader, criterion, optimizer, and number of epochs.\n\n        Parameters:\n            model (torch.nn.Module): The model to be trained and used for predictions.\n            data_loader (torch.utils.data.DataLoader): DataLoader providing the training data.\n            criterion (torch.nn.Module): Loss function used to evaluate the model's performance.\n            optimizer (torch.optim.Optimizer): Optimization algorithm used to update model weights.\n            num_epochs (int): Number of epochs to train the model.\n        \"\"\"\n        self.model = model\n        self.data_loader = data_loader\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.num_epochs = num_epochs\n\n    def train(self):\n        \"\"\"\n        Trains the model using the meta-learning approach.\n\n        This method iterates over the data provided by the data_loader for a specified number of epochs,\n        updating the model's weights using the optimizer and evaluating its performance using the criterion.\n        \"\"\"\n        print(\"Training with Meta-Learning...\")\n        train_model(self.model, self.data_loader, self.criterion, self.optimizer, self.num_epochs)\n\n    def predict(self, graph, node_index, top_k=5, threshold=0.5):\n        \"\"\"\n        Predicts the top-k items for a given node in a graph using the trained model.\n\n        Parameters:\n            graph (torch.Tensor): The graph data structure containing nodes and edges.\n            node_index (int): The index of the node for which predictions are to be made.\n            top_k (int, optional): The number of top items to return. Defaults to 5.\n            threshold (float, optional): The threshold for prediction confidence. Defaults to 0.5.\n\n        Returns:\n            List[int]: A list of indices representing the top-k predicted items.\n        \"\"\"\n        print(\"Predicting with Meta-Learning...\")\n        return predict(self.model, graph, node_index, top_k, threshold)\n</code></pre>"},{"location":"contentFilterEngine/learning_paradigms/meta_learning/#engines.contentFilterEngine.learning_paradigms.meta_learning.MetaLearner.__init__","title":"<code>__init__(model, data_loader, criterion, optimizer, num_epochs)</code>","text":"<p>Initializes the MetaLearner with the given model, data loader, criterion, optimizer, and number of epochs.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be trained and used for predictions.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader providing the training data.</p> required <code>criterion</code> <code>Module</code> <p>Loss function used to evaluate the model's performance.</p> required <code>optimizer</code> <code>Optimizer</code> <p>Optimization algorithm used to update model weights.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs to train the model.</p> required Source code in <code>engines/contentFilterEngine/learning_paradigms/meta_learning.py</code> <pre><code>def __init__(self, model, data_loader, criterion, optimizer, num_epochs):\n    \"\"\"\n    Initializes the MetaLearner with the given model, data loader, criterion, optimizer, and number of epochs.\n\n    Parameters:\n        model (torch.nn.Module): The model to be trained and used for predictions.\n        data_loader (torch.utils.data.DataLoader): DataLoader providing the training data.\n        criterion (torch.nn.Module): Loss function used to evaluate the model's performance.\n        optimizer (torch.optim.Optimizer): Optimization algorithm used to update model weights.\n        num_epochs (int): Number of epochs to train the model.\n    \"\"\"\n    self.model = model\n    self.data_loader = data_loader\n    self.criterion = criterion\n    self.optimizer = optimizer\n    self.num_epochs = num_epochs\n</code></pre>"},{"location":"contentFilterEngine/learning_paradigms/meta_learning/#engines.contentFilterEngine.learning_paradigms.meta_learning.MetaLearner.predict","title":"<code>predict(graph, node_index, top_k=5, threshold=0.5)</code>","text":"<p>Predicts the top-k items for a given node in a graph using the trained model.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Tensor</code> <p>The graph data structure containing nodes and edges.</p> required <code>node_index</code> <code>int</code> <p>The index of the node for which predictions are to be made.</p> required <code>top_k</code> <code>int</code> <p>The number of top items to return. Defaults to 5.</p> <code>5</code> <code>threshold</code> <code>float</code> <p>The threshold for prediction confidence. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <p>List[int]: A list of indices representing the top-k predicted items.</p> Source code in <code>engines/contentFilterEngine/learning_paradigms/meta_learning.py</code> <pre><code>def predict(self, graph, node_index, top_k=5, threshold=0.5):\n    \"\"\"\n    Predicts the top-k items for a given node in a graph using the trained model.\n\n    Parameters:\n        graph (torch.Tensor): The graph data structure containing nodes and edges.\n        node_index (int): The index of the node for which predictions are to be made.\n        top_k (int, optional): The number of top items to return. Defaults to 5.\n        threshold (float, optional): The threshold for prediction confidence. Defaults to 0.5.\n\n    Returns:\n        List[int]: A list of indices representing the top-k predicted items.\n    \"\"\"\n    print(\"Predicting with Meta-Learning...\")\n    return predict(self.model, graph, node_index, top_k, threshold)\n</code></pre>"},{"location":"contentFilterEngine/learning_paradigms/meta_learning/#engines.contentFilterEngine.learning_paradigms.meta_learning.MetaLearner.train","title":"<code>train()</code>","text":"<p>Trains the model using the meta-learning approach.</p> <p>This method iterates over the data provided by the data_loader for a specified number of epochs, updating the model's weights using the optimizer and evaluating its performance using the criterion.</p> Source code in <code>engines/contentFilterEngine/learning_paradigms/meta_learning.py</code> <pre><code>def train(self):\n    \"\"\"\n    Trains the model using the meta-learning approach.\n\n    This method iterates over the data provided by the data_loader for a specified number of epochs,\n    updating the model's weights using the optimizer and evaluating its performance using the criterion.\n    \"\"\"\n    print(\"Training with Meta-Learning...\")\n    train_model(self.model, self.data_loader, self.criterion, self.optimizer, self.num_epochs)\n</code></pre>"},{"location":"contentFilterEngine/learning_paradigms/transfer_learning/","title":"Transfer Learning","text":""},{"location":"contentFilterEngine/learning_paradigms/transfer_learning/#engines.contentFilterEngine.learning_paradigms.transfer_learning.TransferLearningLearner","title":"<code>TransferLearningLearner</code>","text":"Source code in <code>engines/contentFilterEngine/learning_paradigms/transfer_learning.py</code> <pre><code>class TransferLearningLearner:\n    def __init__(self, model, data_loader, criterion, optimizer, num_epochs):\n        \"\"\"\n        Initializes the TransferLearningLearner with the given model, data loader, criterion, optimizer, and number of epochs.\n\n        Parameters:\n            model (torch.nn.Module): The model to be trained and used for predictions.\n            data_loader (torch.utils.data.DataLoader): DataLoader providing the training data.\n            criterion (torch.nn.Module): Loss function used to evaluate the model's performance.\n            optimizer (torch.optim.Optimizer): Optimization algorithm used to update model weights.\n            num_epochs (int): Number of epochs to train the model.\n        \"\"\"\n        self.model = model\n        self.data_loader = data_loader\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.num_epochs = num_epochs\n\n    def train(self):\n        \"\"\"\n        Trains the model using the transfer learning approach.\n\n        This method iterates over the data provided by the data_loader for a specified number of epochs,\n        updating the model's weights using the optimizer and evaluating its performance using the criterion.\n        \"\"\"\n        print(\"Training with Transfer Learning...\")\n        train_model(self.model, self.data_loader, self.criterion, self.optimizer, self.num_epochs)\n\n    def predict(self, graph, node_index, top_k=5, threshold=0.5):\n        \"\"\"\n        Predicts the top-k items for a given node in a graph using the trained model.\n\n        Parameters:\n            graph (torch.Tensor): The graph data structure containing nodes and edges.\n            node_index (int): The index of the node for which predictions are to be made.\n            top_k (int, optional): The number of top items to return. Defaults to 5.\n            threshold (float, optional): The threshold for prediction confidence. Defaults to 0.5.\n\n        Returns:\n            List[int]: A list of indices representing the top-k predicted items.\n        \"\"\"\n        print(\"Predicting with Transfer Learning...\")\n        return predict(self.model, graph, node_index, top_k, threshold)\n</code></pre>"},{"location":"contentFilterEngine/learning_paradigms/transfer_learning/#engines.contentFilterEngine.learning_paradigms.transfer_learning.TransferLearningLearner.__init__","title":"<code>__init__(model, data_loader, criterion, optimizer, num_epochs)</code>","text":"<p>Initializes the TransferLearningLearner with the given model, data loader, criterion, optimizer, and number of epochs.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be trained and used for predictions.</p> required <code>data_loader</code> <code>DataLoader</code> <p>DataLoader providing the training data.</p> required <code>criterion</code> <code>Module</code> <p>Loss function used to evaluate the model's performance.</p> required <code>optimizer</code> <code>Optimizer</code> <p>Optimization algorithm used to update model weights.</p> required <code>num_epochs</code> <code>int</code> <p>Number of epochs to train the model.</p> required Source code in <code>engines/contentFilterEngine/learning_paradigms/transfer_learning.py</code> <pre><code>def __init__(self, model, data_loader, criterion, optimizer, num_epochs):\n    \"\"\"\n    Initializes the TransferLearningLearner with the given model, data loader, criterion, optimizer, and number of epochs.\n\n    Parameters:\n        model (torch.nn.Module): The model to be trained and used for predictions.\n        data_loader (torch.utils.data.DataLoader): DataLoader providing the training data.\n        criterion (torch.nn.Module): Loss function used to evaluate the model's performance.\n        optimizer (torch.optim.Optimizer): Optimization algorithm used to update model weights.\n        num_epochs (int): Number of epochs to train the model.\n    \"\"\"\n    self.model = model\n    self.data_loader = data_loader\n    self.criterion = criterion\n    self.optimizer = optimizer\n    self.num_epochs = num_epochs\n</code></pre>"},{"location":"contentFilterEngine/learning_paradigms/transfer_learning/#engines.contentFilterEngine.learning_paradigms.transfer_learning.TransferLearningLearner.predict","title":"<code>predict(graph, node_index, top_k=5, threshold=0.5)</code>","text":"<p>Predicts the top-k items for a given node in a graph using the trained model.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Tensor</code> <p>The graph data structure containing nodes and edges.</p> required <code>node_index</code> <code>int</code> <p>The index of the node for which predictions are to be made.</p> required <code>top_k</code> <code>int</code> <p>The number of top items to return. Defaults to 5.</p> <code>5</code> <code>threshold</code> <code>float</code> <p>The threshold for prediction confidence. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <p>List[int]: A list of indices representing the top-k predicted items.</p> Source code in <code>engines/contentFilterEngine/learning_paradigms/transfer_learning.py</code> <pre><code>def predict(self, graph, node_index, top_k=5, threshold=0.5):\n    \"\"\"\n    Predicts the top-k items for a given node in a graph using the trained model.\n\n    Parameters:\n        graph (torch.Tensor): The graph data structure containing nodes and edges.\n        node_index (int): The index of the node for which predictions are to be made.\n        top_k (int, optional): The number of top items to return. Defaults to 5.\n        threshold (float, optional): The threshold for prediction confidence. Defaults to 0.5.\n\n    Returns:\n        List[int]: A list of indices representing the top-k predicted items.\n    \"\"\"\n    print(\"Predicting with Transfer Learning...\")\n    return predict(self.model, graph, node_index, top_k, threshold)\n</code></pre>"},{"location":"contentFilterEngine/learning_paradigms/transfer_learning/#engines.contentFilterEngine.learning_paradigms.transfer_learning.TransferLearningLearner.train","title":"<code>train()</code>","text":"<p>Trains the model using the transfer learning approach.</p> <p>This method iterates over the data provided by the data_loader for a specified number of epochs, updating the model's weights using the optimizer and evaluating its performance using the criterion.</p> Source code in <code>engines/contentFilterEngine/learning_paradigms/transfer_learning.py</code> <pre><code>def train(self):\n    \"\"\"\n    Trains the model using the transfer learning approach.\n\n    This method iterates over the data provided by the data_loader for a specified number of epochs,\n    updating the model's weights using the optimizer and evaluating its performance using the criterion.\n    \"\"\"\n    print(\"Training with Transfer Learning...\")\n    train_model(self.model, self.data_loader, self.criterion, self.optimizer, self.num_epochs)\n</code></pre>"},{"location":"contentFilterEngine/learning_paradigms/zero_shot/","title":"Zero Shot Learning","text":""},{"location":"contentFilterEngine/learning_paradigms/zero_shot/#engines.contentFilterEngine.learning_paradigms.zero_shot.ZeroShotLearner","title":"<code>ZeroShotLearner</code>","text":"Source code in <code>engines/contentFilterEngine/learning_paradigms/zero_shot.py</code> <pre><code>class ZeroShotLearner:\n    def __init__(self, model):\n        \"\"\"\n        Initializes the ZeroShotLearner with the given model.\n\n        Parameters:\n            model (torch.nn.Module): The model to be used for predictions.\n        \"\"\"\n        self.model = model\n\n    def predict(self, graph, node_index, top_k=5, threshold=0.5):\n        \"\"\"\n        Predicts the top-k items for a given node in a graph using the model.\n\n        Parameters:\n            graph (torch.Tensor): The graph data structure containing nodes and edges.\n            node_index (int): The index of the node for which predictions are to be made.\n            top_k (int, optional): The number of top items to return. Defaults to 5.\n            threshold (float, optional): The threshold for prediction confidence. Defaults to 0.5.\n\n        Returns:\n            List[int]: A list of indices representing the top-k predicted items.\n        \"\"\"\n        print(\"Predicting with Zero-Shot Learning...\")\n        return predict(self.model, graph, node_index, top_k, threshold)\n</code></pre>"},{"location":"contentFilterEngine/learning_paradigms/zero_shot/#engines.contentFilterEngine.learning_paradigms.zero_shot.ZeroShotLearner.__init__","title":"<code>__init__(model)</code>","text":"<p>Initializes the ZeroShotLearner with the given model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>The model to be used for predictions.</p> required Source code in <code>engines/contentFilterEngine/learning_paradigms/zero_shot.py</code> <pre><code>def __init__(self, model):\n    \"\"\"\n    Initializes the ZeroShotLearner with the given model.\n\n    Parameters:\n        model (torch.nn.Module): The model to be used for predictions.\n    \"\"\"\n    self.model = model\n</code></pre>"},{"location":"contentFilterEngine/learning_paradigms/zero_shot/#engines.contentFilterEngine.learning_paradigms.zero_shot.ZeroShotLearner.predict","title":"<code>predict(graph, node_index, top_k=5, threshold=0.5)</code>","text":"<p>Predicts the top-k items for a given node in a graph using the model.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Tensor</code> <p>The graph data structure containing nodes and edges.</p> required <code>node_index</code> <code>int</code> <p>The index of the node for which predictions are to be made.</p> required <code>top_k</code> <code>int</code> <p>The number of top items to return. Defaults to 5.</p> <code>5</code> <code>threshold</code> <code>float</code> <p>The threshold for prediction confidence. Defaults to 0.5.</p> <code>0.5</code> <p>Returns:</p> Type Description <p>List[int]: A list of indices representing the top-k predicted items.</p> Source code in <code>engines/contentFilterEngine/learning_paradigms/zero_shot.py</code> <pre><code>def predict(self, graph, node_index, top_k=5, threshold=0.5):\n    \"\"\"\n    Predicts the top-k items for a given node in a graph using the model.\n\n    Parameters:\n        graph (torch.Tensor): The graph data structure containing nodes and edges.\n        node_index (int): The index of the node for which predictions are to be made.\n        top_k (int, optional): The number of top items to return. Defaults to 5.\n        threshold (float, optional): The threshold for prediction confidence. Defaults to 0.5.\n\n    Returns:\n        List[int]: A list of indices representing the top-k predicted items.\n    \"\"\"\n    print(\"Predicting with Zero-Shot Learning...\")\n    return predict(self.model, graph, node_index, top_k, threshold)\n</code></pre>"},{"location":"contentFilterEngine/miscellaneous_techniques/cold_start/","title":"Cold Start","text":""},{"location":"contentFilterEngine/miscellaneous_techniques/cold_start/#engines.contentFilterEngine.miscellaneous_techniques.cold_start.COLD_START","title":"<code>COLD_START</code>","text":"Source code in <code>engines/contentFilterEngine/miscellaneous_techniques/cold_start.py</code> <pre><code>class COLD_START:\n    def __init__(self, method='content_based', n_neighbors=5):\n        \"\"\"\n        Initialize cold start handling.\n\n        Args:\n            method (str): Cold start strategy ('content_based', 'popularity', 'hybrid')\n            n_neighbors (int): Number of neighbors for similarity-based methods\n        \"\"\"\n        self.method = method\n        self.n_neighbors = n_neighbors\n        self.scaler = StandardScaler()\n        self.item_features = None\n        self.popularity_scores = None\n        self.knn = None\n\n    def fit(self, item_features, interaction_matrix=None):\n        \"\"\"\n        Fit the cold start handler.\n\n        Args:\n            item_features: Content features of items\n            interaction_matrix: User-item interaction matrix (optional)\n        \"\"\"\n        self.item_features = self.scaler.fit_transform(item_features)\n\n        if interaction_matrix is not None:\n            self.popularity_scores = np.sum(interaction_matrix, axis=0)\n\n        if self.method in ['content_based', 'hybrid']:\n            self.knn = NearestNeighbors(n_neighbors=self.n_neighbors)\n            self.knn.fit(self.item_features)\n\n    def recommend_for_new_user(self, user_profile=None):\n        \"\"\"\n        Generate recommendations for a new user.\n\n        Args:\n            user_profile: User preferences or features (optional)\n        \"\"\"\n        if self.method == 'content_based' and user_profile is not None:\n            user_profile = self.scaler.transform([user_profile])\n            _, indices = self.knn.kneighbors(user_profile)\n            return indices[0]\n\n        elif self.method == 'popularity':\n            if self.popularity_scores is None:\n                raise ValueError(\"Popularity scores not available\")\n            return np.argsort(self.popularity_scores)[-self.n_neighbors:]\n\n        elif self.method == 'hybrid':\n            content_based_scores = self._get_content_based_scores(user_profile)\n            popularity_weights = self._normalize(self.popularity_scores)\n\n            # Ensure both arrays have the same shape\n            if content_based_scores.shape != popularity_weights.shape:\n                # Align the shapes by selecting the top items based on content-based scores\n                top_indices = np.argsort(content_based_scores)[-len(popularity_weights):]\n                content_based_scores = content_based_scores[top_indices]\n                popularity_weights = popularity_weights[top_indices]\n\n            hybrid_scores = 0.7 * content_based_scores + 0.3 * popularity_weights\n            return np.argsort(hybrid_scores)[-self.n_neighbors:]\n\n    def _get_content_based_scores(self, user_profile):\n        \"\"\"Calculate content-based similarity scores.\"\"\"\n        if user_profile is None:\n            return np.zeros(self.item_features.shape[0])\n        user_profile = self.scaler.transform([user_profile])\n        distances, _ = self.knn.kneighbors(user_profile)\n        return self._normalize(1 / (1 + distances[0]))\n\n    def _normalize(self, scores):\n        \"\"\"Normalize scores to [0, 1] range.\"\"\"\n        min_score = np.min(scores)\n        max_score = np.max(scores)\n        if min_score == max_score:\n            return np.zeros_like(scores)  # Avoid division by zero\n        return (scores - min_score) / (max_score - min_score)\n</code></pre>"},{"location":"contentFilterEngine/miscellaneous_techniques/cold_start/#engines.contentFilterEngine.miscellaneous_techniques.cold_start.COLD_START.__init__","title":"<code>__init__(method='content_based', n_neighbors=5)</code>","text":"<p>Initialize cold start handling.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Cold start strategy ('content_based', 'popularity', 'hybrid')</p> <code>'content_based'</code> <code>n_neighbors</code> <code>int</code> <p>Number of neighbors for similarity-based methods</p> <code>5</code> Source code in <code>engines/contentFilterEngine/miscellaneous_techniques/cold_start.py</code> <pre><code>def __init__(self, method='content_based', n_neighbors=5):\n    \"\"\"\n    Initialize cold start handling.\n\n    Args:\n        method (str): Cold start strategy ('content_based', 'popularity', 'hybrid')\n        n_neighbors (int): Number of neighbors for similarity-based methods\n    \"\"\"\n    self.method = method\n    self.n_neighbors = n_neighbors\n    self.scaler = StandardScaler()\n    self.item_features = None\n    self.popularity_scores = None\n    self.knn = None\n</code></pre>"},{"location":"contentFilterEngine/miscellaneous_techniques/cold_start/#engines.contentFilterEngine.miscellaneous_techniques.cold_start.COLD_START.fit","title":"<code>fit(item_features, interaction_matrix=None)</code>","text":"<p>Fit the cold start handler.</p> <p>Parameters:</p> Name Type Description Default <code>item_features</code> <p>Content features of items</p> required <code>interaction_matrix</code> <p>User-item interaction matrix (optional)</p> <code>None</code> Source code in <code>engines/contentFilterEngine/miscellaneous_techniques/cold_start.py</code> <pre><code>def fit(self, item_features, interaction_matrix=None):\n    \"\"\"\n    Fit the cold start handler.\n\n    Args:\n        item_features: Content features of items\n        interaction_matrix: User-item interaction matrix (optional)\n    \"\"\"\n    self.item_features = self.scaler.fit_transform(item_features)\n\n    if interaction_matrix is not None:\n        self.popularity_scores = np.sum(interaction_matrix, axis=0)\n\n    if self.method in ['content_based', 'hybrid']:\n        self.knn = NearestNeighbors(n_neighbors=self.n_neighbors)\n        self.knn.fit(self.item_features)\n</code></pre>"},{"location":"contentFilterEngine/miscellaneous_techniques/cold_start/#engines.contentFilterEngine.miscellaneous_techniques.cold_start.COLD_START.recommend_for_new_user","title":"<code>recommend_for_new_user(user_profile=None)</code>","text":"<p>Generate recommendations for a new user.</p> <p>Parameters:</p> Name Type Description Default <code>user_profile</code> <p>User preferences or features (optional)</p> <code>None</code> Source code in <code>engines/contentFilterEngine/miscellaneous_techniques/cold_start.py</code> <pre><code>def recommend_for_new_user(self, user_profile=None):\n    \"\"\"\n    Generate recommendations for a new user.\n\n    Args:\n        user_profile: User preferences or features (optional)\n    \"\"\"\n    if self.method == 'content_based' and user_profile is not None:\n        user_profile = self.scaler.transform([user_profile])\n        _, indices = self.knn.kneighbors(user_profile)\n        return indices[0]\n\n    elif self.method == 'popularity':\n        if self.popularity_scores is None:\n            raise ValueError(\"Popularity scores not available\")\n        return np.argsort(self.popularity_scores)[-self.n_neighbors:]\n\n    elif self.method == 'hybrid':\n        content_based_scores = self._get_content_based_scores(user_profile)\n        popularity_weights = self._normalize(self.popularity_scores)\n\n        # Ensure both arrays have the same shape\n        if content_based_scores.shape != popularity_weights.shape:\n            # Align the shapes by selecting the top items based on content-based scores\n            top_indices = np.argsort(content_based_scores)[-len(popularity_weights):]\n            content_based_scores = content_based_scores[top_indices]\n            popularity_weights = popularity_weights[top_indices]\n\n        hybrid_scores = 0.7 * content_based_scores + 0.3 * popularity_weights\n        return np.argsort(hybrid_scores)[-self.n_neighbors:]\n</code></pre>"},{"location":"contentFilterEngine/miscellaneous_techniques/feature_selection/","title":"Feature Selection","text":""},{"location":"contentFilterEngine/miscellaneous_techniques/feature_selection/#engines.contentFilterEngine.miscellaneous_techniques.feature_selection.FEATURE_SELECTION","title":"<code>FEATURE_SELECTION</code>","text":"Source code in <code>engines/contentFilterEngine/miscellaneous_techniques/feature_selection.py</code> <pre><code>class FEATURE_SELECTION:\n    def __init__(self, k=10, method='chi2'):\n        \"\"\"\n        Initialize feature selection.\n\n        Args:\n            k (int): Number of top features to select\n            method (str): Feature selection method ('chi2', 'variance', 'correlation')\n        \"\"\"\n        self.k = k\n        self.method = method\n        self.selected_features = None\n        self.feature_scores = None\n        self.scaler = MinMaxScaler()\n\n    def fit_transform(self, X, y=None):\n        \"\"\"\n        Fit the feature selector and transform the data.\n\n        Args:\n            X: Input features\n            y: Target variables (optional for some methods)\n        \"\"\"\n        if self.method == 'chi2':\n            # Scale features to non-negative for chi2\n            X_scaled = self.scaler.fit_transform(X)\n            selector = SelectKBest(chi2, k=self.k)\n            X_selected = selector.fit_transform(X_scaled, y)\n            self.feature_scores = selector.scores_\n            self.selected_features = selector.get_support()\n            return X_selected\n\n        elif self.method == 'variance':\n            # Select features based on variance\n            variances = np.var(X, axis=0)\n            top_k_idx = np.argsort(variances)[-self.k:]\n            self.selected_features = np.zeros(X.shape[1], dtype=bool)\n            self.selected_features[top_k_idx] = True\n            self.feature_scores = variances\n            return X[:, top_k_idx]\n\n        elif self.method == 'correlation':\n            # Select features based on correlation with target\n            correlations = np.array([np.corrcoef(X[:, i], y)[0, 1] for i in range(X.shape[1])])\n            top_k_idx = np.argsort(np.abs(correlations))[-self.k:]\n            self.selected_features = np.zeros(X.shape[1], dtype=bool)\n            self.selected_features[top_k_idx] = True\n            self.feature_scores = correlations\n            return X[:, top_k_idx]\n\n    def transform(self, X):\n        \"\"\"Transform new data using selected features.\"\"\"\n        if self.selected_features is None:\n            raise ValueError(\"Fit the selector first using fit_transform()\")\n        return X[:, self.selected_features]\n\n    def get_feature_importance(self):\n        \"\"\"Return feature importance scores.\"\"\"\n        if self.feature_scores is None:\n            raise ValueError(\"Fit the selector first using fit_transform()\")\n        return self.feature_scores\n</code></pre>"},{"location":"contentFilterEngine/miscellaneous_techniques/feature_selection/#engines.contentFilterEngine.miscellaneous_techniques.feature_selection.FEATURE_SELECTION.__init__","title":"<code>__init__(k=10, method='chi2')</code>","text":"<p>Initialize feature selection.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>Number of top features to select</p> <code>10</code> <code>method</code> <code>str</code> <p>Feature selection method ('chi2', 'variance', 'correlation')</p> <code>'chi2'</code> Source code in <code>engines/contentFilterEngine/miscellaneous_techniques/feature_selection.py</code> <pre><code>def __init__(self, k=10, method='chi2'):\n    \"\"\"\n    Initialize feature selection.\n\n    Args:\n        k (int): Number of top features to select\n        method (str): Feature selection method ('chi2', 'variance', 'correlation')\n    \"\"\"\n    self.k = k\n    self.method = method\n    self.selected_features = None\n    self.feature_scores = None\n    self.scaler = MinMaxScaler()\n</code></pre>"},{"location":"contentFilterEngine/miscellaneous_techniques/feature_selection/#engines.contentFilterEngine.miscellaneous_techniques.feature_selection.FEATURE_SELECTION.fit_transform","title":"<code>fit_transform(X, y=None)</code>","text":"<p>Fit the feature selector and transform the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Input features</p> required <code>y</code> <p>Target variables (optional for some methods)</p> <code>None</code> Source code in <code>engines/contentFilterEngine/miscellaneous_techniques/feature_selection.py</code> <pre><code>def fit_transform(self, X, y=None):\n    \"\"\"\n    Fit the feature selector and transform the data.\n\n    Args:\n        X: Input features\n        y: Target variables (optional for some methods)\n    \"\"\"\n    if self.method == 'chi2':\n        # Scale features to non-negative for chi2\n        X_scaled = self.scaler.fit_transform(X)\n        selector = SelectKBest(chi2, k=self.k)\n        X_selected = selector.fit_transform(X_scaled, y)\n        self.feature_scores = selector.scores_\n        self.selected_features = selector.get_support()\n        return X_selected\n\n    elif self.method == 'variance':\n        # Select features based on variance\n        variances = np.var(X, axis=0)\n        top_k_idx = np.argsort(variances)[-self.k:]\n        self.selected_features = np.zeros(X.shape[1], dtype=bool)\n        self.selected_features[top_k_idx] = True\n        self.feature_scores = variances\n        return X[:, top_k_idx]\n\n    elif self.method == 'correlation':\n        # Select features based on correlation with target\n        correlations = np.array([np.corrcoef(X[:, i], y)[0, 1] for i in range(X.shape[1])])\n        top_k_idx = np.argsort(np.abs(correlations))[-self.k:]\n        self.selected_features = np.zeros(X.shape[1], dtype=bool)\n        self.selected_features[top_k_idx] = True\n        self.feature_scores = correlations\n        return X[:, top_k_idx]\n</code></pre>"},{"location":"contentFilterEngine/miscellaneous_techniques/feature_selection/#engines.contentFilterEngine.miscellaneous_techniques.feature_selection.FEATURE_SELECTION.get_feature_importance","title":"<code>get_feature_importance()</code>","text":"<p>Return feature importance scores.</p> Source code in <code>engines/contentFilterEngine/miscellaneous_techniques/feature_selection.py</code> <pre><code>def get_feature_importance(self):\n    \"\"\"Return feature importance scores.\"\"\"\n    if self.feature_scores is None:\n        raise ValueError(\"Fit the selector first using fit_transform()\")\n    return self.feature_scores\n</code></pre>"},{"location":"contentFilterEngine/miscellaneous_techniques/feature_selection/#engines.contentFilterEngine.miscellaneous_techniques.feature_selection.FEATURE_SELECTION.transform","title":"<code>transform(X)</code>","text":"<p>Transform new data using selected features.</p> Source code in <code>engines/contentFilterEngine/miscellaneous_techniques/feature_selection.py</code> <pre><code>def transform(self, X):\n    \"\"\"Transform new data using selected features.\"\"\"\n    if self.selected_features is None:\n        raise ValueError(\"Fit the selector first using fit_transform()\")\n    return X[:, self.selected_features]\n</code></pre>"},{"location":"contentFilterEngine/miscellaneous_techniques/noise_handling/","title":"Noise Handling","text":""},{"location":"contentFilterEngine/miscellaneous_techniques/noise_handling/#engines.contentFilterEngine.miscellaneous_techniques.noise_handling.NOISE_HANDLING","title":"<code>NOISE_HANDLING</code>","text":"Source code in <code>engines/contentFilterEngine/miscellaneous_techniques/noise_handling.py</code> <pre><code>class NOISE_HANDLING:\n    def __init__(self, method='isolation_forest', contamination=0.1):\n        \"\"\"\n        Initialize noise handling.\n\n        Args:\n            method (str): Noise detection method ('isolation_forest', 'zscore', 'iqr')\n            contamination (float): Expected proportion of outliers in the dataset\n        \"\"\"\n        self.method = method\n        self.contamination = contamination\n        self.outlier_detector = None\n        self.scaler = RobustScaler()\n\n    def fit_transform(self, X):\n        \"\"\"\n        Detect and handle noisy samples in the data.\n\n        Args:\n            X: Input data\n        \"\"\"\n        if self.method == 'isolation_forest':\n            self.outlier_detector = IsolationForest(contamination=self.contamination)\n            is_inlier = self.outlier_detector.fit_predict(X) == 1\n            return X[is_inlier], is_inlier\n\n        elif self.method == 'zscore':\n            X_scaled = self.scaler.fit_transform(X)\n            z_scores = np.abs(X_scaled)\n            is_inlier = np.all(z_scores &lt; 3, axis=1)\n            return X[is_inlier], is_inlier\n\n        elif self.method == 'iqr':\n            Q1 = np.percentile(X, 25, axis=0)\n            Q3 = np.percentile(X, 75, axis=0)\n            IQR = Q3 - Q1\n            is_inlier = np.all((X &gt; (Q1 - 1.5 * IQR)) &amp; (X &lt; (Q3 + 1.5 * IQR)), axis=1)\n            return X[is_inlier], is_inlier\n\n    def transform(self, X):\n        \"\"\"Apply noise detection to new data.\"\"\"\n        if self.method == 'isolation_forest':\n            if self.outlier_detector is None:\n                raise ValueError(\"Fit the detector first using fit_transform()\")\n            return X[self.outlier_detector.predict(X) == 1]\n        else:\n            return self.fit_transform(X)[0]\n</code></pre>"},{"location":"contentFilterEngine/miscellaneous_techniques/noise_handling/#engines.contentFilterEngine.miscellaneous_techniques.noise_handling.NOISE_HANDLING.__init__","title":"<code>__init__(method='isolation_forest', contamination=0.1)</code>","text":"<p>Initialize noise handling.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Noise detection method ('isolation_forest', 'zscore', 'iqr')</p> <code>'isolation_forest'</code> <code>contamination</code> <code>float</code> <p>Expected proportion of outliers in the dataset</p> <code>0.1</code> Source code in <code>engines/contentFilterEngine/miscellaneous_techniques/noise_handling.py</code> <pre><code>def __init__(self, method='isolation_forest', contamination=0.1):\n    \"\"\"\n    Initialize noise handling.\n\n    Args:\n        method (str): Noise detection method ('isolation_forest', 'zscore', 'iqr')\n        contamination (float): Expected proportion of outliers in the dataset\n    \"\"\"\n    self.method = method\n    self.contamination = contamination\n    self.outlier_detector = None\n    self.scaler = RobustScaler()\n</code></pre>"},{"location":"contentFilterEngine/miscellaneous_techniques/noise_handling/#engines.contentFilterEngine.miscellaneous_techniques.noise_handling.NOISE_HANDLING.fit_transform","title":"<code>fit_transform(X)</code>","text":"<p>Detect and handle noisy samples in the data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <p>Input data</p> required Source code in <code>engines/contentFilterEngine/miscellaneous_techniques/noise_handling.py</code> <pre><code>def fit_transform(self, X):\n    \"\"\"\n    Detect and handle noisy samples in the data.\n\n    Args:\n        X: Input data\n    \"\"\"\n    if self.method == 'isolation_forest':\n        self.outlier_detector = IsolationForest(contamination=self.contamination)\n        is_inlier = self.outlier_detector.fit_predict(X) == 1\n        return X[is_inlier], is_inlier\n\n    elif self.method == 'zscore':\n        X_scaled = self.scaler.fit_transform(X)\n        z_scores = np.abs(X_scaled)\n        is_inlier = np.all(z_scores &lt; 3, axis=1)\n        return X[is_inlier], is_inlier\n\n    elif self.method == 'iqr':\n        Q1 = np.percentile(X, 25, axis=0)\n        Q3 = np.percentile(X, 75, axis=0)\n        IQR = Q3 - Q1\n        is_inlier = np.all((X &gt; (Q1 - 1.5 * IQR)) &amp; (X &lt; (Q3 + 1.5 * IQR)), axis=1)\n        return X[is_inlier], is_inlier\n</code></pre>"},{"location":"contentFilterEngine/miscellaneous_techniques/noise_handling/#engines.contentFilterEngine.miscellaneous_techniques.noise_handling.NOISE_HANDLING.transform","title":"<code>transform(X)</code>","text":"<p>Apply noise detection to new data.</p> Source code in <code>engines/contentFilterEngine/miscellaneous_techniques/noise_handling.py</code> <pre><code>def transform(self, X):\n    \"\"\"Apply noise detection to new data.\"\"\"\n    if self.method == 'isolation_forest':\n        if self.outlier_detector is None:\n            raise ValueError(\"Fit the detector first using fit_transform()\")\n        return X[self.outlier_detector.predict(X) == 1]\n    else:\n        return self.fit_transform(X)[0]\n</code></pre>"},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/cross_domain/","title":"Cross Domain","text":""},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/cross_domain/#engines.contentFilterEngine.multi_modal_cross_domain_methods.cross_domain.CROSS_DOMAIN","title":"<code>CROSS_DOMAIN</code>","text":"Source code in <code>engines/contentFilterEngine/multi_modal_cross_domain_methods/cross_domain.py</code> <pre><code>class CROSS_DOMAIN:\n    def __init__(self, source_model, target_model):\n        \"\"\"\n        Initialize cross-domain learning.\n\n        Args:\n            source_model: Model trained on the source domain\n            target_model: Model to be trained on the target domain\n        \"\"\"\n        self.source_model = source_model\n        self.target_model = target_model\n\n    def transfer_knowledge(self, data_loader, criterion, optimizer, num_epochs):\n        \"\"\"\n        Transfer knowledge from source to target domain.\n\n        Args:\n            data_loader: DataLoader for target domain data\n            criterion: Loss function\n            optimizer: Optimizer\n            num_epochs: Number of training epochs\n        \"\"\"\n        self.source_model.eval()\n        self.target_model.train()\n\n        for epoch in range(num_epochs):\n            for inputs, labels in data_loader:\n                optimizer.zero_grad()\n                with torch.no_grad():\n                    source_features = self.source_model(inputs)\n                outputs = self.target_model(source_features)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\n    def evaluate(self, data_loader, criterion):\n        \"\"\"\n        Evaluate the target model on the target domain.\n\n        Args:\n            data_loader: DataLoader for evaluation data\n            criterion: Loss function\n        \"\"\"\n        self.target_model.eval()\n\n        total_loss = 0\n        with torch.no_grad():\n            for inputs, labels in data_loader:\n                outputs = self.target_model(inputs)\n                loss = criterion(outputs, labels)\n                total_loss += loss.item()\n\n        print(f\"Evaluation Loss: {total_loss / len(data_loader):.4f}\")\n</code></pre>"},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/cross_domain/#engines.contentFilterEngine.multi_modal_cross_domain_methods.cross_domain.CROSS_DOMAIN.__init__","title":"<code>__init__(source_model, target_model)</code>","text":"<p>Initialize cross-domain learning.</p> <p>Parameters:</p> Name Type Description Default <code>source_model</code> <p>Model trained on the source domain</p> required <code>target_model</code> <p>Model to be trained on the target domain</p> required Source code in <code>engines/contentFilterEngine/multi_modal_cross_domain_methods/cross_domain.py</code> <pre><code>def __init__(self, source_model, target_model):\n    \"\"\"\n    Initialize cross-domain learning.\n\n    Args:\n        source_model: Model trained on the source domain\n        target_model: Model to be trained on the target domain\n    \"\"\"\n    self.source_model = source_model\n    self.target_model = target_model\n</code></pre>"},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/cross_domain/#engines.contentFilterEngine.multi_modal_cross_domain_methods.cross_domain.CROSS_DOMAIN.evaluate","title":"<code>evaluate(data_loader, criterion)</code>","text":"<p>Evaluate the target model on the target domain.</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <p>DataLoader for evaluation data</p> required <code>criterion</code> <p>Loss function</p> required Source code in <code>engines/contentFilterEngine/multi_modal_cross_domain_methods/cross_domain.py</code> <pre><code>def evaluate(self, data_loader, criterion):\n    \"\"\"\n    Evaluate the target model on the target domain.\n\n    Args:\n        data_loader: DataLoader for evaluation data\n        criterion: Loss function\n    \"\"\"\n    self.target_model.eval()\n\n    total_loss = 0\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            outputs = self.target_model(inputs)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n\n    print(f\"Evaluation Loss: {total_loss / len(data_loader):.4f}\")\n</code></pre>"},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/cross_domain/#engines.contentFilterEngine.multi_modal_cross_domain_methods.cross_domain.CROSS_DOMAIN.transfer_knowledge","title":"<code>transfer_knowledge(data_loader, criterion, optimizer, num_epochs)</code>","text":"<p>Transfer knowledge from source to target domain.</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <p>DataLoader for target domain data</p> required <code>criterion</code> <p>Loss function</p> required <code>optimizer</code> <p>Optimizer</p> required <code>num_epochs</code> <p>Number of training epochs</p> required Source code in <code>engines/contentFilterEngine/multi_modal_cross_domain_methods/cross_domain.py</code> <pre><code>def transfer_knowledge(self, data_loader, criterion, optimizer, num_epochs):\n    \"\"\"\n    Transfer knowledge from source to target domain.\n\n    Args:\n        data_loader: DataLoader for target domain data\n        criterion: Loss function\n        optimizer: Optimizer\n        num_epochs: Number of training epochs\n    \"\"\"\n    self.source_model.eval()\n    self.target_model.train()\n\n    for epoch in range(num_epochs):\n        for inputs, labels in data_loader:\n            optimizer.zero_grad()\n            with torch.no_grad():\n                source_features = self.source_model(inputs)\n            outputs = self.target_model(source_features)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n</code></pre>"},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/cross_lingual/","title":"Cross Lingual","text":""},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/cross_lingual/#engines.contentFilterEngine.multi_modal_cross_domain_methods.cross_lingual.CROSS_LINGUAL","title":"<code>CROSS_LINGUAL</code>","text":"Source code in <code>engines/contentFilterEngine/multi_modal_cross_domain_methods/cross_lingual.py</code> <pre><code>class CROSS_LINGUAL:\n    def __init__(self, multilingual_model):\n        \"\"\"\n        Initialize cross-lingual learning.\n\n        Args:\n            multilingual_model: Model capable of handling multiple languages\n        \"\"\"\n        self.multilingual_model = multilingual_model\n\n    def translate(self, text_input, source_lang, target_lang):\n        \"\"\"\n        Translate text from source language to target language.\n\n        Args:\n            text_input: Input text data\n            source_lang: Source language code\n            target_lang: Target language code\n        \"\"\"\n        # Placeholder for translation logic\n        translated_text = self.multilingual_model.translate(text_input, source_lang, target_lang)\n        return translated_text\n\n    def train(self, data_loader, criterion, optimizer, num_epochs):\n        \"\"\"\n        Train the cross-lingual model.\n\n        Args:\n            data_loader: DataLoader for training data\n            criterion: Loss function\n            optimizer: Optimizer\n            num_epochs: Number of training epochs\n        \"\"\"\n        self.multilingual_model.train()\n\n        for epoch in range(num_epochs):\n            for text_input, labels in data_loader:\n                optimizer.zero_grad()\n                outputs = self.multilingual_model(text_input)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n\n    def evaluate(self, data_loader, criterion):\n        \"\"\"\n        Evaluate the cross-lingual model.\n\n        Args:\n            data_loader: DataLoader for evaluation data\n            criterion: Loss function\n        \"\"\"\n        self.multilingual_model.eval()\n\n        total_loss = 0\n        with torch.no_grad():\n            for text_input, labels in data_loader:\n                outputs = self.multilingual_model(text_input)\n                loss = criterion(outputs, labels)\n                total_loss += loss.item()\n\n        print(f\"Evaluation Loss: {total_loss / len(data_loader):.4f}\")\n</code></pre>"},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/cross_lingual/#engines.contentFilterEngine.multi_modal_cross_domain_methods.cross_lingual.CROSS_LINGUAL.__init__","title":"<code>__init__(multilingual_model)</code>","text":"<p>Initialize cross-lingual learning.</p> <p>Parameters:</p> Name Type Description Default <code>multilingual_model</code> <p>Model capable of handling multiple languages</p> required Source code in <code>engines/contentFilterEngine/multi_modal_cross_domain_methods/cross_lingual.py</code> <pre><code>def __init__(self, multilingual_model):\n    \"\"\"\n    Initialize cross-lingual learning.\n\n    Args:\n        multilingual_model: Model capable of handling multiple languages\n    \"\"\"\n    self.multilingual_model = multilingual_model\n</code></pre>"},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/cross_lingual/#engines.contentFilterEngine.multi_modal_cross_domain_methods.cross_lingual.CROSS_LINGUAL.evaluate","title":"<code>evaluate(data_loader, criterion)</code>","text":"<p>Evaluate the cross-lingual model.</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <p>DataLoader for evaluation data</p> required <code>criterion</code> <p>Loss function</p> required Source code in <code>engines/contentFilterEngine/multi_modal_cross_domain_methods/cross_lingual.py</code> <pre><code>def evaluate(self, data_loader, criterion):\n    \"\"\"\n    Evaluate the cross-lingual model.\n\n    Args:\n        data_loader: DataLoader for evaluation data\n        criterion: Loss function\n    \"\"\"\n    self.multilingual_model.eval()\n\n    total_loss = 0\n    with torch.no_grad():\n        for text_input, labels in data_loader:\n            outputs = self.multilingual_model(text_input)\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n\n    print(f\"Evaluation Loss: {total_loss / len(data_loader):.4f}\")\n</code></pre>"},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/cross_lingual/#engines.contentFilterEngine.multi_modal_cross_domain_methods.cross_lingual.CROSS_LINGUAL.train","title":"<code>train(data_loader, criterion, optimizer, num_epochs)</code>","text":"<p>Train the cross-lingual model.</p> <p>Parameters:</p> Name Type Description Default <code>data_loader</code> <p>DataLoader for training data</p> required <code>criterion</code> <p>Loss function</p> required <code>optimizer</code> <p>Optimizer</p> required <code>num_epochs</code> <p>Number of training epochs</p> required Source code in <code>engines/contentFilterEngine/multi_modal_cross_domain_methods/cross_lingual.py</code> <pre><code>def train(self, data_loader, criterion, optimizer, num_epochs):\n    \"\"\"\n    Train the cross-lingual model.\n\n    Args:\n        data_loader: DataLoader for training data\n        criterion: Loss function\n        optimizer: Optimizer\n        num_epochs: Number of training epochs\n    \"\"\"\n    self.multilingual_model.train()\n\n    for epoch in range(num_epochs):\n        for text_input, labels in data_loader:\n            optimizer.zero_grad()\n            outputs = self.multilingual_model(text_input)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n</code></pre>"},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/cross_lingual/#engines.contentFilterEngine.multi_modal_cross_domain_methods.cross_lingual.CROSS_LINGUAL.translate","title":"<code>translate(text_input, source_lang, target_lang)</code>","text":"<p>Translate text from source language to target language.</p> <p>Parameters:</p> Name Type Description Default <code>text_input</code> <p>Input text data</p> required <code>source_lang</code> <p>Source language code</p> required <code>target_lang</code> <p>Target language code</p> required Source code in <code>engines/contentFilterEngine/multi_modal_cross_domain_methods/cross_lingual.py</code> <pre><code>def translate(self, text_input, source_lang, target_lang):\n    \"\"\"\n    Translate text from source language to target language.\n\n    Args:\n        text_input: Input text data\n        source_lang: Source language code\n        target_lang: Target language code\n    \"\"\"\n    # Placeholder for translation logic\n    translated_text = self.multilingual_model.translate(text_input, source_lang, target_lang)\n    return translated_text\n</code></pre>"},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/multi_modal/","title":"Multi Modal","text":""},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/multi_modal/#engines.contentFilterEngine.multi_modal_cross_domain_methods.multi_modal.MULTI_MODAL","title":"<code>MULTI_MODAL</code>","text":"Source code in <code>engines/contentFilterEngine/multi_modal_cross_domain_methods/multi_modal.py</code> <pre><code>class MULTI_MODAL:\n    def __init__(self, text_model, genre_model, audio_model=None):\n        \"\"\"\n        Initialize multi-modal learning.\n\n        Args:\n            text_model: Vectorizer for processing text data (e.g., TfidfVectorizer)\n            genre_model: Binarizer for processing genre data (e.g., MultiLabelBinarizer)\n            audio_model: Model for processing audio data (optional)\n        \"\"\"\n        self.text_model = text_model\n        self.genre_model = genre_model\n        self.audio_model = audio_model\n\n    def forward(self, title_inputs, genre_inputs, audio_inputs=None):\n        \"\"\"\n        Combine the features from different modalities.\n\n        Args:\n            title_inputs (list or array-like): List of movie titles.\n            genre_inputs (list or array-like): List of movie genres.\n            audio_inputs (list or array-like, optional): List of audio features.\n\n        Returns:\n            numpy.ndarray: Combined feature array.\n        \"\"\"\n        # Transform text data\n        title_features = self.text_model.transform(title_inputs).toarray()\n\n        # Transform genre data\n        genre_features = self.genre_model.transform(genre_inputs)\n\n        if self.audio_model and audio_inputs is not None:\n            # Example: If you have audio features, process them accordingly\n            # For simplicity, let's assume audio_model is a transformer that returns numpy arrays\n            audio_features = self.audio_model.transform(audio_inputs).toarray()\n            combined_features = np.hstack((title_features, genre_features, audio_features))\n        else:\n            combined_features = np.hstack((title_features, genre_features))\n\n        return combined_features\n\n    def train(self, data_loader, criterion, optimizer, num_epochs):\n        \"\"\"\n        Training method is not applicable for this implementation.\n        \"\"\"\n        pass\n\n    def evaluate(self, data_loader, criterion):\n        \"\"\"\n        Evaluation method is not applicable for this implementation.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/multi_modal/#engines.contentFilterEngine.multi_modal_cross_domain_methods.multi_modal.MULTI_MODAL.__init__","title":"<code>__init__(text_model, genre_model, audio_model=None)</code>","text":"<p>Initialize multi-modal learning.</p> <p>Parameters:</p> Name Type Description Default <code>text_model</code> <p>Vectorizer for processing text data (e.g., TfidfVectorizer)</p> required <code>genre_model</code> <p>Binarizer for processing genre data (e.g., MultiLabelBinarizer)</p> required <code>audio_model</code> <p>Model for processing audio data (optional)</p> <code>None</code> Source code in <code>engines/contentFilterEngine/multi_modal_cross_domain_methods/multi_modal.py</code> <pre><code>def __init__(self, text_model, genre_model, audio_model=None):\n    \"\"\"\n    Initialize multi-modal learning.\n\n    Args:\n        text_model: Vectorizer for processing text data (e.g., TfidfVectorizer)\n        genre_model: Binarizer for processing genre data (e.g., MultiLabelBinarizer)\n        audio_model: Model for processing audio data (optional)\n    \"\"\"\n    self.text_model = text_model\n    self.genre_model = genre_model\n    self.audio_model = audio_model\n</code></pre>"},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/multi_modal/#engines.contentFilterEngine.multi_modal_cross_domain_methods.multi_modal.MULTI_MODAL.evaluate","title":"<code>evaluate(data_loader, criterion)</code>","text":"<p>Evaluation method is not applicable for this implementation.</p> Source code in <code>engines/contentFilterEngine/multi_modal_cross_domain_methods/multi_modal.py</code> <pre><code>def evaluate(self, data_loader, criterion):\n    \"\"\"\n    Evaluation method is not applicable for this implementation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/multi_modal/#engines.contentFilterEngine.multi_modal_cross_domain_methods.multi_modal.MULTI_MODAL.forward","title":"<code>forward(title_inputs, genre_inputs, audio_inputs=None)</code>","text":"<p>Combine the features from different modalities.</p> <p>Parameters:</p> Name Type Description Default <code>title_inputs</code> <code>list or array - like</code> <p>List of movie titles.</p> required <code>genre_inputs</code> <code>list or array - like</code> <p>List of movie genres.</p> required <code>audio_inputs</code> <code>list or array - like</code> <p>List of audio features.</p> <code>None</code> <p>Returns:</p> Type Description <p>numpy.ndarray: Combined feature array.</p> Source code in <code>engines/contentFilterEngine/multi_modal_cross_domain_methods/multi_modal.py</code> <pre><code>def forward(self, title_inputs, genre_inputs, audio_inputs=None):\n    \"\"\"\n    Combine the features from different modalities.\n\n    Args:\n        title_inputs (list or array-like): List of movie titles.\n        genre_inputs (list or array-like): List of movie genres.\n        audio_inputs (list or array-like, optional): List of audio features.\n\n    Returns:\n        numpy.ndarray: Combined feature array.\n    \"\"\"\n    # Transform text data\n    title_features = self.text_model.transform(title_inputs).toarray()\n\n    # Transform genre data\n    genre_features = self.genre_model.transform(genre_inputs)\n\n    if self.audio_model and audio_inputs is not None:\n        # Example: If you have audio features, process them accordingly\n        # For simplicity, let's assume audio_model is a transformer that returns numpy arrays\n        audio_features = self.audio_model.transform(audio_inputs).toarray()\n        combined_features = np.hstack((title_features, genre_features, audio_features))\n    else:\n        combined_features = np.hstack((title_features, genre_features))\n\n    return combined_features\n</code></pre>"},{"location":"contentFilterEngine/multi_modal_cross_domain_methods/multi_modal/#engines.contentFilterEngine.multi_modal_cross_domain_methods.multi_modal.MULTI_MODAL.train","title":"<code>train(data_loader, criterion, optimizer, num_epochs)</code>","text":"<p>Training method is not applicable for this implementation.</p> Source code in <code>engines/contentFilterEngine/multi_modal_cross_domain_methods/multi_modal.py</code> <pre><code>def train(self, data_loader, criterion, optimizer, num_epochs):\n    \"\"\"\n    Training method is not applicable for this implementation.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/AITM/","title":"AITM","text":""},{"location":"contentFilterEngine/nn_based_algorithms/DSSM/","title":"DSSM","text":""},{"location":"contentFilterEngine/nn_based_algorithms/DSSM/#engines.contentFilterEngine.nn_based_algorithms.DSSM.DSSM","title":"<code>DSSM</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/DSSM.py</code> <pre><code>class DSSM(nn.Module):\n    def __init__(self, \n                 vocab_size, \n                 embedding_dim, \n                 hidden_dims=[256, 128],\n                 dropout=0.5):\n        \"\"\"\n        Initialize the DSSM model.\n\n        Args:\n            vocab_size (int): Size of the vocabulary for text encoding.\n            embedding_dim (int): Dimension of word embeddings.\n            hidden_dims (list): List of hidden layer dimensions.\n            dropout (float): Dropout rate.\n        \"\"\"\n        super(DSSM, self).__init__()\n\n        # Text Embedding Layer\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n\n        # Fully Connected Layers\n        layers = []\n        input_dim = embedding_dim\n        for hidden_dim in hidden_dims:\n            layers.append(nn.Linear(input_dim, hidden_dim))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(dropout))\n            input_dim = hidden_dim\n        self.fc = nn.Sequential(*layers)\n\n        # Output Embedding Layer\n        self.output = nn.Linear(input_dim, hidden_dim)  # Final embedding\n\n    def forward(self, text):\n        \"\"\"\n        Forward pass of the DSSM model.\n\n        Args:\n            text (torch.Tensor): Input text tensor of shape (batch_size, seq_length).\n\n        Returns:\n            torch.Tensor: Semantic embeddings of shape (batch_size, embed_dim).\n        \"\"\"\n        # Text Embedding\n        embedded = self.embedding(text)  # (batch_size, seq_length, embedding_dim)\n        embedded = torch.mean(embedded, dim=1)  # (batch_size, embedding_dim)\n\n        # Fully Connected Layers\n        features = self.fc(embedded)  # (batch_size, hidden_dims[-1])\n\n        # Output Embedding\n        output = self.output(features)  # (batch_size, hidden_dim)\n\n        # Normalize embeddings\n        output = F.normalize(output, p=2, dim=1)\n\n        return output\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/DSSM/#engines.contentFilterEngine.nn_based_algorithms.DSSM.DSSM.__init__","title":"<code>__init__(vocab_size, embedding_dim, hidden_dims=[256, 128], dropout=0.5)</code>","text":"<p>Initialize the DSSM model.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>Size of the vocabulary for text encoding.</p> required <code>embedding_dim</code> <code>int</code> <p>Dimension of word embeddings.</p> required <code>hidden_dims</code> <code>list</code> <p>List of hidden layer dimensions.</p> <code>[256, 128]</code> <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>0.5</code> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/DSSM.py</code> <pre><code>def __init__(self, \n             vocab_size, \n             embedding_dim, \n             hidden_dims=[256, 128],\n             dropout=0.5):\n    \"\"\"\n    Initialize the DSSM model.\n\n    Args:\n        vocab_size (int): Size of the vocabulary for text encoding.\n        embedding_dim (int): Dimension of word embeddings.\n        hidden_dims (list): List of hidden layer dimensions.\n        dropout (float): Dropout rate.\n    \"\"\"\n    super(DSSM, self).__init__()\n\n    # Text Embedding Layer\n    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n\n    # Fully Connected Layers\n    layers = []\n    input_dim = embedding_dim\n    for hidden_dim in hidden_dims:\n        layers.append(nn.Linear(input_dim, hidden_dim))\n        layers.append(nn.ReLU())\n        layers.append(nn.Dropout(dropout))\n        input_dim = hidden_dim\n    self.fc = nn.Sequential(*layers)\n\n    # Output Embedding Layer\n    self.output = nn.Linear(input_dim, hidden_dim)  # Final embedding\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/DSSM/#engines.contentFilterEngine.nn_based_algorithms.DSSM.DSSM.forward","title":"<code>forward(text)</code>","text":"<p>Forward pass of the DSSM model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Tensor</code> <p>Input text tensor of shape (batch_size, seq_length).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Semantic embeddings of shape (batch_size, embed_dim).</p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/DSSM.py</code> <pre><code>def forward(self, text):\n    \"\"\"\n    Forward pass of the DSSM model.\n\n    Args:\n        text (torch.Tensor): Input text tensor of shape (batch_size, seq_length).\n\n    Returns:\n        torch.Tensor: Semantic embeddings of shape (batch_size, embed_dim).\n    \"\"\"\n    # Text Embedding\n    embedded = self.embedding(text)  # (batch_size, seq_length, embedding_dim)\n    embedded = torch.mean(embedded, dim=1)  # (batch_size, embedding_dim)\n\n    # Fully Connected Layers\n    features = self.fc(embedded)  # (batch_size, hidden_dims[-1])\n\n    # Output Embedding\n    output = self.output(features)  # (batch_size, hidden_dim)\n\n    # Normalize embeddings\n    output = F.normalize(output, p=2, dim=1)\n\n    return output\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/MIND/","title":"MIND","text":""},{"location":"contentFilterEngine/nn_based_algorithms/MIND/#engines.contentFilterEngine.nn_based_algorithms.MIND.MIND","title":"<code>MIND</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/MIND.py</code> <pre><code>class MIND(nn.Module):\n    def __init__(self, \n                 vocab_size, \n                 embedding_dim, \n                 num_interests=4, \n                 interest_dim=64, \n                 dropout=0.5, \n                 num_classes=1):\n        \"\"\"\n        Initialize the Multi-Interest Network for Recommendation (MIND).\n\n        Args:\n            vocab_size (int): Size of the vocabulary for text encoding.\n            embedding_dim (int): Dimension of word embeddings.\n            num_interests (int): Number of distinct user interests to capture.\n            interest_dim (int): Dimension of each interest representation.\n            dropout (float): Dropout rate.\n            num_classes (int): Number of output classes.\n        \"\"\"\n        super(MIND, self).__init__()\n\n        # Embedding Layer\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n\n        # Interest Embedding Layers\n        self.interest_layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(embedding_dim, interest_dim),\n                nn.ReLU(),\n                nn.Dropout(dropout)\n            ) for _ in range(num_interests)\n        ])\n\n        # Fusion Layer\n        self.fusion = nn.Linear(interest_dim * num_interests, 128)\n\n        # Output Layer\n        self.fc_out = nn.Linear(128, num_classes)\n\n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, text):\n        \"\"\"\n        Forward pass of the MIND model.\n\n        Args:\n            text (torch.Tensor): Input text tensor of shape (batch_size, seq_length).\n\n        Returns:\n            torch.Tensor: Output logits of shape (batch_size, num_classes).\n        \"\"\"\n        # Embedding\n        embedded = self.embedding(text)  # (batch_size, seq_length, embedding_dim)\n        embedded = embedded.mean(dim=1)   # (batch_size, embedding_dim)\n\n        # Interest Embeddings\n        interests = []\n        for layer in self.interest_layers:\n            interest = layer(embedded)  # (batch_size, interest_dim)\n            interests.append(interest)\n        interests = torch.cat(interests, dim=1)  # (batch_size, interest_dim * num_interests)\n\n        # Fusion Layer\n        fused = F.relu(self.fusion(interests))  # (batch_size, 128)\n        fused = self.dropout(fused)\n\n        # Output Layer\n        out = self.fc_out(fused)  # (batch_size, num_classes)\n\n        return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/MIND/#engines.contentFilterEngine.nn_based_algorithms.MIND.MIND.__init__","title":"<code>__init__(vocab_size, embedding_dim, num_interests=4, interest_dim=64, dropout=0.5, num_classes=1)</code>","text":"<p>Initialize the Multi-Interest Network for Recommendation (MIND).</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>Size of the vocabulary for text encoding.</p> required <code>embedding_dim</code> <code>int</code> <p>Dimension of word embeddings.</p> required <code>num_interests</code> <code>int</code> <p>Number of distinct user interests to capture.</p> <code>4</code> <code>interest_dim</code> <code>int</code> <p>Dimension of each interest representation.</p> <code>64</code> <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>0.5</code> <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>1</code> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/MIND.py</code> <pre><code>def __init__(self, \n             vocab_size, \n             embedding_dim, \n             num_interests=4, \n             interest_dim=64, \n             dropout=0.5, \n             num_classes=1):\n    \"\"\"\n    Initialize the Multi-Interest Network for Recommendation (MIND).\n\n    Args:\n        vocab_size (int): Size of the vocabulary for text encoding.\n        embedding_dim (int): Dimension of word embeddings.\n        num_interests (int): Number of distinct user interests to capture.\n        interest_dim (int): Dimension of each interest representation.\n        dropout (float): Dropout rate.\n        num_classes (int): Number of output classes.\n    \"\"\"\n    super(MIND, self).__init__()\n\n    # Embedding Layer\n    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n\n    # Interest Embedding Layers\n    self.interest_layers = nn.ModuleList([\n        nn.Sequential(\n            nn.Linear(embedding_dim, interest_dim),\n            nn.ReLU(),\n            nn.Dropout(dropout)\n        ) for _ in range(num_interests)\n    ])\n\n    # Fusion Layer\n    self.fusion = nn.Linear(interest_dim * num_interests, 128)\n\n    # Output Layer\n    self.fc_out = nn.Linear(128, num_classes)\n\n    # Dropout\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/MIND/#engines.contentFilterEngine.nn_based_algorithms.MIND.MIND.forward","title":"<code>forward(text)</code>","text":"<p>Forward pass of the MIND model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Tensor</code> <p>Input text tensor of shape (batch_size, seq_length).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output logits of shape (batch_size, num_classes).</p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/MIND.py</code> <pre><code>def forward(self, text):\n    \"\"\"\n    Forward pass of the MIND model.\n\n    Args:\n        text (torch.Tensor): Input text tensor of shape (batch_size, seq_length).\n\n    Returns:\n        torch.Tensor: Output logits of shape (batch_size, num_classes).\n    \"\"\"\n    # Embedding\n    embedded = self.embedding(text)  # (batch_size, seq_length, embedding_dim)\n    embedded = embedded.mean(dim=1)   # (batch_size, embedding_dim)\n\n    # Interest Embeddings\n    interests = []\n    for layer in self.interest_layers:\n        interest = layer(embedded)  # (batch_size, interest_dim)\n        interests.append(interest)\n    interests = torch.cat(interests, dim=1)  # (batch_size, interest_dim * num_interests)\n\n    # Fusion Layer\n    fused = F.relu(self.fusion(interests))  # (batch_size, 128)\n    fused = self.dropout(fused)\n\n    # Output Layer\n    out = self.fc_out(fused)  # (batch_size, num_classes)\n\n    return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/TDM/","title":"TDM","text":""},{"location":"contentFilterEngine/nn_based_algorithms/TDM/#engines.contentFilterEngine.nn_based_algorithms.TDM.TDM","title":"<code>TDM</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/TDM.py</code> <pre><code>class TDM(nn.Module):\n    def __init__(self, \n                 vocab_size, \n                 embedding_dim, \n                 hidden_dim=128, \n                 dropout=0.5, \n                 num_classes=1):\n        \"\"\"\n        Initialize the Text Domain Model (TDM).\n\n        Args:\n            vocab_size (int): Size of the vocabulary for text encoding.\n            embedding_dim (int): Dimension of word embeddings.\n            hidden_dim (int): Dimension of the hidden layer.\n            dropout (float): Dropout rate.\n            num_classes (int): Number of output classes.\n        \"\"\"\n        super(TDM, self).__init__()\n\n        # Embedding Layer\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n\n        # Convolutional Layer for Domain Features\n        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n        self.pool = nn.AdaptiveMaxPool1d(1)\n\n        # Fully Connected Layer\n        self.fc = nn.Linear(hidden_dim, num_classes)\n\n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, text):\n        \"\"\"\n        Forward pass of the TDM model.\n\n        Args:\n            text (torch.Tensor): Input text tensor of shape (batch_size, seq_length).\n\n        Returns:\n            torch.Tensor: Output logits of shape (batch_size, num_classes).\n        \"\"\"\n        # Embedding\n        embedded = self.embedding(text)  # (batch_size, seq_length, embedding_dim)\n        embedded = embedded.permute(0, 2, 1)  # (batch_size, embedding_dim, seq_length)\n\n        # Convolution and Pooling\n        conv_out = F.relu(self.conv(embedded))  # (batch_size, hidden_dim, seq_length)\n        pooled = self.pool(conv_out).squeeze(2)  # (batch_size, hidden_dim)\n\n        # Fully Connected Layer\n        out = self.fc(pooled)  # (batch_size, num_classes)\n        out = self.dropout(out)\n\n        return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/TDM/#engines.contentFilterEngine.nn_based_algorithms.TDM.TDM.__init__","title":"<code>__init__(vocab_size, embedding_dim, hidden_dim=128, dropout=0.5, num_classes=1)</code>","text":"<p>Initialize the Text Domain Model (TDM).</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>Size of the vocabulary for text encoding.</p> required <code>embedding_dim</code> <code>int</code> <p>Dimension of word embeddings.</p> required <code>hidden_dim</code> <code>int</code> <p>Dimension of the hidden layer.</p> <code>128</code> <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>0.5</code> <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>1</code> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/TDM.py</code> <pre><code>def __init__(self, \n             vocab_size, \n             embedding_dim, \n             hidden_dim=128, \n             dropout=0.5, \n             num_classes=1):\n    \"\"\"\n    Initialize the Text Domain Model (TDM).\n\n    Args:\n        vocab_size (int): Size of the vocabulary for text encoding.\n        embedding_dim (int): Dimension of word embeddings.\n        hidden_dim (int): Dimension of the hidden layer.\n        dropout (float): Dropout rate.\n        num_classes (int): Number of output classes.\n    \"\"\"\n    super(TDM, self).__init__()\n\n    # Embedding Layer\n    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n\n    # Convolutional Layer for Domain Features\n    self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=hidden_dim, kernel_size=3, padding=1)\n    self.pool = nn.AdaptiveMaxPool1d(1)\n\n    # Fully Connected Layer\n    self.fc = nn.Linear(hidden_dim, num_classes)\n\n    # Dropout\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/TDM/#engines.contentFilterEngine.nn_based_algorithms.TDM.TDM.forward","title":"<code>forward(text)</code>","text":"<p>Forward pass of the TDM model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Tensor</code> <p>Input text tensor of shape (batch_size, seq_length).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output logits of shape (batch_size, num_classes).</p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/TDM.py</code> <pre><code>def forward(self, text):\n    \"\"\"\n    Forward pass of the TDM model.\n\n    Args:\n        text (torch.Tensor): Input text tensor of shape (batch_size, seq_length).\n\n    Returns:\n        torch.Tensor: Output logits of shape (batch_size, num_classes).\n    \"\"\"\n    # Embedding\n    embedded = self.embedding(text)  # (batch_size, seq_length, embedding_dim)\n    embedded = embedded.permute(0, 2, 1)  # (batch_size, embedding_dim, seq_length)\n\n    # Convolution and Pooling\n    conv_out = F.relu(self.conv(embedded))  # (batch_size, hidden_dim, seq_length)\n    pooled = self.pool(conv_out).squeeze(2)  # (batch_size, hidden_dim)\n\n    # Fully Connected Layer\n    out = self.fc(pooled)  # (batch_size, num_classes)\n    out = self.dropout(out)\n\n    return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/WidenDeep/","title":"WidenDeep","text":""},{"location":"contentFilterEngine/nn_based_algorithms/WidenDeep/#engines.contentFilterEngine.nn_based_algorithms.WidenDeep.WidenDeep","title":"<code>WidenDeep</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/WidenDeep.py</code> <pre><code>class WidenDeep(nn.Module):\n    def __init__(self, \n                 vocab_size, \n                 embedding_dim, \n                 hidden_dims=[128, 64], \n                 dropout=0.5, \n                 num_classes=1):\n        \"\"\"\n        Initialize the Wide &amp; Deep model.\n\n        Args:\n            vocab_size (int): Size of the vocabulary for text encoding.\n            embedding_dim (int): Dimension of word embeddings.\n            hidden_dims (list): List of hidden layer dimensions for the deep part.\n            dropout (float): Dropout rate.\n            num_classes (int): Number of output classes.\n        \"\"\"\n        super(WidenDeep, self).__init__()\n\n        # Embedding Layer\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n\n        # Wide (Linear) Component\n        self.wide = nn.Linear(embedding_dim, num_classes)\n\n        # Deep Component\n        layers = []\n        input_dim = embedding_dim\n        for hidden_dim in hidden_dims:\n            layers.append(nn.Linear(input_dim, hidden_dim))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(dropout))\n            input_dim = hidden_dim\n        self.deep = nn.Sequential(*layers)\n        self.deep_output = nn.Linear(input_dim, num_classes)\n\n    def forward(self, text):\n        \"\"\"\n        Forward pass of the Wide &amp; Deep model.\n\n        Args:\n            text (torch.Tensor): Input text tensor of shape (batch_size, seq_length).\n\n        Returns:\n            torch.Tensor: Output logits of shape (batch_size, num_classes).\n        \"\"\"\n        # Embedding\n        embedded = self.embedding(text)  # (batch_size, seq_length, embedding_dim)\n        embedded = torch.mean(embedded, dim=1)  # (batch_size, embedding_dim)\n\n        # Wide Component\n        wide_out = self.wide(embedded)  # (batch_size, num_classes)\n\n        # Deep Component\n        deep_features = self.deep(embedded)  # (batch_size, hidden_dims[-1])\n        deep_out = self.deep_output(deep_features)  # (batch_size, num_classes)\n\n        # Combine Wide and Deep\n        out = wide_out + deep_out  # (batch_size, num_classes)\n\n        return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/WidenDeep/#engines.contentFilterEngine.nn_based_algorithms.WidenDeep.WidenDeep.__init__","title":"<code>__init__(vocab_size, embedding_dim, hidden_dims=[128, 64], dropout=0.5, num_classes=1)</code>","text":"<p>Initialize the Wide &amp; Deep model.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>Size of the vocabulary for text encoding.</p> required <code>embedding_dim</code> <code>int</code> <p>Dimension of word embeddings.</p> required <code>hidden_dims</code> <code>list</code> <p>List of hidden layer dimensions for the deep part.</p> <code>[128, 64]</code> <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>0.5</code> <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>1</code> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/WidenDeep.py</code> <pre><code>def __init__(self, \n             vocab_size, \n             embedding_dim, \n             hidden_dims=[128, 64], \n             dropout=0.5, \n             num_classes=1):\n    \"\"\"\n    Initialize the Wide &amp; Deep model.\n\n    Args:\n        vocab_size (int): Size of the vocabulary for text encoding.\n        embedding_dim (int): Dimension of word embeddings.\n        hidden_dims (list): List of hidden layer dimensions for the deep part.\n        dropout (float): Dropout rate.\n        num_classes (int): Number of output classes.\n    \"\"\"\n    super(WidenDeep, self).__init__()\n\n    # Embedding Layer\n    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n\n    # Wide (Linear) Component\n    self.wide = nn.Linear(embedding_dim, num_classes)\n\n    # Deep Component\n    layers = []\n    input_dim = embedding_dim\n    for hidden_dim in hidden_dims:\n        layers.append(nn.Linear(input_dim, hidden_dim))\n        layers.append(nn.ReLU())\n        layers.append(nn.Dropout(dropout))\n        input_dim = hidden_dim\n    self.deep = nn.Sequential(*layers)\n    self.deep_output = nn.Linear(input_dim, num_classes)\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/WidenDeep/#engines.contentFilterEngine.nn_based_algorithms.WidenDeep.WidenDeep.forward","title":"<code>forward(text)</code>","text":"<p>Forward pass of the Wide &amp; Deep model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Tensor</code> <p>Input text tensor of shape (batch_size, seq_length).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output logits of shape (batch_size, num_classes).</p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/WidenDeep.py</code> <pre><code>def forward(self, text):\n    \"\"\"\n    Forward pass of the Wide &amp; Deep model.\n\n    Args:\n        text (torch.Tensor): Input text tensor of shape (batch_size, seq_length).\n\n    Returns:\n        torch.Tensor: Output logits of shape (batch_size, num_classes).\n    \"\"\"\n    # Embedding\n    embedded = self.embedding(text)  # (batch_size, seq_length, embedding_dim)\n    embedded = torch.mean(embedded, dim=1)  # (batch_size, embedding_dim)\n\n    # Wide Component\n    wide_out = self.wide(embedded)  # (batch_size, num_classes)\n\n    # Deep Component\n    deep_features = self.deep(embedded)  # (batch_size, hidden_dims[-1])\n    deep_out = self.deep_output(deep_features)  # (batch_size, num_classes)\n\n    # Combine Wide and Deep\n    out = wide_out + deep_out  # (batch_size, num_classes)\n\n    return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/Word2Vec/","title":"Word2Vec","text":""},{"location":"contentFilterEngine/nn_based_algorithms/Youtube_dnn/","title":"Youtube dnn","text":""},{"location":"contentFilterEngine/nn_based_algorithms/Youtube_dnn/#engines.contentFilterEngine.nn_based_algorithms.Youtube_dnn.YoutubeDNN","title":"<code>YoutubeDNN</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/Youtube_dnn.py</code> <pre><code>class YoutubeDNN(nn.Module):\n    def __init__(self, \n                 vocab_size, \n                 embedding_dim, \n                 hidden_dims=[256, 128], \n                 dropout=0.5, \n                 num_classes=1):\n        \"\"\"\n        Initialize the YouTube Deep Neural Network (YoutubeDNN) model.\n\n        Args:\n            vocab_size (int): Size of the vocabulary for text encoding.\n            embedding_dim (int): Dimension of word embeddings.\n            hidden_dims (list): List of hidden layer dimensions.\n            dropout (float): Dropout rate.\n            num_classes (int): Number of output classes.\n        \"\"\"\n        super(YoutubeDNN, self).__init__()\n\n        # Embedding Layer\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n\n        # Fully Connected Layers\n        layers = []\n        input_dim = embedding_dim\n        for hidden_dim in hidden_dims:\n            layers.append(nn.Linear(input_dim, hidden_dim))\n            layers.append(nn.ReLU())\n            layers.append(nn.Dropout(dropout))\n            input_dim = hidden_dim\n        self.fc = nn.Sequential(*layers)\n\n        # Output Layer\n        self.output = nn.Linear(input_dim, num_classes)\n\n    def forward(self, text):\n        \"\"\"\n        Forward pass of the YoutubeDNN model.\n\n        Args:\n            text (torch.Tensor): Input text tensor of shape (batch_size, seq_length).\n\n        Returns:\n            torch.Tensor: Output logits of shape (batch_size, num_classes).\n        \"\"\"\n        # Embedding\n        embedded = self.embedding(text)  # (batch_size, seq_length, embedding_dim)\n        embedded = embedded.mean(dim=1)   # (batch_size, embedding_dim)\n\n        # Fully Connected Layers\n        features = self.fc(embedded)  # (batch_size, hidden_dims[-1])\n\n        # Output Layer\n        out = self.output(features)   # (batch_size, num_classes)\n\n        return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/Youtube_dnn/#engines.contentFilterEngine.nn_based_algorithms.Youtube_dnn.YoutubeDNN.__init__","title":"<code>__init__(vocab_size, embedding_dim, hidden_dims=[256, 128], dropout=0.5, num_classes=1)</code>","text":"<p>Initialize the YouTube Deep Neural Network (YoutubeDNN) model.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>Size of the vocabulary for text encoding.</p> required <code>embedding_dim</code> <code>int</code> <p>Dimension of word embeddings.</p> required <code>hidden_dims</code> <code>list</code> <p>List of hidden layer dimensions.</p> <code>[256, 128]</code> <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>0.5</code> <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>1</code> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/Youtube_dnn.py</code> <pre><code>def __init__(self, \n             vocab_size, \n             embedding_dim, \n             hidden_dims=[256, 128], \n             dropout=0.5, \n             num_classes=1):\n    \"\"\"\n    Initialize the YouTube Deep Neural Network (YoutubeDNN) model.\n\n    Args:\n        vocab_size (int): Size of the vocabulary for text encoding.\n        embedding_dim (int): Dimension of word embeddings.\n        hidden_dims (list): List of hidden layer dimensions.\n        dropout (float): Dropout rate.\n        num_classes (int): Number of output classes.\n    \"\"\"\n    super(YoutubeDNN, self).__init__()\n\n    # Embedding Layer\n    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n\n    # Fully Connected Layers\n    layers = []\n    input_dim = embedding_dim\n    for hidden_dim in hidden_dims:\n        layers.append(nn.Linear(input_dim, hidden_dim))\n        layers.append(nn.ReLU())\n        layers.append(nn.Dropout(dropout))\n        input_dim = hidden_dim\n    self.fc = nn.Sequential(*layers)\n\n    # Output Layer\n    self.output = nn.Linear(input_dim, num_classes)\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/Youtube_dnn/#engines.contentFilterEngine.nn_based_algorithms.Youtube_dnn.YoutubeDNN.forward","title":"<code>forward(text)</code>","text":"<p>Forward pass of the YoutubeDNN model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Tensor</code> <p>Input text tensor of shape (batch_size, seq_length).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output logits of shape (batch_size, num_classes).</p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/Youtube_dnn.py</code> <pre><code>def forward(self, text):\n    \"\"\"\n    Forward pass of the YoutubeDNN model.\n\n    Args:\n        text (torch.Tensor): Input text tensor of shape (batch_size, seq_length).\n\n    Returns:\n        torch.Tensor: Output logits of shape (batch_size, num_classes).\n    \"\"\"\n    # Embedding\n    embedded = self.embedding(text)  # (batch_size, seq_length, embedding_dim)\n    embedded = embedded.mean(dim=1)   # (batch_size, embedding_dim)\n\n    # Fully Connected Layers\n    features = self.fc(embedded)  # (batch_size, hidden_dims[-1])\n\n    # Output Layer\n    out = self.output(features)   # (batch_size, num_classes)\n\n    return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/autoencoder/","title":"Autoencoder","text":""},{"location":"contentFilterEngine/nn_based_algorithms/cnn/","title":"CNN","text":""},{"location":"contentFilterEngine/nn_based_algorithms/cnn/#engines.contentFilterEngine.nn_based_algorithms.cnn.CNN","title":"<code>CNN</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/cnn.py</code> <pre><code>class CNN(nn.Module):\n    def __init__(self, input_dim, num_classes, emb_dim=128, kernel_sizes=[3, 4, 5], num_filters=100, dropout=0.5):\n        \"\"\"\n        Initialize the CNN model for classification.\n\n        Args:\n            input_dim (int): Dimension of the input features.\n            num_classes (int): Number of output classes.\n            emb_dim (int): Embedding dimension.\n            kernel_sizes (list): List of kernel sizes for convolution.\n            num_filters (int): Number of filters per kernel size.\n            dropout (float): Dropout rate.\n        \"\"\"\n        super(CNN, self).__init__()\n        self.embedding = nn.Linear(input_dim, emb_dim)\n\n        self.convs = nn.ModuleList([\n            nn.Conv1d(in_channels=emb_dim, out_channels=num_filters, kernel_size=k)\n            for k in kernel_sizes\n        ])\n\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the CNN model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n\n        Returns:\n            torch.Tensor: Output logits of shape (batch_size, num_classes).\n        \"\"\"\n        x = self.embedding(x)  # (batch_size, emb_dim)\n        x = x.unsqueeze(2)  # (batch_size, emb_dim, 1)\n\n        # Determine the required padding based on the largest kernel size\n        max_kernel_size = max([conv.kernel_size[0] for conv in self.convs])\n        pad_size = (max_kernel_size // 2, max_kernel_size // 2)\n        x = torch.nn.functional.pad(x, pad_size)  # (batch_size, emb_dim, padded_length)\n\n        # Apply convolution and activation\n        conv_out = [torch.relu(conv(x)) for conv in self.convs]  # List of (batch_size, num_filters, L)\n\n        # Apply max pooling over the time dimension\n        pooled = [torch.max(feature_map, dim=2)[0] for feature_map in conv_out]  # List of (batch_size, num_filters)\n\n        # Concatenate pooled features\n        concat = torch.cat(pooled, dim=1)  # (batch_size, num_filters * len(kernel_sizes))\n\n        # Apply dropout\n        drop = self.dropout(concat)\n\n        # Final fully connected layer\n        out = self.fc(drop)  # (batch_size, num_classes)\n\n        return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/cnn/#engines.contentFilterEngine.nn_based_algorithms.cnn.CNN.__init__","title":"<code>__init__(input_dim, num_classes, emb_dim=128, kernel_sizes=[3, 4, 5], num_filters=100, dropout=0.5)</code>","text":"<p>Initialize the CNN model for classification.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of the input features.</p> required <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> required <code>emb_dim</code> <code>int</code> <p>Embedding dimension.</p> <code>128</code> <code>kernel_sizes</code> <code>list</code> <p>List of kernel sizes for convolution.</p> <code>[3, 4, 5]</code> <code>num_filters</code> <code>int</code> <p>Number of filters per kernel size.</p> <code>100</code> <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>0.5</code> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/cnn.py</code> <pre><code>def __init__(self, input_dim, num_classes, emb_dim=128, kernel_sizes=[3, 4, 5], num_filters=100, dropout=0.5):\n    \"\"\"\n    Initialize the CNN model for classification.\n\n    Args:\n        input_dim (int): Dimension of the input features.\n        num_classes (int): Number of output classes.\n        emb_dim (int): Embedding dimension.\n        kernel_sizes (list): List of kernel sizes for convolution.\n        num_filters (int): Number of filters per kernel size.\n        dropout (float): Dropout rate.\n    \"\"\"\n    super(CNN, self).__init__()\n    self.embedding = nn.Linear(input_dim, emb_dim)\n\n    self.convs = nn.ModuleList([\n        nn.Conv1d(in_channels=emb_dim, out_channels=num_filters, kernel_size=k)\n        for k in kernel_sizes\n    ])\n\n    self.dropout = nn.Dropout(dropout)\n    self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/cnn/#engines.contentFilterEngine.nn_based_algorithms.cnn.CNN.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the CNN model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, input_dim).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output logits of shape (batch_size, num_classes).</p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/cnn.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the CNN model.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n\n    Returns:\n        torch.Tensor: Output logits of shape (batch_size, num_classes).\n    \"\"\"\n    x = self.embedding(x)  # (batch_size, emb_dim)\n    x = x.unsqueeze(2)  # (batch_size, emb_dim, 1)\n\n    # Determine the required padding based on the largest kernel size\n    max_kernel_size = max([conv.kernel_size[0] for conv in self.convs])\n    pad_size = (max_kernel_size // 2, max_kernel_size // 2)\n    x = torch.nn.functional.pad(x, pad_size)  # (batch_size, emb_dim, padded_length)\n\n    # Apply convolution and activation\n    conv_out = [torch.relu(conv(x)) for conv in self.convs]  # List of (batch_size, num_filters, L)\n\n    # Apply max pooling over the time dimension\n    pooled = [torch.max(feature_map, dim=2)[0] for feature_map in conv_out]  # List of (batch_size, num_filters)\n\n    # Concatenate pooled features\n    concat = torch.cat(pooled, dim=1)  # (batch_size, num_filters * len(kernel_sizes))\n\n    # Apply dropout\n    drop = self.dropout(concat)\n\n    # Final fully connected layer\n    out = self.fc(drop)  # (batch_size, num_classes)\n\n    return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/dkn/","title":"Dkn","text":""},{"location":"contentFilterEngine/nn_based_algorithms/dkn/#engines.contentFilterEngine.nn_based_algorithms.dkn.DKN","title":"<code>DKN</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/dkn.py</code> <pre><code>class DKN(nn.Module):\n    def __init__(self, \n                 vocab_size, \n                 embedding_dim, \n                 entity_embedding_dim, \n                 knowledge_graph_size, \n                 text_kernel_sizes=[3, 4, 5], \n                 text_num_filters=100, \n                 dropout=0.5,\n                 num_classes=1):\n        \"\"\"\n        Initialize the DKN model.\n\n        Args:\n            vocab_size (int): Size of the vocabulary for text encoding.\n            embedding_dim (int): Dimension of word embeddings.\n            entity_embedding_dim (int): Dimension of entity embeddings from the knowledge graph.\n            knowledge_graph_size (int): Number of entities in the knowledge graph.\n            text_kernel_sizes (list): List of kernel sizes for text CNN.\n            text_num_filters (int): Number of filters per kernel size for text CNN.\n            dropout (float): Dropout rate.\n            num_classes (int): Number of output classes.\n        \"\"\"\n        super(DKN, self).__init__()\n\n        # Text Embedding Layer\n        self.text_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n\n        # Text CNN Encoder\n        self.text_convs = nn.ModuleList([\n            nn.Conv1d(in_channels=embedding_dim, \n                      out_channels=text_num_filters, \n                      kernel_size=k)\n            for k in text_kernel_sizes\n        ])\n\n        # Knowledge Graph Embedding Layer\n        self.entity_embedding = nn.Embedding(num_embeddings=knowledge_graph_size, embedding_dim=entity_embedding_dim, padding_idx=0)\n\n        # Attention Mechanism\n        embed_dim = text_num_filters * len(text_kernel_sizes)  # 100 * 3 = 300\n        num_heads = get_divisible_num_heads(embed_dim, max_heads=8)\n        print(f\"Initializing MultiheadAttention with embed_dim={embed_dim} and num_heads={num_heads}\")\n        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n\n        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=0.1)\n\n        # Fusion Layer\n        self.fusion = nn.Linear(embed_dim + entity_embedding_dim, 256)\n\n        # Dropout Layer\n        self.dropout = nn.Dropout(dropout)\n\n        # Output Layer\n        self.fc_out = nn.Linear(256, num_classes)\n\n    def forward(self, text, entities):\n        \"\"\"\n        Forward pass of the DKN model.\n\n        Args:\n            text (torch.Tensor): Input text tensor of shape (batch_size, seq_length).\n            entities (torch.Tensor): Input entity tensor of shape (batch_size, num_entities).\n\n        Returns:\n            torch.Tensor: Output logits of shape (batch_size, num_classes).\n        \"\"\"\n        # Text Embedding\n        embedded_text = self.text_embedding(text)  # (batch_size, seq_length, embedding_dim)\n        embedded_text = embedded_text.permute(0, 2, 1)  # (batch_size, embedding_dim, seq_length)\n\n        # Text CNN Encoder\n        text_conv_out = [F.relu(conv(embedded_text)) for conv in self.text_convs]  # List of (batch_size, num_filters, L_out)\n        text_pooled = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in text_conv_out]  # List of (batch_size, num_filters)\n        text_features = torch.cat(text_pooled, dim=1)  # (batch_size, embed_dim)\n\n        # Knowledge Graph Embedding\n        entity_embedded = self.entity_embedding(entities)  # (batch_size, num_entities, entity_embedding_dim)\n        entity_features = torch.mean(entity_embedded, dim=1)  # (batch_size, entity_embedding_dim)\n\n        # Attention Mechanism\n        text_features = text_features.unsqueeze(1)  # (batch_size, 1, embed_dim)\n        text_features, _ = self.attention(text_features, text_features, text_features)  # (batch_size, 1, embed_dim)\n        text_features = text_features.squeeze(1)  # (batch_size, embed_dim)\n\n        # Fusion Layer\n        fused = torch.cat([text_features, entity_features], dim=1)  # (batch_size, embed_dim + entity_embedding_dim)\n        fused = self.fusion(fused)  # (batch_size, 256)\n        fused = F.relu(fused)\n        fused = self.dropout(fused)\n\n        # Output Layer\n        out = self.fc_out(fused)  # (batch_size, num_classes)\n\n        return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/dkn/#engines.contentFilterEngine.nn_based_algorithms.dkn.DKN.__init__","title":"<code>__init__(vocab_size, embedding_dim, entity_embedding_dim, knowledge_graph_size, text_kernel_sizes=[3, 4, 5], text_num_filters=100, dropout=0.5, num_classes=1)</code>","text":"<p>Initialize the DKN model.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>Size of the vocabulary for text encoding.</p> required <code>embedding_dim</code> <code>int</code> <p>Dimension of word embeddings.</p> required <code>entity_embedding_dim</code> <code>int</code> <p>Dimension of entity embeddings from the knowledge graph.</p> required <code>knowledge_graph_size</code> <code>int</code> <p>Number of entities in the knowledge graph.</p> required <code>text_kernel_sizes</code> <code>list</code> <p>List of kernel sizes for text CNN.</p> <code>[3, 4, 5]</code> <code>text_num_filters</code> <code>int</code> <p>Number of filters per kernel size for text CNN.</p> <code>100</code> <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>0.5</code> <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>1</code> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/dkn.py</code> <pre><code>def __init__(self, \n             vocab_size, \n             embedding_dim, \n             entity_embedding_dim, \n             knowledge_graph_size, \n             text_kernel_sizes=[3, 4, 5], \n             text_num_filters=100, \n             dropout=0.5,\n             num_classes=1):\n    \"\"\"\n    Initialize the DKN model.\n\n    Args:\n        vocab_size (int): Size of the vocabulary for text encoding.\n        embedding_dim (int): Dimension of word embeddings.\n        entity_embedding_dim (int): Dimension of entity embeddings from the knowledge graph.\n        knowledge_graph_size (int): Number of entities in the knowledge graph.\n        text_kernel_sizes (list): List of kernel sizes for text CNN.\n        text_num_filters (int): Number of filters per kernel size for text CNN.\n        dropout (float): Dropout rate.\n        num_classes (int): Number of output classes.\n    \"\"\"\n    super(DKN, self).__init__()\n\n    # Text Embedding Layer\n    self.text_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n\n    # Text CNN Encoder\n    self.text_convs = nn.ModuleList([\n        nn.Conv1d(in_channels=embedding_dim, \n                  out_channels=text_num_filters, \n                  kernel_size=k)\n        for k in text_kernel_sizes\n    ])\n\n    # Knowledge Graph Embedding Layer\n    self.entity_embedding = nn.Embedding(num_embeddings=knowledge_graph_size, embedding_dim=entity_embedding_dim, padding_idx=0)\n\n    # Attention Mechanism\n    embed_dim = text_num_filters * len(text_kernel_sizes)  # 100 * 3 = 300\n    num_heads = get_divisible_num_heads(embed_dim, max_heads=8)\n    print(f\"Initializing MultiheadAttention with embed_dim={embed_dim} and num_heads={num_heads}\")\n    assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n\n    self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, dropout=0.1)\n\n    # Fusion Layer\n    self.fusion = nn.Linear(embed_dim + entity_embedding_dim, 256)\n\n    # Dropout Layer\n    self.dropout = nn.Dropout(dropout)\n\n    # Output Layer\n    self.fc_out = nn.Linear(256, num_classes)\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/dkn/#engines.contentFilterEngine.nn_based_algorithms.dkn.DKN.forward","title":"<code>forward(text, entities)</code>","text":"<p>Forward pass of the DKN model.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>Tensor</code> <p>Input text tensor of shape (batch_size, seq_length).</p> required <code>entities</code> <code>Tensor</code> <p>Input entity tensor of shape (batch_size, num_entities).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output logits of shape (batch_size, num_classes).</p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/dkn.py</code> <pre><code>def forward(self, text, entities):\n    \"\"\"\n    Forward pass of the DKN model.\n\n    Args:\n        text (torch.Tensor): Input text tensor of shape (batch_size, seq_length).\n        entities (torch.Tensor): Input entity tensor of shape (batch_size, num_entities).\n\n    Returns:\n        torch.Tensor: Output logits of shape (batch_size, num_classes).\n    \"\"\"\n    # Text Embedding\n    embedded_text = self.text_embedding(text)  # (batch_size, seq_length, embedding_dim)\n    embedded_text = embedded_text.permute(0, 2, 1)  # (batch_size, embedding_dim, seq_length)\n\n    # Text CNN Encoder\n    text_conv_out = [F.relu(conv(embedded_text)) for conv in self.text_convs]  # List of (batch_size, num_filters, L_out)\n    text_pooled = [F.max_pool1d(conv, conv.size(2)).squeeze(2) for conv in text_conv_out]  # List of (batch_size, num_filters)\n    text_features = torch.cat(text_pooled, dim=1)  # (batch_size, embed_dim)\n\n    # Knowledge Graph Embedding\n    entity_embedded = self.entity_embedding(entities)  # (batch_size, num_entities, entity_embedding_dim)\n    entity_features = torch.mean(entity_embedded, dim=1)  # (batch_size, entity_embedding_dim)\n\n    # Attention Mechanism\n    text_features = text_features.unsqueeze(1)  # (batch_size, 1, embed_dim)\n    text_features, _ = self.attention(text_features, text_features, text_features)  # (batch_size, 1, embed_dim)\n    text_features = text_features.squeeze(1)  # (batch_size, embed_dim)\n\n    # Fusion Layer\n    fused = torch.cat([text_features, entity_features], dim=1)  # (batch_size, embed_dim + entity_embedding_dim)\n    fused = self.fusion(fused)  # (batch_size, 256)\n    fused = F.relu(fused)\n    fused = self.dropout(fused)\n\n    # Output Layer\n    out = self.fc_out(fused)  # (batch_size, num_classes)\n\n    return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/dkn/#engines.contentFilterEngine.nn_based_algorithms.dkn.get_divisible_num_heads","title":"<code>get_divisible_num_heads(embed_dim, max_heads=8)</code>","text":"<p>Returns the largest number of heads less than or equal to max_heads that divides embed_dim.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>The embedding dimension.</p> required <code>max_heads</code> <code>int</code> <p>The maximum number of heads to consider.</p> <code>8</code> <p>Returns:</p> Name Type Description <code>int</code> <p>A suitable number of heads.</p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/dkn.py</code> <pre><code>def get_divisible_num_heads(embed_dim, max_heads=8):\n    \"\"\"\n    Returns the largest number of heads less than or equal to max_heads that divides embed_dim.\n\n    Args:\n        embed_dim (int): The embedding dimension.\n        max_heads (int): The maximum number of heads to consider.\n\n    Returns:\n        int: A suitable number of heads.\n    \"\"\"\n    for heads in range(max_heads, 0, -1):\n        if embed_dim % heads == 0:\n            return heads\n    return 1  # Fallback to single head if no suitable number found\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/lstur/","title":"LSTUR","text":""},{"location":"contentFilterEngine/nn_based_algorithms/naml/","title":"NAML","text":""},{"location":"contentFilterEngine/nn_based_algorithms/npa/","title":"NPA","text":""},{"location":"contentFilterEngine/nn_based_algorithms/nrms/","title":"NRMS","text":""},{"location":"contentFilterEngine/nn_based_algorithms/rnn/","title":"Rnn","text":""},{"location":"contentFilterEngine/nn_based_algorithms/rnn/#engines.contentFilterEngine.nn_based_algorithms.rnn.RNNModel","title":"<code>RNNModel</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/rnn.py</code> <pre><code>class RNNModel(nn.Module):\n    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers, dropout=0.1, bidirectional=True, num_classes=2):\n        \"\"\"\n        Initialize the RNN model.\n\n        Args:\n            input_dim (int): Dimension of the input features.\n            embed_dim (int): Embedding dimension.\n            hidden_dim (int): Hidden state dimension.\n            num_layers (int): Number of RNN layers.\n            dropout (float): Dropout rate.\n            bidirectional (bool): If True, use a bidirectional RNN.\n            num_classes (int): Number of output classes.\n        \"\"\"\n        super(RNNModel, self).__init__()\n        self.embedding = nn.Linear(input_dim, embed_dim)\n        self.rnn = nn.LSTM(\n            input_size=embed_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            dropout=dropout if num_layers &gt; 1 else 0,\n            bidirectional=bidirectional,\n            batch_first=True\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.fc_out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, num_classes)\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the RNN model.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_dim).\n\n        Returns:\n            torch.Tensor: Output logits of shape (batch_size, num_classes).\n        \"\"\"\n        embed = self.embedding(x)  # (batch_size, seq_length, embed_dim)\n        rnn_out, _ = self.rnn(embed)  # (batch_size, seq_length, hidden_dim * num_directions)\n        # Use the last hidden state for classification\n        if self.rnn.bidirectional:\n            # Concatenate the final forward and backward hidden states\n            last_hidden = torch.cat(\n                (rnn_out[:, -1, :self.rnn.hidden_size], rnn_out[:, 0, self.rnn.hidden_size:]),\n                dim=1\n            )\n        else:\n            last_hidden = rnn_out[:, -1, :]\n        out = self.dropout(last_hidden)\n        out = self.fc_out(out)  # (batch_size, num_classes)\n        return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/rnn/#engines.contentFilterEngine.nn_based_algorithms.rnn.RNNModel.__init__","title":"<code>__init__(input_dim, embed_dim, hidden_dim, num_layers, dropout=0.1, bidirectional=True, num_classes=2)</code>","text":"<p>Initialize the RNN model.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of the input features.</p> required <code>embed_dim</code> <code>int</code> <p>Embedding dimension.</p> required <code>hidden_dim</code> <code>int</code> <p>Hidden state dimension.</p> required <code>num_layers</code> <code>int</code> <p>Number of RNN layers.</p> required <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>0.1</code> <code>bidirectional</code> <code>bool</code> <p>If True, use a bidirectional RNN.</p> <code>True</code> <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>2</code> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/rnn.py</code> <pre><code>def __init__(self, input_dim, embed_dim, hidden_dim, num_layers, dropout=0.1, bidirectional=True, num_classes=2):\n    \"\"\"\n    Initialize the RNN model.\n\n    Args:\n        input_dim (int): Dimension of the input features.\n        embed_dim (int): Embedding dimension.\n        hidden_dim (int): Hidden state dimension.\n        num_layers (int): Number of RNN layers.\n        dropout (float): Dropout rate.\n        bidirectional (bool): If True, use a bidirectional RNN.\n        num_classes (int): Number of output classes.\n    \"\"\"\n    super(RNNModel, self).__init__()\n    self.embedding = nn.Linear(input_dim, embed_dim)\n    self.rnn = nn.LSTM(\n        input_size=embed_dim,\n        hidden_size=hidden_dim,\n        num_layers=num_layers,\n        dropout=dropout if num_layers &gt; 1 else 0,\n        bidirectional=bidirectional,\n        batch_first=True\n    )\n    self.dropout = nn.Dropout(dropout)\n    self.fc_out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, num_classes)\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/rnn/#engines.contentFilterEngine.nn_based_algorithms.rnn.RNNModel.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the RNN model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_length, input_dim).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output logits of shape (batch_size, num_classes).</p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/rnn.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the RNN model.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_dim).\n\n    Returns:\n        torch.Tensor: Output logits of shape (batch_size, num_classes).\n    \"\"\"\n    embed = self.embedding(x)  # (batch_size, seq_length, embed_dim)\n    rnn_out, _ = self.rnn(embed)  # (batch_size, seq_length, hidden_dim * num_directions)\n    # Use the last hidden state for classification\n    if self.rnn.bidirectional:\n        # Concatenate the final forward and backward hidden states\n        last_hidden = torch.cat(\n            (rnn_out[:, -1, :self.rnn.hidden_size], rnn_out[:, 0, self.rnn.hidden_size:]),\n            dim=1\n        )\n    else:\n        last_hidden = rnn_out[:, -1, :]\n    out = self.dropout(last_hidden)\n    out = self.fc_out(out)  # (batch_size, num_classes)\n    return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/transformer/","title":"Transformer Based Models","text":""},{"location":"contentFilterEngine/nn_based_algorithms/transformer/#engines.contentFilterEngine.nn_based_algorithms.transformer.PositionalEncoding","title":"<code>PositionalEncoding</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/transformer.py</code> <pre><code>class PositionalEncoding(nn.Module):\n    def __init__(self, embed_dim, dropout=0.1, max_len=5000):\n        \"\"\"\n        Initialize the positional encoding.\n\n        Args:\n            embed_dim (int): Embedding dimension.\n            dropout (float): Dropout rate.\n            max_len (int): Maximum length of input sequences.\n        \"\"\"\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, embed_dim)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(1)  # (max_len, 1, embed_dim)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Apply positional encoding to the input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (seq_length, batch_size, embed_dim).\n\n        Returns:\n            torch.Tensor: Positionally encoded tensor.\n        \"\"\"\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/transformer/#engines.contentFilterEngine.nn_based_algorithms.transformer.PositionalEncoding.__init__","title":"<code>__init__(embed_dim, dropout=0.1, max_len=5000)</code>","text":"<p>Initialize the positional encoding.</p> <p>Parameters:</p> Name Type Description Default <code>embed_dim</code> <code>int</code> <p>Embedding dimension.</p> required <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>0.1</code> <code>max_len</code> <code>int</code> <p>Maximum length of input sequences.</p> <code>5000</code> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/transformer.py</code> <pre><code>def __init__(self, embed_dim, dropout=0.1, max_len=5000):\n    \"\"\"\n    Initialize the positional encoding.\n\n    Args:\n        embed_dim (int): Embedding dimension.\n        dropout (float): Dropout rate.\n        max_len (int): Maximum length of input sequences.\n    \"\"\"\n    super(PositionalEncoding, self).__init__()\n    self.dropout = nn.Dropout(p=dropout)\n\n    pe = torch.zeros(max_len, embed_dim)\n    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    pe = pe.unsqueeze(1)  # (max_len, 1, embed_dim)\n    self.register_buffer('pe', pe)\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/transformer/#engines.contentFilterEngine.nn_based_algorithms.transformer.PositionalEncoding.forward","title":"<code>forward(x)</code>","text":"<p>Apply positional encoding to the input tensor.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (seq_length, batch_size, embed_dim).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Positionally encoded tensor.</p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/transformer.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Apply positional encoding to the input tensor.\n\n    Args:\n        x (torch.Tensor): Input tensor of shape (seq_length, batch_size, embed_dim).\n\n    Returns:\n        torch.Tensor: Positionally encoded tensor.\n    \"\"\"\n    x = x + self.pe[:x.size(0), :]\n    return self.dropout(x)\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/transformer/#engines.contentFilterEngine.nn_based_algorithms.transformer.TransformerModel","title":"<code>TransformerModel</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/transformer.py</code> <pre><code>class TransformerModel(nn.Module):\n    def __init__(self, input_dim, embed_dim, num_heads, hidden_dim, num_layers, dropout=0.1, num_classes=2):\n        \"\"\"\n        Initialize the Transformer model.\n\n        Args:\n            input_dim (int): Dimension of the input features.\n            embed_dim (int): Embedding dimension.\n            num_heads (int): Number of attention heads.\n            hidden_dim (int): Dimension of the feedforward network.\n            num_layers (int): Number of Transformer encoder layers.\n            dropout (float): Dropout rate.\n            num_classes (int): Number of output classes.\n        \"\"\"\n        super(TransformerModel, self).__init__()\n        self.embedding = nn.Linear(input_dim, embed_dim)\n        self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n        encoder_layers = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n        self.fc_out = nn.Linear(embed_dim, num_classes)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src):\n        \"\"\"\n        Forward pass of the Transformer model.\n\n        Args:\n            src (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_dim).\n\n        Returns:\n            torch.Tensor: Output logits of shape (batch_size, num_classes).\n        \"\"\"\n        # Check if src is 2D and add a sequence dimension if necessary\n        if src.dim() == 2:\n            src = src.unsqueeze(1)  # (batch_size, seq_length=1, input_dim)\n        src = self.embedding(src)  # (batch_size, seq_length, embed_dim)\n        src = src.permute(1, 0, 2)  # (seq_length, batch_size, embed_dim)\n        src = self.pos_encoder(src)\n        memory = self.transformer_encoder(src)  # (seq_length, batch_size, embed_dim)\n        memory = memory.mean(dim=0)  # (batch_size, embed_dim)\n        memory = self.dropout(memory)\n        out = self.fc_out(memory)  # (batch_size, num_classes)\n        return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/transformer/#engines.contentFilterEngine.nn_based_algorithms.transformer.TransformerModel.__init__","title":"<code>__init__(input_dim, embed_dim, num_heads, hidden_dim, num_layers, dropout=0.1, num_classes=2)</code>","text":"<p>Initialize the Transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimension of the input features.</p> required <code>embed_dim</code> <code>int</code> <p>Embedding dimension.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>hidden_dim</code> <code>int</code> <p>Dimension of the feedforward network.</p> required <code>num_layers</code> <code>int</code> <p>Number of Transformer encoder layers.</p> required <code>dropout</code> <code>float</code> <p>Dropout rate.</p> <code>0.1</code> <code>num_classes</code> <code>int</code> <p>Number of output classes.</p> <code>2</code> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/transformer.py</code> <pre><code>def __init__(self, input_dim, embed_dim, num_heads, hidden_dim, num_layers, dropout=0.1, num_classes=2):\n    \"\"\"\n    Initialize the Transformer model.\n\n    Args:\n        input_dim (int): Dimension of the input features.\n        embed_dim (int): Embedding dimension.\n        num_heads (int): Number of attention heads.\n        hidden_dim (int): Dimension of the feedforward network.\n        num_layers (int): Number of Transformer encoder layers.\n        dropout (float): Dropout rate.\n        num_classes (int): Number of output classes.\n    \"\"\"\n    super(TransformerModel, self).__init__()\n    self.embedding = nn.Linear(input_dim, embed_dim)\n    self.pos_encoder = PositionalEncoding(embed_dim, dropout)\n    encoder_layers = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=dropout)\n    self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n    self.fc_out = nn.Linear(embed_dim, num_classes)\n    self.dropout = nn.Dropout(dropout)\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/transformer/#engines.contentFilterEngine.nn_based_algorithms.transformer.TransformerModel.forward","title":"<code>forward(src)</code>","text":"<p>Forward pass of the Transformer model.</p> <p>Parameters:</p> Name Type Description Default <code>src</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, seq_length, input_dim).</p> required <p>Returns:</p> Type Description <p>torch.Tensor: Output logits of shape (batch_size, num_classes).</p> Source code in <code>engines/contentFilterEngine/nn_based_algorithms/transformer.py</code> <pre><code>def forward(self, src):\n    \"\"\"\n    Forward pass of the Transformer model.\n\n    Args:\n        src (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_dim).\n\n    Returns:\n        torch.Tensor: Output logits of shape (batch_size, num_classes).\n    \"\"\"\n    # Check if src is 2D and add a sequence dimension if necessary\n    if src.dim() == 2:\n        src = src.unsqueeze(1)  # (batch_size, seq_length=1, input_dim)\n    src = self.embedding(src)  # (batch_size, seq_length, embed_dim)\n    src = src.permute(1, 0, 2)  # (seq_length, batch_size, embed_dim)\n    src = self.pos_encoder(src)\n    memory = self.transformer_encoder(src)  # (seq_length, batch_size, embed_dim)\n    memory = memory.mean(dim=0)  # (batch_size, embed_dim)\n    memory = self.dropout(memory)\n    out = self.fc_out(memory)  # (batch_size, num_classes)\n    return out\n</code></pre>"},{"location":"contentFilterEngine/nn_based_algorithms/vae/","title":"VAE","text":""},{"location":"contentFilterEngine/other_approaches/ontology_based/","title":"Ontology Based","text":""},{"location":"contentFilterEngine/other_approaches/ontology_based/#engines.contentFilterEngine.other_approaches.ontology_based.OntologyBasedFilter","title":"<code>OntologyBasedFilter</code>","text":"Source code in <code>engines/contentFilterEngine/other_approaches/ontology_based.py</code> <pre><code>class OntologyBasedFilter:\n    def __init__(self, ontology_path):\n        \"\"\"\n        Initializes the OntologyBasedFilter with a specific ontology.\n\n        Parameters:\n        - ontology_path (str): The file path to the ontology (.owl) file.\n        \"\"\"\n        try:\n            self.ontology = get_ontology(ontology_path).load()\n        except Exception as e:\n            raise ValueError(f\"Failed to load ontology from {ontology_path}: {e}\")\n\n    def get_concepts(self, content):\n        \"\"\"\n        Extracts concepts from the content based on the ontology.\n\n        Parameters:\n        - content (str): The content to extract concepts from.\n\n        Returns:\n        - set: A set of concepts identified in the content.\n        \"\"\"\n        concepts_found = set()\n        content_lower = content.lower()\n\n        for cls in self.ontology.classes():\n            if cls.name.lower() in content_lower:\n                concepts_found.add(cls.name)\n\n        return concepts_found\n\n    def filter_content(self, content):\n        \"\"\"\n        Filters the content based on ontology-defined relationships.\n\n        Parameters:\n        - content (str): The content to be filtered.\n\n        Returns:\n        - dict: A dictionary with 'status' and 'related_concepts'.\n        \"\"\"\n        concepts = self.get_concepts(content)\n        related_concepts = self.find_related_concepts(concepts)\n\n        if related_concepts:\n            return {'status': 'filtered', 'related_concepts': related_concepts}\n        else:\n            return {'status': 'allowed', 'related_concepts': related_concepts}\n\n    def find_related_concepts(self, concepts):\n        \"\"\"\n        Finds related concepts within the ontology.\n\n        Parameters:\n        - concepts (set): A set of concepts to find relationships for.\n\n        Returns:\n        - dict: A dictionary mapping each concept to its related concepts.\n        \"\"\"\n        related = {}\n        for concept in concepts:\n            try:\n                cls = self.ontology[concept]\n                related[concept] = [str(rel) for rel in cls.is_a]\n            except KeyError:\n                related[concept] = []\n        return related\n</code></pre>"},{"location":"contentFilterEngine/other_approaches/ontology_based/#engines.contentFilterEngine.other_approaches.ontology_based.OntologyBasedFilter.__init__","title":"<code>__init__(ontology_path)</code>","text":"<p>Initializes the OntologyBasedFilter with a specific ontology.</p> <p>Parameters: - ontology_path (str): The file path to the ontology (.owl) file.</p> Source code in <code>engines/contentFilterEngine/other_approaches/ontology_based.py</code> <pre><code>def __init__(self, ontology_path):\n    \"\"\"\n    Initializes the OntologyBasedFilter with a specific ontology.\n\n    Parameters:\n    - ontology_path (str): The file path to the ontology (.owl) file.\n    \"\"\"\n    try:\n        self.ontology = get_ontology(ontology_path).load()\n    except Exception as e:\n        raise ValueError(f\"Failed to load ontology from {ontology_path}: {e}\")\n</code></pre>"},{"location":"contentFilterEngine/other_approaches/ontology_based/#engines.contentFilterEngine.other_approaches.ontology_based.OntologyBasedFilter.filter_content","title":"<code>filter_content(content)</code>","text":"<p>Filters the content based on ontology-defined relationships.</p> <p>Parameters: - content (str): The content to be filtered.</p> <p>Returns: - dict: A dictionary with 'status' and 'related_concepts'.</p> Source code in <code>engines/contentFilterEngine/other_approaches/ontology_based.py</code> <pre><code>def filter_content(self, content):\n    \"\"\"\n    Filters the content based on ontology-defined relationships.\n\n    Parameters:\n    - content (str): The content to be filtered.\n\n    Returns:\n    - dict: A dictionary with 'status' and 'related_concepts'.\n    \"\"\"\n    concepts = self.get_concepts(content)\n    related_concepts = self.find_related_concepts(concepts)\n\n    if related_concepts:\n        return {'status': 'filtered', 'related_concepts': related_concepts}\n    else:\n        return {'status': 'allowed', 'related_concepts': related_concepts}\n</code></pre>"},{"location":"contentFilterEngine/other_approaches/ontology_based/#engines.contentFilterEngine.other_approaches.ontology_based.OntologyBasedFilter.find_related_concepts","title":"<code>find_related_concepts(concepts)</code>","text":"<p>Finds related concepts within the ontology.</p> <p>Parameters: - concepts (set): A set of concepts to find relationships for.</p> <p>Returns: - dict: A dictionary mapping each concept to its related concepts.</p> Source code in <code>engines/contentFilterEngine/other_approaches/ontology_based.py</code> <pre><code>def find_related_concepts(self, concepts):\n    \"\"\"\n    Finds related concepts within the ontology.\n\n    Parameters:\n    - concepts (set): A set of concepts to find relationships for.\n\n    Returns:\n    - dict: A dictionary mapping each concept to its related concepts.\n    \"\"\"\n    related = {}\n    for concept in concepts:\n        try:\n            cls = self.ontology[concept]\n            related[concept] = [str(rel) for rel in cls.is_a]\n        except KeyError:\n            related[concept] = []\n    return related\n</code></pre>"},{"location":"contentFilterEngine/other_approaches/ontology_based/#engines.contentFilterEngine.other_approaches.ontology_based.OntologyBasedFilter.get_concepts","title":"<code>get_concepts(content)</code>","text":"<p>Extracts concepts from the content based on the ontology.</p> <p>Parameters: - content (str): The content to extract concepts from.</p> <p>Returns: - set: A set of concepts identified in the content.</p> Source code in <code>engines/contentFilterEngine/other_approaches/ontology_based.py</code> <pre><code>def get_concepts(self, content):\n    \"\"\"\n    Extracts concepts from the content based on the ontology.\n\n    Parameters:\n    - content (str): The content to extract concepts from.\n\n    Returns:\n    - set: A set of concepts identified in the content.\n    \"\"\"\n    concepts_found = set()\n    content_lower = content.lower()\n\n    for cls in self.ontology.classes():\n        if cls.name.lower() in content_lower:\n            concepts_found.add(cls.name)\n\n    return concepts_found\n</code></pre>"},{"location":"contentFilterEngine/other_approaches/rule_based/","title":"Rule Based","text":""},{"location":"contentFilterEngine/other_approaches/rule_based/#engines.contentFilterEngine.other_approaches.rule_based.RuleBasedFilter","title":"<code>RuleBasedFilter</code>","text":"Source code in <code>engines/contentFilterEngine/other_approaches/rule_based.py</code> <pre><code>class RuleBasedFilter:\n    def __init__(self, rules=None):\n        \"\"\"\n        Initializes the RuleBasedFilter with a set of rules.\n\n        Parameters:\n        - rules (list of dict): A list where each rule is a dictionary containing\n                                 'keyword' and 'action' keys.\n        \"\"\"\n        if rules is None:\n            self.rules = []\n        else:\n            self.rules = rules\n\n    def add_rule(self, keyword, action):\n        \"\"\"\n        Adds a new rule to the filter.\n\n        Parameters:\n        - keyword (str): The keyword to look for in the content.\n        - action (str): The action to take ('block', 'flag', etc.).\n        \"\"\"\n        rule = {'keyword': keyword.lower(), 'action': action.lower()}\n        self.rules.append(rule)\n\n    def filter_content(self, content):\n        \"\"\"\n        Filters the content based on the predefined rules.\n\n        Parameters:\n        - content (str): The content to be filtered.\n\n        Returns:\n        - dict: A dictionary with 'status' and 'actions' applied.\n        \"\"\"\n        actions_applied = []\n        content_lower = content.lower()\n\n        for rule in self.rules:\n            if rule['keyword'] in content_lower:\n                actions_applied.append(rule['action'])\n\n        if 'block' in actions_applied:\n            return {'status': 'blocked', 'actions': actions_applied}\n        elif 'flag' in actions_applied:\n            return {'status': 'flagged', 'actions': actions_applied}\n        else:\n            return {'status': 'allowed', 'actions': actions_applied}\n</code></pre>"},{"location":"contentFilterEngine/other_approaches/rule_based/#engines.contentFilterEngine.other_approaches.rule_based.RuleBasedFilter.__init__","title":"<code>__init__(rules=None)</code>","text":"<p>Initializes the RuleBasedFilter with a set of rules.</p> <ul> <li>rules (list of dict): A list where each rule is a dictionary containing                          'keyword' and 'action' keys.</li> </ul> Source code in <code>engines/contentFilterEngine/other_approaches/rule_based.py</code> <pre><code>def __init__(self, rules=None):\n    \"\"\"\n    Initializes the RuleBasedFilter with a set of rules.\n\n    Parameters:\n    - rules (list of dict): A list where each rule is a dictionary containing\n                             'keyword' and 'action' keys.\n    \"\"\"\n    if rules is None:\n        self.rules = []\n    else:\n        self.rules = rules\n</code></pre>"},{"location":"contentFilterEngine/other_approaches/rule_based/#engines.contentFilterEngine.other_approaches.rule_based.RuleBasedFilter.add_rule","title":"<code>add_rule(keyword, action)</code>","text":"<p>Adds a new rule to the filter.</p> <p>Parameters: - keyword (str): The keyword to look for in the content. - action (str): The action to take ('block', 'flag', etc.).</p> Source code in <code>engines/contentFilterEngine/other_approaches/rule_based.py</code> <pre><code>def add_rule(self, keyword, action):\n    \"\"\"\n    Adds a new rule to the filter.\n\n    Parameters:\n    - keyword (str): The keyword to look for in the content.\n    - action (str): The action to take ('block', 'flag', etc.).\n    \"\"\"\n    rule = {'keyword': keyword.lower(), 'action': action.lower()}\n    self.rules.append(rule)\n</code></pre>"},{"location":"contentFilterEngine/other_approaches/rule_based/#engines.contentFilterEngine.other_approaches.rule_based.RuleBasedFilter.filter_content","title":"<code>filter_content(content)</code>","text":"<p>Filters the content based on the predefined rules.</p> <p>Parameters: - content (str): The content to be filtered.</p> <p>Returns: - dict: A dictionary with 'status' and 'actions' applied.</p> Source code in <code>engines/contentFilterEngine/other_approaches/rule_based.py</code> <pre><code>def filter_content(self, content):\n    \"\"\"\n    Filters the content based on the predefined rules.\n\n    Parameters:\n    - content (str): The content to be filtered.\n\n    Returns:\n    - dict: A dictionary with 'status' and 'actions' applied.\n    \"\"\"\n    actions_applied = []\n    content_lower = content.lower()\n\n    for rule in self.rules:\n        if rule['keyword'] in content_lower:\n            actions_applied.append(rule['action'])\n\n    if 'block' in actions_applied:\n        return {'status': 'blocked', 'actions': actions_applied}\n    elif 'flag' in actions_applied:\n        return {'status': 'flagged', 'actions': actions_applied}\n    else:\n        return {'status': 'allowed', 'actions': actions_applied}\n</code></pre>"},{"location":"contentFilterEngine/other_approaches/sentiment_analysis/","title":"Sentiment Analysis","text":""},{"location":"contentFilterEngine/other_approaches/sentiment_analysis/#engines.contentFilterEngine.other_approaches.sentiment_analysis.SentimentAnalysisFilter","title":"<code>SentimentAnalysisFilter</code>","text":"Source code in <code>engines/contentFilterEngine/other_approaches/sentiment_analysis.py</code> <pre><code>class SentimentAnalysisFilter:\n    def __init__(self, threshold=0.1):\n        \"\"\"\n        Initializes the SentimentAnalysisFilter.\n\n        Parameters:\n        - threshold (float): The sentiment polarity threshold to trigger actions.\n                             Positive values can indicate positive sentiment,\n                             negative values indicate negative sentiment.\n        \"\"\"\n        self.threshold = threshold\n\n    def analyze_sentiment(self, content):\n        \"\"\"\n        Analyzes the sentiment of the given content.\n\n        Parameters:\n        - content (str): The content to analyze.\n\n        Returns:\n        - float: The sentiment polarity score ranging from -1.0 to 1.0.\n        \"\"\"\n        blob = TextBlob(content)\n        return blob.sentiment.polarity\n\n    def filter_content(self, content):\n        \"\"\"\n        Filters the content based on its sentiment.\n\n        Parameters:\n        - content (str): The content to be filtered.\n\n        Returns:\n        - dict: A dictionary with 'status' and 'sentiment_score'.\n        \"\"\"\n        sentiment_score = self.analyze_sentiment(content)\n\n        if sentiment_score &lt; -self.threshold:\n            return {'status': 'negative', 'sentiment_score': sentiment_score}\n        elif sentiment_score &gt; self.threshold:\n            return {'status': 'positive', 'sentiment_score': sentiment_score}\n        else:\n            return {'status': 'neutral', 'sentiment_score': sentiment_score}\n</code></pre>"},{"location":"contentFilterEngine/other_approaches/sentiment_analysis/#engines.contentFilterEngine.other_approaches.sentiment_analysis.SentimentAnalysisFilter.__init__","title":"<code>__init__(threshold=0.1)</code>","text":"<p>Initializes the SentimentAnalysisFilter.</p> <ul> <li>threshold (float): The sentiment polarity threshold to trigger actions.                      Positive values can indicate positive sentiment,                      negative values indicate negative sentiment.</li> </ul> Source code in <code>engines/contentFilterEngine/other_approaches/sentiment_analysis.py</code> <pre><code>def __init__(self, threshold=0.1):\n    \"\"\"\n    Initializes the SentimentAnalysisFilter.\n\n    Parameters:\n    - threshold (float): The sentiment polarity threshold to trigger actions.\n                         Positive values can indicate positive sentiment,\n                         negative values indicate negative sentiment.\n    \"\"\"\n    self.threshold = threshold\n</code></pre>"},{"location":"contentFilterEngine/other_approaches/sentiment_analysis/#engines.contentFilterEngine.other_approaches.sentiment_analysis.SentimentAnalysisFilter.analyze_sentiment","title":"<code>analyze_sentiment(content)</code>","text":"<p>Analyzes the sentiment of the given content.</p> <p>Parameters: - content (str): The content to analyze.</p> <p>Returns: - float: The sentiment polarity score ranging from -1.0 to 1.0.</p> Source code in <code>engines/contentFilterEngine/other_approaches/sentiment_analysis.py</code> <pre><code>def analyze_sentiment(self, content):\n    \"\"\"\n    Analyzes the sentiment of the given content.\n\n    Parameters:\n    - content (str): The content to analyze.\n\n    Returns:\n    - float: The sentiment polarity score ranging from -1.0 to 1.0.\n    \"\"\"\n    blob = TextBlob(content)\n    return blob.sentiment.polarity\n</code></pre>"},{"location":"contentFilterEngine/other_approaches/sentiment_analysis/#engines.contentFilterEngine.other_approaches.sentiment_analysis.SentimentAnalysisFilter.filter_content","title":"<code>filter_content(content)</code>","text":"<p>Filters the content based on its sentiment.</p> <p>Parameters: - content (str): The content to be filtered.</p> <p>Returns: - dict: A dictionary with 'status' and 'sentiment_score'.</p> Source code in <code>engines/contentFilterEngine/other_approaches/sentiment_analysis.py</code> <pre><code>def filter_content(self, content):\n    \"\"\"\n    Filters the content based on its sentiment.\n\n    Parameters:\n    - content (str): The content to be filtered.\n\n    Returns:\n    - dict: A dictionary with 'status' and 'sentiment_score'.\n    \"\"\"\n    sentiment_score = self.analyze_sentiment(content)\n\n    if sentiment_score &lt; -self.threshold:\n        return {'status': 'negative', 'sentiment_score': sentiment_score}\n    elif sentiment_score &gt; self.threshold:\n        return {'status': 'positive', 'sentiment_score': sentiment_score}\n    else:\n        return {'status': 'neutral', 'sentiment_score': sentiment_score}\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/feature_extraction/","title":"Feature Extraction","text":""},{"location":"contentFilterEngine/performance_scalability/feature_extraction/#engines.contentFilterEngine.performance_scalability.feature_extraction.FeatureExtraction","title":"<code>FeatureExtraction</code>","text":"Source code in <code>engines/contentFilterEngine/performance_scalability/feature_extraction.py</code> <pre><code>class FeatureExtraction:\n    def __init__(self, max_features=5000):\n        \"\"\"\n        Initializes the FeatureExtraction with a TF-IDF vectorizer.\n\n        Parameters:\n        - max_features (int): The maximum number of features (vocabulary size).\n        \"\"\"\n        self.max_features = max_features\n        self.lemmatizer = WordNetLemmatizer()\n        self.vectorizer = TfidfVectorizer(\n            max_features=self.max_features,\n            stop_words='english',  # Use built-in stop words\n            tokenizer=self.tokenize\n        )\n        logger.info(f\"FeatureExtraction initialized with max_features={self.max_features}.\")\n\n    def tokenize(self, text: str) -&gt; List[str]:\n        \"\"\"\n        Tokenizes and lemmatizes the input text.\n\n        Parameters:\n        - text (str): The text to tokenize.\n\n        Returns:\n        - list: A list of processed tokens.\n        \"\"\"\n        tokens = nltk.word_tokenize(text.lower())\n        lemmatized = [\n            self.lemmatizer.lemmatize(token)\n            for token in tokens\n            if token.isalpha()\n        ]\n        logger.debug(f\"Tokenized text: {lemmatized}\")\n        return lemmatized\n\n    def fit_transform(self, documents: List[str]):\n        \"\"\"\n        Fits the TF-IDF vectorizer on the documents and transforms them into feature vectors.\n\n        Parameters:\n        - documents (list of str): The list of documents to process.\n\n        Returns:\n        - sparse matrix: The TF-IDF feature matrix.\n        \"\"\"\n        logger.info(\"Fitting and transforming documents into TF-IDF features.\")\n        return self.vectorizer.fit_transform(documents)\n\n    def transform(self, documents: List[str]) -&gt; Any:\n        \"\"\"\n        Transforms the documents into TF-IDF feature vectors using the already fitted vectorizer.\n\n        Parameters:\n        - documents (list of str): The list of documents to transform.\n\n        Returns:\n        - sparse matrix: The TF-IDF feature matrix.\n        \"\"\"\n        logger.info(\"Transforming documents into LSA latent space.\")\n        tfidf_matrix = self.vectorizer.transform(documents)  # Use transform, not fit\n        return self.lsa_model.transform(tfidf_matrix)\n\n    def get_feature_names(self) -&gt; List[str]:\n        \"\"\"\n        Retrieves the feature names (vocabulary) from the vectorizer.\n\n        Returns:\n        - list: A list of feature names.\n        \"\"\"\n        return self.vectorizer.get_feature_names_out()\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/feature_extraction/#engines.contentFilterEngine.performance_scalability.feature_extraction.FeatureExtraction.__init__","title":"<code>__init__(max_features=5000)</code>","text":"<p>Initializes the FeatureExtraction with a TF-IDF vectorizer.</p> <p>Parameters: - max_features (int): The maximum number of features (vocabulary size).</p> Source code in <code>engines/contentFilterEngine/performance_scalability/feature_extraction.py</code> <pre><code>def __init__(self, max_features=5000):\n    \"\"\"\n    Initializes the FeatureExtraction with a TF-IDF vectorizer.\n\n    Parameters:\n    - max_features (int): The maximum number of features (vocabulary size).\n    \"\"\"\n    self.max_features = max_features\n    self.lemmatizer = WordNetLemmatizer()\n    self.vectorizer = TfidfVectorizer(\n        max_features=self.max_features,\n        stop_words='english',  # Use built-in stop words\n        tokenizer=self.tokenize\n    )\n    logger.info(f\"FeatureExtraction initialized with max_features={self.max_features}.\")\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/feature_extraction/#engines.contentFilterEngine.performance_scalability.feature_extraction.FeatureExtraction.fit_transform","title":"<code>fit_transform(documents)</code>","text":"<p>Fits the TF-IDF vectorizer on the documents and transforms them into feature vectors.</p> <p>Parameters: - documents (list of str): The list of documents to process.</p> <p>Returns: - sparse matrix: The TF-IDF feature matrix.</p> Source code in <code>engines/contentFilterEngine/performance_scalability/feature_extraction.py</code> <pre><code>def fit_transform(self, documents: List[str]):\n    \"\"\"\n    Fits the TF-IDF vectorizer on the documents and transforms them into feature vectors.\n\n    Parameters:\n    - documents (list of str): The list of documents to process.\n\n    Returns:\n    - sparse matrix: The TF-IDF feature matrix.\n    \"\"\"\n    logger.info(\"Fitting and transforming documents into TF-IDF features.\")\n    return self.vectorizer.fit_transform(documents)\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/feature_extraction/#engines.contentFilterEngine.performance_scalability.feature_extraction.FeatureExtraction.get_feature_names","title":"<code>get_feature_names()</code>","text":"<p>Retrieves the feature names (vocabulary) from the vectorizer.</p> <p>Returns: - list: A list of feature names.</p> Source code in <code>engines/contentFilterEngine/performance_scalability/feature_extraction.py</code> <pre><code>def get_feature_names(self) -&gt; List[str]:\n    \"\"\"\n    Retrieves the feature names (vocabulary) from the vectorizer.\n\n    Returns:\n    - list: A list of feature names.\n    \"\"\"\n    return self.vectorizer.get_feature_names_out()\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/feature_extraction/#engines.contentFilterEngine.performance_scalability.feature_extraction.FeatureExtraction.tokenize","title":"<code>tokenize(text)</code>","text":"<p>Tokenizes and lemmatizes the input text.</p> <p>Parameters: - text (str): The text to tokenize.</p> <p>Returns: - list: A list of processed tokens.</p> Source code in <code>engines/contentFilterEngine/performance_scalability/feature_extraction.py</code> <pre><code>def tokenize(self, text: str) -&gt; List[str]:\n    \"\"\"\n    Tokenizes and lemmatizes the input text.\n\n    Parameters:\n    - text (str): The text to tokenize.\n\n    Returns:\n    - list: A list of processed tokens.\n    \"\"\"\n    tokens = nltk.word_tokenize(text.lower())\n    lemmatized = [\n        self.lemmatizer.lemmatize(token)\n        for token in tokens\n        if token.isalpha()\n    ]\n    logger.debug(f\"Tokenized text: {lemmatized}\")\n    return lemmatized\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/feature_extraction/#engines.contentFilterEngine.performance_scalability.feature_extraction.FeatureExtraction.transform","title":"<code>transform(documents)</code>","text":"<p>Transforms the documents into TF-IDF feature vectors using the already fitted vectorizer.</p> <p>Parameters: - documents (list of str): The list of documents to transform.</p> <p>Returns: - sparse matrix: The TF-IDF feature matrix.</p> Source code in <code>engines/contentFilterEngine/performance_scalability/feature_extraction.py</code> <pre><code>def transform(self, documents: List[str]) -&gt; Any:\n    \"\"\"\n    Transforms the documents into TF-IDF feature vectors using the already fitted vectorizer.\n\n    Parameters:\n    - documents (list of str): The list of documents to transform.\n\n    Returns:\n    - sparse matrix: The TF-IDF feature matrix.\n    \"\"\"\n    logger.info(\"Transforming documents into LSA latent space.\")\n    tfidf_matrix = self.vectorizer.transform(documents)  # Use transform, not fit\n    return self.lsa_model.transform(tfidf_matrix)\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/load_balancing/","title":"Load Balancing","text":""},{"location":"contentFilterEngine/performance_scalability/load_balancing/#engines.contentFilterEngine.performance_scalability.load_balancing.LoadBalancing","title":"<code>LoadBalancing</code>","text":"Source code in <code>engines/contentFilterEngine/performance_scalability/load_balancing.py</code> <pre><code>class LoadBalancing:\n    def __init__(self, num_workers=4):\n        \"\"\"\n        Initializes the LoadBalancing with a specified number of worker threads.\n\n        Parameters:\n        - num_workers (int): The number of worker threads to spawn.\n        \"\"\"\n        self.num_workers = num_workers\n        self.task_queue = Queue()\n        self.results = []\n        self.threads = []\n        self._init_workers()\n        logger.info(f\"LoadBalancing initialized with {self.num_workers} workers.\")\n\n    def _init_workers(self):\n        \"\"\"\n        Initializes worker threads that continuously process tasks from the queue.\n        \"\"\"\n        for i in range(self.num_workers):\n            thread = Thread(target=self._worker, name=f\"Worker-{i+1}\", daemon=True)\n            thread.start()\n            self.threads.append(thread)\n            logger.debug(f\"Started {thread.name}.\")\n\n    def _worker(self):\n        \"\"\"\n        Worker thread that processes tasks from the queue.\n        \"\"\"\n        while True:\n            func, args, kwargs = self.task_queue.get()\n            if func is None:\n                # Sentinel found, terminate the thread\n                logger.debug(f\"{threading.current_thread().name} received sentinel. Exiting.\")\n                break\n            try:\n                result = func(*args, **kwargs)\n                self.results.append(result)\n                logger.debug(f\"{threading.current_thread().name} processed a task with result: {result}\")\n            except Exception as e:\n                logger.error(f\"Error processing task: {e}\")\n            finally:\n                self.task_queue.task_done()\n\n    def add_task(self, func, *args, **kwargs):\n        \"\"\"\n        Adds a new task to the queue.\n\n        Parameters:\n        - func (callable): The function to execute.\n        - *args: Positional arguments for the function.\n        - **kwargs: Keyword arguments for the function.\n        \"\"\"\n        self.task_queue.put((func, args, kwargs))\n        logger.debug(f\"Added task {func.__name__} to the queue.\")\n\n    def get_results(self):\n        \"\"\"\n        Waits for all tasks to be processed and returns the results.\n\n        Returns:\n        - list: A list of results from all tasks.\n        \"\"\"\n        self.task_queue.join()\n        return self.results\n\n    def shutdown(self):\n        \"\"\"\n        Shuts down all worker threads gracefully by sending sentinel tasks.\n        \"\"\"\n        for _ in self.threads:\n            self.task_queue.put((None, (), {}))  # Sentinel\n        for thread in self.threads:\n            thread.join()\n            logger.debug(f\"{thread.name} has terminated.\")\n        logger.info(\"LoadBalancing has been shutdown.\")\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/load_balancing/#engines.contentFilterEngine.performance_scalability.load_balancing.LoadBalancing.__init__","title":"<code>__init__(num_workers=4)</code>","text":"<p>Initializes the LoadBalancing with a specified number of worker threads.</p> <p>Parameters: - num_workers (int): The number of worker threads to spawn.</p> Source code in <code>engines/contentFilterEngine/performance_scalability/load_balancing.py</code> <pre><code>def __init__(self, num_workers=4):\n    \"\"\"\n    Initializes the LoadBalancing with a specified number of worker threads.\n\n    Parameters:\n    - num_workers (int): The number of worker threads to spawn.\n    \"\"\"\n    self.num_workers = num_workers\n    self.task_queue = Queue()\n    self.results = []\n    self.threads = []\n    self._init_workers()\n    logger.info(f\"LoadBalancing initialized with {self.num_workers} workers.\")\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/load_balancing/#engines.contentFilterEngine.performance_scalability.load_balancing.LoadBalancing.add_task","title":"<code>add_task(func, *args, **kwargs)</code>","text":"<p>Adds a new task to the queue.</p> <p>Parameters: - func (callable): The function to execute. - args: Positional arguments for the function. - *kwargs: Keyword arguments for the function.</p> Source code in <code>engines/contentFilterEngine/performance_scalability/load_balancing.py</code> <pre><code>def add_task(self, func, *args, **kwargs):\n    \"\"\"\n    Adds a new task to the queue.\n\n    Parameters:\n    - func (callable): The function to execute.\n    - *args: Positional arguments for the function.\n    - **kwargs: Keyword arguments for the function.\n    \"\"\"\n    self.task_queue.put((func, args, kwargs))\n    logger.debug(f\"Added task {func.__name__} to the queue.\")\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/load_balancing/#engines.contentFilterEngine.performance_scalability.load_balancing.LoadBalancing.get_results","title":"<code>get_results()</code>","text":"<p>Waits for all tasks to be processed and returns the results.</p> <p>Returns: - list: A list of results from all tasks.</p> Source code in <code>engines/contentFilterEngine/performance_scalability/load_balancing.py</code> <pre><code>def get_results(self):\n    \"\"\"\n    Waits for all tasks to be processed and returns the results.\n\n    Returns:\n    - list: A list of results from all tasks.\n    \"\"\"\n    self.task_queue.join()\n    return self.results\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/load_balancing/#engines.contentFilterEngine.performance_scalability.load_balancing.LoadBalancing.shutdown","title":"<code>shutdown()</code>","text":"<p>Shuts down all worker threads gracefully by sending sentinel tasks.</p> Source code in <code>engines/contentFilterEngine/performance_scalability/load_balancing.py</code> <pre><code>def shutdown(self):\n    \"\"\"\n    Shuts down all worker threads gracefully by sending sentinel tasks.\n    \"\"\"\n    for _ in self.threads:\n        self.task_queue.put((None, (), {}))  # Sentinel\n    for thread in self.threads:\n        thread.join()\n        logger.debug(f\"{thread.name} has terminated.\")\n    logger.info(\"LoadBalancing has been shutdown.\")\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/scalable_algorithms/","title":"Scalable Algorithms","text":""},{"location":"contentFilterEngine/performance_scalability/scalable_algorithms/#engines.contentFilterEngine.performance_scalability.scalable_algorithms.ScalableAlgorithms","title":"<code>ScalableAlgorithms</code>","text":"Source code in <code>engines/contentFilterEngine/performance_scalability/scalable_algorithms.py</code> <pre><code>class ScalableAlgorithms:\n    def __init__(self, num_workers=None):\n        \"\"\"\n        Initializes the ScalableAlgorithms with a specified number of worker processes.\n\n        Parameters:\n        - num_workers (int, optional): The number of worker processes to use.\n                                        Defaults to the number of CPU cores available.\n        \"\"\"\n        if num_workers is None:\n            self.num_workers = multiprocessing.cpu_count()\n        else:\n            self.num_workers = num_workers\n        logger.info(f\"ScalableAlgorithms initialized with {self.num_workers} workers.\")\n\n    def parallel_process(self, function, data, chunksize=1):\n        \"\"\"\n        Processes data in parallel using a specified function.\n\n        Parameters:\n        - function (callable): The function to apply to each data chunk.\n        - data (iterable): The data to process.\n        - chunksize (int): The size of each data chunk.\n\n        Returns:\n        - list: A list of results after applying the function.\n        \"\"\"\n        results = []\n        with ProcessPoolExecutor(max_workers=self.num_workers) as executor:\n            future_to_data = {executor.submit(function, item): item for item in data}\n            for future in as_completed(future_to_data):\n                item = future_to_data[future]\n                try:\n                    result = future.result()\n                    results.append(result)\n                    logger.debug(f\"Processed item: {item} with result: {result}\")\n                except Exception as exc:\n                    logger.error(f\"Item {item} generated an exception: {exc}\")\n        logger.info(\"Parallel processing completed.\")\n        return results\n\n    def map_async(self, function, data):\n        \"\"\"\n        Asynchronously maps a function over data using multiprocessing.\n\n        Parameters:\n        - function (callable): The function to apply to each data item.\n        - data (iterable): The data to process.\n\n        Returns:\n        - list: A list of results after applying the function.\n        \"\"\"\n        with multiprocessing.Pool(processes=self.num_workers) as pool:\n            results = pool.map_async(function, data).get()\n        logger.info(\"Asynchronous mapping completed.\")\n        return results\n\n    def chunkify(self, data, n_chunks):\n        \"\"\"\n        Splits data into specified number of chunks.\n\n        Parameters:\n        - data (list): The data to split.\n        - n_chunks (int): The number of chunks to create.\n\n        Returns:\n        - list of lists: A list containing the data chunks.\n        \"\"\"\n        chunk_size = len(data) // n_chunks\n        chunks = [data[i * chunk_size:(i + 1) * chunk_size] for i in range(n_chunks - 1)]\n        chunks.append(data[(n_chunks - 1) * chunk_size:])\n        logger.info(f\"Data split into {n_chunks} chunks.\")\n        return chunks\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/scalable_algorithms/#engines.contentFilterEngine.performance_scalability.scalable_algorithms.ScalableAlgorithms.__init__","title":"<code>__init__(num_workers=None)</code>","text":"<p>Initializes the ScalableAlgorithms with a specified number of worker processes.</p> <ul> <li>num_workers (int, optional): The number of worker processes to use.                                 Defaults to the number of CPU cores available.</li> </ul> Source code in <code>engines/contentFilterEngine/performance_scalability/scalable_algorithms.py</code> <pre><code>def __init__(self, num_workers=None):\n    \"\"\"\n    Initializes the ScalableAlgorithms with a specified number of worker processes.\n\n    Parameters:\n    - num_workers (int, optional): The number of worker processes to use.\n                                    Defaults to the number of CPU cores available.\n    \"\"\"\n    if num_workers is None:\n        self.num_workers = multiprocessing.cpu_count()\n    else:\n        self.num_workers = num_workers\n    logger.info(f\"ScalableAlgorithms initialized with {self.num_workers} workers.\")\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/scalable_algorithms/#engines.contentFilterEngine.performance_scalability.scalable_algorithms.ScalableAlgorithms.chunkify","title":"<code>chunkify(data, n_chunks)</code>","text":"<p>Splits data into specified number of chunks.</p> <p>Parameters: - data (list): The data to split. - n_chunks (int): The number of chunks to create.</p> <p>Returns: - list of lists: A list containing the data chunks.</p> Source code in <code>engines/contentFilterEngine/performance_scalability/scalable_algorithms.py</code> <pre><code>def chunkify(self, data, n_chunks):\n    \"\"\"\n    Splits data into specified number of chunks.\n\n    Parameters:\n    - data (list): The data to split.\n    - n_chunks (int): The number of chunks to create.\n\n    Returns:\n    - list of lists: A list containing the data chunks.\n    \"\"\"\n    chunk_size = len(data) // n_chunks\n    chunks = [data[i * chunk_size:(i + 1) * chunk_size] for i in range(n_chunks - 1)]\n    chunks.append(data[(n_chunks - 1) * chunk_size:])\n    logger.info(f\"Data split into {n_chunks} chunks.\")\n    return chunks\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/scalable_algorithms/#engines.contentFilterEngine.performance_scalability.scalable_algorithms.ScalableAlgorithms.map_async","title":"<code>map_async(function, data)</code>","text":"<p>Asynchronously maps a function over data using multiprocessing.</p> <p>Parameters: - function (callable): The function to apply to each data item. - data (iterable): The data to process.</p> <p>Returns: - list: A list of results after applying the function.</p> Source code in <code>engines/contentFilterEngine/performance_scalability/scalable_algorithms.py</code> <pre><code>def map_async(self, function, data):\n    \"\"\"\n    Asynchronously maps a function over data using multiprocessing.\n\n    Parameters:\n    - function (callable): The function to apply to each data item.\n    - data (iterable): The data to process.\n\n    Returns:\n    - list: A list of results after applying the function.\n    \"\"\"\n    with multiprocessing.Pool(processes=self.num_workers) as pool:\n        results = pool.map_async(function, data).get()\n    logger.info(\"Asynchronous mapping completed.\")\n    return results\n</code></pre>"},{"location":"contentFilterEngine/performance_scalability/scalable_algorithms/#engines.contentFilterEngine.performance_scalability.scalable_algorithms.ScalableAlgorithms.parallel_process","title":"<code>parallel_process(function, data, chunksize=1)</code>","text":"<p>Processes data in parallel using a specified function.</p> <p>Parameters: - function (callable): The function to apply to each data chunk. - data (iterable): The data to process. - chunksize (int): The size of each data chunk.</p> <p>Returns: - list: A list of results after applying the function.</p> Source code in <code>engines/contentFilterEngine/performance_scalability/scalable_algorithms.py</code> <pre><code>def parallel_process(self, function, data, chunksize=1):\n    \"\"\"\n    Processes data in parallel using a specified function.\n\n    Parameters:\n    - function (callable): The function to apply to each data chunk.\n    - data (iterable): The data to process.\n    - chunksize (int): The size of each data chunk.\n\n    Returns:\n    - list: A list of results after applying the function.\n    \"\"\"\n    results = []\n    with ProcessPoolExecutor(max_workers=self.num_workers) as executor:\n        future_to_data = {executor.submit(function, item): item for item in data}\n        for future in as_completed(future_to_data):\n            item = future_to_data[future]\n            try:\n                result = future.result()\n                results.append(result)\n                logger.debug(f\"Processed item: {item} with result: {result}\")\n            except Exception as exc:\n                logger.error(f\"Item {item} generated an exception: {exc}\")\n    logger.info(\"Parallel processing completed.\")\n    return results\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/bayesian/","title":"Bayesian","text":""},{"location":"contentFilterEngine/probabilistic_statistical_methods/bayesian/#engines.contentFilterEngine.probabilistic_statistical_methods.bayesian.BAYESIAN","title":"<code>BAYESIAN</code>","text":"Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/bayesian.py</code> <pre><code>class BAYESIAN:\n    def __init__(self):\n        \"\"\"\n        Initialize the Bayesian classifier using Multinomial Naive Bayes.\n        \"\"\"\n        self.vectorizer = CountVectorizer(stop_words='english')\n        self.model = MultinomialNB()\n        logger.info(\"Bayesian classifier initialized.\")\n\n    def fit(self, documents: List[str], labels: List[int]):\n        \"\"\"\n        Fit the Bayesian classifier on the provided documents and labels.\n\n        Parameters:\n        - documents (List[str]): List of documents to train the model.\n        - labels (List[int]): Corresponding labels for the documents.\n        \"\"\"\n        logger.info(\"Fitting Bayesian classifier on documents.\")\n        count_matrix = self.vectorizer.fit_transform(documents)\n        self.model.fit(count_matrix, labels)\n        logger.info(\"Bayesian classifier training completed.\")\n\n    def predict(self, query: str) -&gt; int:\n        \"\"\"\n        Predict the label for a given query.\n\n        Parameters:\n        - query (str): The query text to classify.\n\n        Returns:\n        - int: Predicted label.\n        \"\"\"\n        logger.info(\"Predicting label using Bayesian classifier.\")\n        query_vec = self.vectorizer.transform([query])\n        prediction = self.model.predict(query_vec)[0]\n        logger.info(f\"Predicted label: {prediction}\")\n        return prediction\n\n    def recommend(self, query: str, top_n: int = 10) -&gt; List[int]:\n        \"\"\"\n        Recommend items based on the Bayesian classifier's prediction.\n\n        Parameters:\n        - query (str): The query text for which to generate recommendations.\n        - top_n (int): Number of top recommendations to return.\n\n        Returns:\n        - List[int]: List of recommended item indices.\n        \"\"\"\n        logger.info(\"Generating recommendations using Bayesian classifier.\")\n        predicted_label = self.predict(query)\n        # Example: Recommend items with the same label\n        # This requires access to labeled items; here we return an empty list as a placeholder\n        recommendations = []  # Implement logic based on the application\n        logger.info(f\"Top {top_n} recommendations generated using Bayesian classifier.\")\n        return recommendations[:top_n]\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/bayesian/#engines.contentFilterEngine.probabilistic_statistical_methods.bayesian.BAYESIAN.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Bayesian classifier using Multinomial Naive Bayes.</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/bayesian.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the Bayesian classifier using Multinomial Naive Bayes.\n    \"\"\"\n    self.vectorizer = CountVectorizer(stop_words='english')\n    self.model = MultinomialNB()\n    logger.info(\"Bayesian classifier initialized.\")\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/bayesian/#engines.contentFilterEngine.probabilistic_statistical_methods.bayesian.BAYESIAN.fit","title":"<code>fit(documents, labels)</code>","text":"<p>Fit the Bayesian classifier on the provided documents and labels.</p> <p>Parameters: - documents (List[str]): List of documents to train the model. - labels (List[int]): Corresponding labels for the documents.</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/bayesian.py</code> <pre><code>def fit(self, documents: List[str], labels: List[int]):\n    \"\"\"\n    Fit the Bayesian classifier on the provided documents and labels.\n\n    Parameters:\n    - documents (List[str]): List of documents to train the model.\n    - labels (List[int]): Corresponding labels for the documents.\n    \"\"\"\n    logger.info(\"Fitting Bayesian classifier on documents.\")\n    count_matrix = self.vectorizer.fit_transform(documents)\n    self.model.fit(count_matrix, labels)\n    logger.info(\"Bayesian classifier training completed.\")\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/bayesian/#engines.contentFilterEngine.probabilistic_statistical_methods.bayesian.BAYESIAN.predict","title":"<code>predict(query)</code>","text":"<p>Predict the label for a given query.</p> <p>Parameters: - query (str): The query text to classify.</p> <p>Returns: - int: Predicted label.</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/bayesian.py</code> <pre><code>def predict(self, query: str) -&gt; int:\n    \"\"\"\n    Predict the label for a given query.\n\n    Parameters:\n    - query (str): The query text to classify.\n\n    Returns:\n    - int: Predicted label.\n    \"\"\"\n    logger.info(\"Predicting label using Bayesian classifier.\")\n    query_vec = self.vectorizer.transform([query])\n    prediction = self.model.predict(query_vec)[0]\n    logger.info(f\"Predicted label: {prediction}\")\n    return prediction\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/bayesian/#engines.contentFilterEngine.probabilistic_statistical_methods.bayesian.BAYESIAN.recommend","title":"<code>recommend(query, top_n=10)</code>","text":"<p>Recommend items based on the Bayesian classifier's prediction.</p> <p>Parameters: - query (str): The query text for which to generate recommendations. - top_n (int): Number of top recommendations to return.</p> <p>Returns: - List[int]: List of recommended item indices.</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/bayesian.py</code> <pre><code>def recommend(self, query: str, top_n: int = 10) -&gt; List[int]:\n    \"\"\"\n    Recommend items based on the Bayesian classifier's prediction.\n\n    Parameters:\n    - query (str): The query text for which to generate recommendations.\n    - top_n (int): Number of top recommendations to return.\n\n    Returns:\n    - List[int]: List of recommended item indices.\n    \"\"\"\n    logger.info(\"Generating recommendations using Bayesian classifier.\")\n    predicted_label = self.predict(query)\n    # Example: Recommend items with the same label\n    # This requires access to labeled items; here we return an empty list as a placeholder\n    recommendations = []  # Implement logic based on the application\n    logger.info(f\"Top {top_n} recommendations generated using Bayesian classifier.\")\n    return recommendations[:top_n]\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/fuzzy_logic/","title":"Fuzzy Logic","text":""},{"location":"contentFilterEngine/probabilistic_statistical_methods/fuzzy_logic/#engines.contentFilterEngine.probabilistic_statistical_methods.fuzzy_logic.FUZZY_LOGIC","title":"<code>FUZZY_LOGIC</code>","text":"Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/fuzzy_logic.py</code> <pre><code>class FUZZY_LOGIC:\n    def __init__(self, input_vars: Dict[str, Callable[[int], Dict[str, float]]], \n                 output_vars: Dict[str, Callable[[Dict[str, float]], float]], \n                 rules: List[Callable[[Dict[str, float]], Dict[str, float]]]):\n        \"\"\"\n        Initialize the Fuzzy Logic System with custom inputs, outputs, and rules.\n\n        Parameters:\n        - input_vars (Dict[str, Callable]): Dictionary of input variable names and their fuzzification functions.\n        - output_vars (Dict[str, Callable]): Dictionary of output variable names and their defuzzification functions.\n        - rules (List[Callable]): List of functions representing the fuzzy rules.\n        \"\"\"\n        self.input_vars = input_vars\n        self.output_vars = output_vars\n        self.rules = rules\n        logger.info(\"Fuzzy Logic System initialized with custom inputs, outputs, and rules.\")\n\n    def evaluate(self, input_values: Dict[str, int]) -&gt; Dict[str, float]:\n        \"\"\"\n        Evaluate the fuzzy logic system based on input values.\n\n        Parameters:\n        - input_values (Dict[str, int]): Dictionary with input variable names and their values.\n\n        Returns:\n        - Dict[str, float]: Dictionary with the defuzzified output values.\n        \"\"\"\n        logger.info(\"Evaluating Fuzzy Logic System.\")\n        try:\n            # Fuzzify inputs\n            fuzzy_inputs = {var: self.input_vars[var](value) for var, value in input_values.items()}\n\n            # Apply rules\n            fuzzy_outputs = {}\n            for rule in self.rules:\n                rule_output = rule(fuzzy_inputs)\n                for key, value in rule_output.items():\n                    if key not in fuzzy_outputs:\n                        fuzzy_outputs[key] = value\n                    else:\n                        fuzzy_outputs[key] = max(fuzzy_outputs[key], value)\n\n            # Defuzzify outputs\n            defuzzified_outputs = {var: self.output_vars[var](fuzzy_outputs) for var in self.output_vars}\n            logger.info(f\"Fuzzy Logic evaluation result: {defuzzified_outputs}\")\n            return defuzzified_outputs\n        except Exception as e:\n            logger.error(f\"Error in Fuzzy Logic evaluation: {e}\")\n            return {}\n\n    def recommend(self, input_values: Dict[str, int], top_n: int = 10) -&gt; List[int]:\n        \"\"\"\n        Recommend actions based on fuzzy logic evaluation.\n\n        Parameters:\n        - input_values (Dict[str, int]): Dictionary with input values for fuzzy evaluation.\n        - top_n (int): Number of top recommendations to return.\n\n        Returns:\n        - List[int]: List of recommended actions (placeholder for actual implementation).\n        \"\"\"\n        logger.info(\"Generating recommendations using Fuzzy Logic System.\")\n        evaluation = self.evaluate(input_values)\n        # Example: Recommend actions based on evaluation\n        # This requires a mapping from evaluation results to actions; here we return an empty list as a placeholder\n        recommendations = []  # Implement logic based on the application\n        logger.info(f\"Top {top_n} recommendations generated using Fuzzy Logic.\")\n        return recommendations[:top_n]\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/fuzzy_logic/#engines.contentFilterEngine.probabilistic_statistical_methods.fuzzy_logic.FUZZY_LOGIC.__init__","title":"<code>__init__(input_vars, output_vars, rules)</code>","text":"<p>Initialize the Fuzzy Logic System with custom inputs, outputs, and rules.</p> <p>Parameters: - input_vars (Dict[str, Callable]): Dictionary of input variable names and their fuzzification functions. - output_vars (Dict[str, Callable]): Dictionary of output variable names and their defuzzification functions. - rules (List[Callable]): List of functions representing the fuzzy rules.</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/fuzzy_logic.py</code> <pre><code>def __init__(self, input_vars: Dict[str, Callable[[int], Dict[str, float]]], \n             output_vars: Dict[str, Callable[[Dict[str, float]], float]], \n             rules: List[Callable[[Dict[str, float]], Dict[str, float]]]):\n    \"\"\"\n    Initialize the Fuzzy Logic System with custom inputs, outputs, and rules.\n\n    Parameters:\n    - input_vars (Dict[str, Callable]): Dictionary of input variable names and their fuzzification functions.\n    - output_vars (Dict[str, Callable]): Dictionary of output variable names and their defuzzification functions.\n    - rules (List[Callable]): List of functions representing the fuzzy rules.\n    \"\"\"\n    self.input_vars = input_vars\n    self.output_vars = output_vars\n    self.rules = rules\n    logger.info(\"Fuzzy Logic System initialized with custom inputs, outputs, and rules.\")\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/fuzzy_logic/#engines.contentFilterEngine.probabilistic_statistical_methods.fuzzy_logic.FUZZY_LOGIC.evaluate","title":"<code>evaluate(input_values)</code>","text":"<p>Evaluate the fuzzy logic system based on input values.</p> <p>Parameters: - input_values (Dict[str, int]): Dictionary with input variable names and their values.</p> <p>Returns: - Dict[str, float]: Dictionary with the defuzzified output values.</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/fuzzy_logic.py</code> <pre><code>def evaluate(self, input_values: Dict[str, int]) -&gt; Dict[str, float]:\n    \"\"\"\n    Evaluate the fuzzy logic system based on input values.\n\n    Parameters:\n    - input_values (Dict[str, int]): Dictionary with input variable names and their values.\n\n    Returns:\n    - Dict[str, float]: Dictionary with the defuzzified output values.\n    \"\"\"\n    logger.info(\"Evaluating Fuzzy Logic System.\")\n    try:\n        # Fuzzify inputs\n        fuzzy_inputs = {var: self.input_vars[var](value) for var, value in input_values.items()}\n\n        # Apply rules\n        fuzzy_outputs = {}\n        for rule in self.rules:\n            rule_output = rule(fuzzy_inputs)\n            for key, value in rule_output.items():\n                if key not in fuzzy_outputs:\n                    fuzzy_outputs[key] = value\n                else:\n                    fuzzy_outputs[key] = max(fuzzy_outputs[key], value)\n\n        # Defuzzify outputs\n        defuzzified_outputs = {var: self.output_vars[var](fuzzy_outputs) for var in self.output_vars}\n        logger.info(f\"Fuzzy Logic evaluation result: {defuzzified_outputs}\")\n        return defuzzified_outputs\n    except Exception as e:\n        logger.error(f\"Error in Fuzzy Logic evaluation: {e}\")\n        return {}\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/fuzzy_logic/#engines.contentFilterEngine.probabilistic_statistical_methods.fuzzy_logic.FUZZY_LOGIC.recommend","title":"<code>recommend(input_values, top_n=10)</code>","text":"<p>Recommend actions based on fuzzy logic evaluation.</p> <p>Parameters: - input_values (Dict[str, int]): Dictionary with input values for fuzzy evaluation. - top_n (int): Number of top recommendations to return.</p> <p>Returns: - List[int]: List of recommended actions (placeholder for actual implementation).</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/fuzzy_logic.py</code> <pre><code>def recommend(self, input_values: Dict[str, int], top_n: int = 10) -&gt; List[int]:\n    \"\"\"\n    Recommend actions based on fuzzy logic evaluation.\n\n    Parameters:\n    - input_values (Dict[str, int]): Dictionary with input values for fuzzy evaluation.\n    - top_n (int): Number of top recommendations to return.\n\n    Returns:\n    - List[int]: List of recommended actions (placeholder for actual implementation).\n    \"\"\"\n    logger.info(\"Generating recommendations using Fuzzy Logic System.\")\n    evaluation = self.evaluate(input_values)\n    # Example: Recommend actions based on evaluation\n    # This requires a mapping from evaluation results to actions; here we return an empty list as a placeholder\n    recommendations = []  # Implement logic based on the application\n    logger.info(f\"Top {top_n} recommendations generated using Fuzzy Logic.\")\n    return recommendations[:top_n]\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/lda/","title":"LDA","text":""},{"location":"contentFilterEngine/probabilistic_statistical_methods/lda/#engines.contentFilterEngine.probabilistic_statistical_methods.lda.LDA","title":"<code>LDA</code>","text":"Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/lda.py</code> <pre><code>class LDA:\n    def __init__(self, n_components: int = 10, max_iter: int = 10):\n        \"\"\"\n        Initialize the LDA model with the specified number of topics.\n\n        Parameters:\n        - n_components (int): Number of topics.\n        - max_iter (int): Maximum number of iterations for the EM algorithm.\n        \"\"\"\n        self.vectorizer = CountVectorizer(stop_words='english')\n        self.lda_model = LatentDirichletAllocation(n_components=n_components, max_iter=max_iter, random_state=42)\n        logger.info(f\"LDA initialized with {n_components} topics and {max_iter} max iterations.\")\n\n    def fit(self, documents: List[str]):\n        \"\"\"\n        Fit the LDA model on the provided documents.\n\n        Parameters:\n        - documents (List[str]): List of documents to train the model.\n        \"\"\"\n        logger.info(\"Fitting LDA model on documents.\")\n        count_matrix = self.vectorizer.fit_transform(documents)\n        self.lda_model.fit(count_matrix)\n        logger.info(\"LDA model training completed.\")\n\n    def transform(self, documents: List[str]) -&gt; Any:\n        \"\"\"\n        Transform documents into the LDA topic space.\n\n        Parameters:\n        - documents (List[str]): List of documents to transform.\n\n        Returns:\n        - Transformed document matrix in topic space.\n        \"\"\"\n        logger.info(\"Transforming documents into LDA topic space.\")\n        count_matrix = self.vectorizer.transform(documents)\n        return self.lda_model.transform(count_matrix)\n\n    def recommend(self, query: str, top_n: int = 10) -&gt; List[int]:\n        \"\"\"\n        Recommend items based on the similarity of the query to the topics.\n\n        Parameters:\n        - query (str): The query text for which to generate recommendations.\n        - top_n (int): Number of top recommendations to return.\n\n        Returns:\n        - List[int]: List of recommended item indices.\n        \"\"\"\n        logger.info(\"Generating recommendations using LDA.\")\n        query_vec = self.transform([query])\n        topic_distribution = self.lda_model.transform(self.vectorizer.transform([query]))\n        similarity_scores = (topic_distribution @ self.lda_model.components_.T).flatten()\n        top_indices = similarity_scores.argsort()[::-1][:top_n]\n        logger.info(f\"Top {top_n} recommendations generated using LDA.\")\n        return top_indices.tolist()\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/lda/#engines.contentFilterEngine.probabilistic_statistical_methods.lda.LDA.__init__","title":"<code>__init__(n_components=10, max_iter=10)</code>","text":"<p>Initialize the LDA model with the specified number of topics.</p> <p>Parameters: - n_components (int): Number of topics. - max_iter (int): Maximum number of iterations for the EM algorithm.</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/lda.py</code> <pre><code>def __init__(self, n_components: int = 10, max_iter: int = 10):\n    \"\"\"\n    Initialize the LDA model with the specified number of topics.\n\n    Parameters:\n    - n_components (int): Number of topics.\n    - max_iter (int): Maximum number of iterations for the EM algorithm.\n    \"\"\"\n    self.vectorizer = CountVectorizer(stop_words='english')\n    self.lda_model = LatentDirichletAllocation(n_components=n_components, max_iter=max_iter, random_state=42)\n    logger.info(f\"LDA initialized with {n_components} topics and {max_iter} max iterations.\")\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/lda/#engines.contentFilterEngine.probabilistic_statistical_methods.lda.LDA.fit","title":"<code>fit(documents)</code>","text":"<p>Fit the LDA model on the provided documents.</p> <p>Parameters: - documents (List[str]): List of documents to train the model.</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/lda.py</code> <pre><code>def fit(self, documents: List[str]):\n    \"\"\"\n    Fit the LDA model on the provided documents.\n\n    Parameters:\n    - documents (List[str]): List of documents to train the model.\n    \"\"\"\n    logger.info(\"Fitting LDA model on documents.\")\n    count_matrix = self.vectorizer.fit_transform(documents)\n    self.lda_model.fit(count_matrix)\n    logger.info(\"LDA model training completed.\")\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/lda/#engines.contentFilterEngine.probabilistic_statistical_methods.lda.LDA.recommend","title":"<code>recommend(query, top_n=10)</code>","text":"<p>Recommend items based on the similarity of the query to the topics.</p> <p>Parameters: - query (str): The query text for which to generate recommendations. - top_n (int): Number of top recommendations to return.</p> <p>Returns: - List[int]: List of recommended item indices.</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/lda.py</code> <pre><code>def recommend(self, query: str, top_n: int = 10) -&gt; List[int]:\n    \"\"\"\n    Recommend items based on the similarity of the query to the topics.\n\n    Parameters:\n    - query (str): The query text for which to generate recommendations.\n    - top_n (int): Number of top recommendations to return.\n\n    Returns:\n    - List[int]: List of recommended item indices.\n    \"\"\"\n    logger.info(\"Generating recommendations using LDA.\")\n    query_vec = self.transform([query])\n    topic_distribution = self.lda_model.transform(self.vectorizer.transform([query]))\n    similarity_scores = (topic_distribution @ self.lda_model.components_.T).flatten()\n    top_indices = similarity_scores.argsort()[::-1][:top_n]\n    logger.info(f\"Top {top_n} recommendations generated using LDA.\")\n    return top_indices.tolist()\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/lda/#engines.contentFilterEngine.probabilistic_statistical_methods.lda.LDA.transform","title":"<code>transform(documents)</code>","text":"<p>Transform documents into the LDA topic space.</p> <p>Parameters: - documents (List[str]): List of documents to transform.</p> <p>Returns: - Transformed document matrix in topic space.</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/lda.py</code> <pre><code>def transform(self, documents: List[str]) -&gt; Any:\n    \"\"\"\n    Transform documents into the LDA topic space.\n\n    Parameters:\n    - documents (List[str]): List of documents to transform.\n\n    Returns:\n    - Transformed document matrix in topic space.\n    \"\"\"\n    logger.info(\"Transforming documents into LDA topic space.\")\n    count_matrix = self.vectorizer.transform(documents)\n    return self.lda_model.transform(count_matrix)\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/lsa/","title":"Lsa","text":""},{"location":"contentFilterEngine/probabilistic_statistical_methods/lsa/#engines.contentFilterEngine.probabilistic_statistical_methods.lsa.LSA","title":"<code>LSA</code>","text":"Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/lsa.py</code> <pre><code>class LSA:\n    def __init__(self, n_components: int = 100):\n        \"\"\"\n        Initialize the LSA model with the specified number of components.\n\n        Parameters:\n        - n_components (int): Number of latent components to extract.\n        \"\"\"\n        self.vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n        self.lsa_model = TruncatedSVD(n_components=n_components, random_state=42)\n        self.item_ids = []\n        logger.info(f\"LSA initialized with {n_components} components.\")\n\n    def fit(self, documents: List[str]):\n        \"\"\"\n        Fit the LSA model on the provided documents.\n\n        Parameters:\n        - documents (List[str]): List of documents to train the model.\n        \"\"\"\n        logger.info(\"Fitting LSA model on documents.\")\n        tfidf_matrix = self.vectorizer.fit_transform(documents)\n        self.lsa_model.fit(tfidf_matrix)\n        logger.info(\"LSA model training completed.\")\n\n    def transform(self, documents: List[str]) -&gt; Any:\n        \"\"\"\n        Transform documents into the LSA latent space.\n\n        Parameters:\n        - documents (List[str]): List of documents to transform.\n\n        Returns:\n        - Transformed document matrix in latent space.\n        \"\"\"\n        logger.info(\"Transforming documents into LSA latent space.\")\n        tfidf_matrix = self.vectorizer.transform(documents)\n        return self.lsa_model.transform(tfidf_matrix)\n\n    def recommend(self, query: str, top_n: int = 10) -&gt; List[int]:\n        \"\"\"\n        Recommend items based on the similarity of the query to the documents.\n\n        Parameters:\n        - query (str): The query text for which to generate recommendations.\n        - top_n (int): Number of top recommendations to return.\n\n        Returns:\n        - List[int]: List of recommended item indices.\n        \"\"\"\n        logger.info(\"Generating recommendations using LSA.\")\n        query_vec = self.transform([query])\n        doc_vecs = self.lsa_model.transform(self.vectorizer.transform(self.vectorizer.get_feature_names_out()))\n        similarity_scores = (doc_vecs @ query_vec.T).flatten()\n        top_indices = similarity_scores.argsort()[::-1][:top_n]\n        logger.info(f\"Top {top_n} recommendations generated using LSA.\")\n        return top_indices.tolist()\n\n    def add_item(self, item_id: int, item_features: Dict[str, Any]):\n        logger.info(f\"Adding item {item_id} to LSA.\")\n        # Assuming item_features contains 'genres' as a list\n        genres = ' '.join(item_features.get('genres', []))\n        new_tfidf = self.vectorizer.transform([genres])  # Use transform, not fit\n        new_vec = self.lsa_model.transform(new_tfidf)\n        # You would need to handle incorporating the new_vec into the existing model\n        # This is a placeholder for actual implementation\n        logger.info(f\"Item {item_id} added to LSA successfully.\")\n\n    def remove_item(self, item_id: int):\n        logger.info(f\"Removing item {item_id} from LSA.\")\n        # Placeholder for actual implementation\n        logger.info(f\"Item {item_id} removed from LSA successfully.\")\n\n    def update_item_features(self, item_id: int, new_features: Dict[str, Any]):\n        logger.info(f\"Updating features for item {item_id} in LSA.\")\n        # Placeholder for actual implementation\n        logger.info(f\"Item {item_id} features updated in LSA successfully.\")\n\n    def update_user_profile(self, user_id: int, item_id: int, feedback_score: float):\n        logger.info(f\"Updating user {user_id}'s profile based on feedback for item {item_id}.\")\n        # Placeholder for actual implementation\n        logger.info(f\"User {user_id}'s profile updated successfully.\")\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/lsa/#engines.contentFilterEngine.probabilistic_statistical_methods.lsa.LSA.__init__","title":"<code>__init__(n_components=100)</code>","text":"<p>Initialize the LSA model with the specified number of components.</p> <p>Parameters: - n_components (int): Number of latent components to extract.</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/lsa.py</code> <pre><code>def __init__(self, n_components: int = 100):\n    \"\"\"\n    Initialize the LSA model with the specified number of components.\n\n    Parameters:\n    - n_components (int): Number of latent components to extract.\n    \"\"\"\n    self.vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n    self.lsa_model = TruncatedSVD(n_components=n_components, random_state=42)\n    self.item_ids = []\n    logger.info(f\"LSA initialized with {n_components} components.\")\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/lsa/#engines.contentFilterEngine.probabilistic_statistical_methods.lsa.LSA.fit","title":"<code>fit(documents)</code>","text":"<p>Fit the LSA model on the provided documents.</p> <p>Parameters: - documents (List[str]): List of documents to train the model.</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/lsa.py</code> <pre><code>def fit(self, documents: List[str]):\n    \"\"\"\n    Fit the LSA model on the provided documents.\n\n    Parameters:\n    - documents (List[str]): List of documents to train the model.\n    \"\"\"\n    logger.info(\"Fitting LSA model on documents.\")\n    tfidf_matrix = self.vectorizer.fit_transform(documents)\n    self.lsa_model.fit(tfidf_matrix)\n    logger.info(\"LSA model training completed.\")\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/lsa/#engines.contentFilterEngine.probabilistic_statistical_methods.lsa.LSA.recommend","title":"<code>recommend(query, top_n=10)</code>","text":"<p>Recommend items based on the similarity of the query to the documents.</p> <p>Parameters: - query (str): The query text for which to generate recommendations. - top_n (int): Number of top recommendations to return.</p> <p>Returns: - List[int]: List of recommended item indices.</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/lsa.py</code> <pre><code>def recommend(self, query: str, top_n: int = 10) -&gt; List[int]:\n    \"\"\"\n    Recommend items based on the similarity of the query to the documents.\n\n    Parameters:\n    - query (str): The query text for which to generate recommendations.\n    - top_n (int): Number of top recommendations to return.\n\n    Returns:\n    - List[int]: List of recommended item indices.\n    \"\"\"\n    logger.info(\"Generating recommendations using LSA.\")\n    query_vec = self.transform([query])\n    doc_vecs = self.lsa_model.transform(self.vectorizer.transform(self.vectorizer.get_feature_names_out()))\n    similarity_scores = (doc_vecs @ query_vec.T).flatten()\n    top_indices = similarity_scores.argsort()[::-1][:top_n]\n    logger.info(f\"Top {top_n} recommendations generated using LSA.\")\n    return top_indices.tolist()\n</code></pre>"},{"location":"contentFilterEngine/probabilistic_statistical_methods/lsa/#engines.contentFilterEngine.probabilistic_statistical_methods.lsa.LSA.transform","title":"<code>transform(documents)</code>","text":"<p>Transform documents into the LSA latent space.</p> <p>Parameters: - documents (List[str]): List of documents to transform.</p> <p>Returns: - Transformed document matrix in latent space.</p> Source code in <code>engines/contentFilterEngine/probabilistic_statistical_methods/lsa.py</code> <pre><code>def transform(self, documents: List[str]) -&gt; Any:\n    \"\"\"\n    Transform documents into the LSA latent space.\n\n    Parameters:\n    - documents (List[str]): List of documents to transform.\n\n    Returns:\n    - Transformed document matrix in latent space.\n    \"\"\"\n    logger.info(\"Transforming documents into LSA latent space.\")\n    tfidf_matrix = self.vectorizer.transform(documents)\n    return self.lsa_model.transform(tfidf_matrix)\n</code></pre>"},{"location":"contentFilterEngine/special_techniques/dynamic_filtering/","title":"Dynamic Filtering","text":""},{"location":"contentFilterEngine/special_techniques/dynamic_filtering/#engines.contentFilterEngine.special_techniques.dynamic_filtering.DynamicFilteringRecommender","title":"<code>DynamicFilteringRecommender</code>","text":"Source code in <code>engines/contentFilterEngine/special_techniques/dynamic_filtering.py</code> <pre><code>class DynamicFilteringRecommender:\n    def __init__(self, base_recommender: Any):\n        \"\"\"\n        Initialize the DynamicFilteringRecommender with a base recommender.\n\n        Parameters:\n        - base_recommender (Any): An instance of a base recommender (e.g., LSA, LDA).\n        \"\"\"\n        self.base_recommender = base_recommender\n        # Keeps track of items that have been added or removed\n        self.added_items: List[int] = []\n        self.removed_items: List[int] = []\n        logger.info(\"DynamicFilteringRecommender initialized with base recommender.\")\n\n    def add_item(self, item_id: int, item_features: Dict[str, Any]):\n        \"\"\"\n        Add a new item to the recommender system.\n\n        Parameters:\n        - item_id (int): The ID of the new item.\n        - item_features (Dict[str, Any]): The features of the new item.\n        \"\"\"\n        logger.info(f\"Adding item {item_id} to the base recommender.\")\n        if hasattr(self.base_recommender, 'add_item'):\n            self.base_recommender.add_item(item_id, item_features)\n            self.added_items.append(item_id)\n            logger.info(f\"Item {item_id} added to the base recommender successfully.\")\n        else:\n            logger.warning(\"Base recommender does not support adding items dynamically.\")\n\n    def remove_item(self, item_id: int):\n        \"\"\"\n        Remove an existing item from the recommender system.\n\n        Parameters:\n        - item_id (int): The ID of the item to remove.\n        \"\"\"\n        logger.info(f\"Removing item {item_id} from the base recommender.\")\n        if hasattr(self.base_recommender, 'remove_item'):\n            self.base_recommender.remove_item(item_id)\n            self.removed_items.append(item_id)\n            logger.info(f\"Item {item_id} removed from the base recommender successfully.\")\n        else:\n            logger.warning(\"Base recommender does not support removing items dynamically.\")\n\n    def update_item_features(self, item_id: int, new_features: Dict[str, Any]):\n        \"\"\"\n        Update the features of an existing item.\n\n        Parameters:\n        - item_id (int): The ID of the item to update.\n        - new_features (Dict[str, Any]): The updated features of the item.\n        \"\"\"\n        try:\n            if hasattr(self.base_recommender, 'update_item_features'):\n                self.base_recommender.update_item_features(item_id, new_features)\n                logger.info(f\"Updated features for item {item_id} in the base recommender.\")\n            else:\n                logger.warning(\"Base recommender does not support updating item features dynamically.\")\n        except Exception as e:\n            logger.error(f\"Error updating features for item {item_id}: {e}\")\n\n    def handle_data_change(self, event: Dict[str, Any]):\n        \"\"\"\n        Handle dynamic data changes such as adding or removing items.\n\n        Parameters:\n        - event (Dict[str, Any]): A dictionary containing the type of event and relevant data.\n          Example:\n          {\n              'action': 'add',\n              'item_id': 123,\n              'item_features': {'genre': 'Comedy', 'duration': 120}\n          }\n        \"\"\"\n        action = event.get('action')\n        if action == 'add':\n            self.add_item(event['item_id'], event['item_features'])\n        elif action == 'remove':\n            self.remove_item(event['item_id'])\n        elif action == 'update':\n            self.update_item_features(event['item_id'], event['item_features'])\n        else:\n            logger.warning(f\"Unsupported event action: {action}\")\n\n    def recommend(self, user_id: int, query: str, top_n: int = 10) -&gt; List[int]:\n        \"\"\"\n        Generate top-N item recommendations for a user, considering dynamic changes.\n\n        Parameters:\n        - user_id (int): The ID of the user.\n        - query (str): The query text for generating recommendations.\n        - top_n (int): Number of top recommendations to return.\n\n        Returns:\n        - List[int]: List of recommended item IDs.\n        \"\"\"\n        logger.info(f\"Generating recommendations for user {user_id} with query '{query}' using DynamicFilteringRecommender.\")\n        return self.base_recommender.recommend(query, top_n=top_n)\n</code></pre>"},{"location":"contentFilterEngine/special_techniques/dynamic_filtering/#engines.contentFilterEngine.special_techniques.dynamic_filtering.DynamicFilteringRecommender.__init__","title":"<code>__init__(base_recommender)</code>","text":"<p>Initialize the DynamicFilteringRecommender with a base recommender.</p> <p>Parameters: - base_recommender (Any): An instance of a base recommender (e.g., LSA, LDA).</p> Source code in <code>engines/contentFilterEngine/special_techniques/dynamic_filtering.py</code> <pre><code>def __init__(self, base_recommender: Any):\n    \"\"\"\n    Initialize the DynamicFilteringRecommender with a base recommender.\n\n    Parameters:\n    - base_recommender (Any): An instance of a base recommender (e.g., LSA, LDA).\n    \"\"\"\n    self.base_recommender = base_recommender\n    # Keeps track of items that have been added or removed\n    self.added_items: List[int] = []\n    self.removed_items: List[int] = []\n    logger.info(\"DynamicFilteringRecommender initialized with base recommender.\")\n</code></pre>"},{"location":"contentFilterEngine/special_techniques/dynamic_filtering/#engines.contentFilterEngine.special_techniques.dynamic_filtering.DynamicFilteringRecommender.add_item","title":"<code>add_item(item_id, item_features)</code>","text":"<p>Add a new item to the recommender system.</p> <p>Parameters: - item_id (int): The ID of the new item. - item_features (Dict[str, Any]): The features of the new item.</p> Source code in <code>engines/contentFilterEngine/special_techniques/dynamic_filtering.py</code> <pre><code>def add_item(self, item_id: int, item_features: Dict[str, Any]):\n    \"\"\"\n    Add a new item to the recommender system.\n\n    Parameters:\n    - item_id (int): The ID of the new item.\n    - item_features (Dict[str, Any]): The features of the new item.\n    \"\"\"\n    logger.info(f\"Adding item {item_id} to the base recommender.\")\n    if hasattr(self.base_recommender, 'add_item'):\n        self.base_recommender.add_item(item_id, item_features)\n        self.added_items.append(item_id)\n        logger.info(f\"Item {item_id} added to the base recommender successfully.\")\n    else:\n        logger.warning(\"Base recommender does not support adding items dynamically.\")\n</code></pre>"},{"location":"contentFilterEngine/special_techniques/dynamic_filtering/#engines.contentFilterEngine.special_techniques.dynamic_filtering.DynamicFilteringRecommender.handle_data_change","title":"<code>handle_data_change(event)</code>","text":"<p>Handle dynamic data changes such as adding or removing items.</p> <ul> <li>event (Dict[str, Any]): A dictionary containing the type of event and relevant data.   Example:   {       'action': 'add',       'item_id': 123,       'item_features': {'genre': 'Comedy', 'duration': 120}   }</li> </ul> Source code in <code>engines/contentFilterEngine/special_techniques/dynamic_filtering.py</code> <pre><code>def handle_data_change(self, event: Dict[str, Any]):\n    \"\"\"\n    Handle dynamic data changes such as adding or removing items.\n\n    Parameters:\n    - event (Dict[str, Any]): A dictionary containing the type of event and relevant data.\n      Example:\n      {\n          'action': 'add',\n          'item_id': 123,\n          'item_features': {'genre': 'Comedy', 'duration': 120}\n      }\n    \"\"\"\n    action = event.get('action')\n    if action == 'add':\n        self.add_item(event['item_id'], event['item_features'])\n    elif action == 'remove':\n        self.remove_item(event['item_id'])\n    elif action == 'update':\n        self.update_item_features(event['item_id'], event['item_features'])\n    else:\n        logger.warning(f\"Unsupported event action: {action}\")\n</code></pre>"},{"location":"contentFilterEngine/special_techniques/dynamic_filtering/#engines.contentFilterEngine.special_techniques.dynamic_filtering.DynamicFilteringRecommender.recommend","title":"<code>recommend(user_id, query, top_n=10)</code>","text":"<p>Generate top-N item recommendations for a user, considering dynamic changes.</p> <p>Parameters: - user_id (int): The ID of the user. - query (str): The query text for generating recommendations. - top_n (int): Number of top recommendations to return.</p> <p>Returns: - List[int]: List of recommended item IDs.</p> Source code in <code>engines/contentFilterEngine/special_techniques/dynamic_filtering.py</code> <pre><code>def recommend(self, user_id: int, query: str, top_n: int = 10) -&gt; List[int]:\n    \"\"\"\n    Generate top-N item recommendations for a user, considering dynamic changes.\n\n    Parameters:\n    - user_id (int): The ID of the user.\n    - query (str): The query text for generating recommendations.\n    - top_n (int): Number of top recommendations to return.\n\n    Returns:\n    - List[int]: List of recommended item IDs.\n    \"\"\"\n    logger.info(f\"Generating recommendations for user {user_id} with query '{query}' using DynamicFilteringRecommender.\")\n    return self.base_recommender.recommend(query, top_n=top_n)\n</code></pre>"},{"location":"contentFilterEngine/special_techniques/dynamic_filtering/#engines.contentFilterEngine.special_techniques.dynamic_filtering.DynamicFilteringRecommender.remove_item","title":"<code>remove_item(item_id)</code>","text":"<p>Remove an existing item from the recommender system.</p> <p>Parameters: - item_id (int): The ID of the item to remove.</p> Source code in <code>engines/contentFilterEngine/special_techniques/dynamic_filtering.py</code> <pre><code>def remove_item(self, item_id: int):\n    \"\"\"\n    Remove an existing item from the recommender system.\n\n    Parameters:\n    - item_id (int): The ID of the item to remove.\n    \"\"\"\n    logger.info(f\"Removing item {item_id} from the base recommender.\")\n    if hasattr(self.base_recommender, 'remove_item'):\n        self.base_recommender.remove_item(item_id)\n        self.removed_items.append(item_id)\n        logger.info(f\"Item {item_id} removed from the base recommender successfully.\")\n    else:\n        logger.warning(\"Base recommender does not support removing items dynamically.\")\n</code></pre>"},{"location":"contentFilterEngine/special_techniques/dynamic_filtering/#engines.contentFilterEngine.special_techniques.dynamic_filtering.DynamicFilteringRecommender.update_item_features","title":"<code>update_item_features(item_id, new_features)</code>","text":"<p>Update the features of an existing item.</p> <p>Parameters: - item_id (int): The ID of the item to update. - new_features (Dict[str, Any]): The updated features of the item.</p> Source code in <code>engines/contentFilterEngine/special_techniques/dynamic_filtering.py</code> <pre><code>def update_item_features(self, item_id: int, new_features: Dict[str, Any]):\n    \"\"\"\n    Update the features of an existing item.\n\n    Parameters:\n    - item_id (int): The ID of the item to update.\n    - new_features (Dict[str, Any]): The updated features of the item.\n    \"\"\"\n    try:\n        if hasattr(self.base_recommender, 'update_item_features'):\n            self.base_recommender.update_item_features(item_id, new_features)\n            logger.info(f\"Updated features for item {item_id} in the base recommender.\")\n        else:\n            logger.warning(\"Base recommender does not support updating item features dynamically.\")\n    except Exception as e:\n        logger.error(f\"Error updating features for item {item_id}: {e}\")\n</code></pre>"},{"location":"contentFilterEngine/special_techniques/interactive_filtering/","title":"Interactive Filtering","text":""},{"location":"contentFilterEngine/special_techniques/interactive_filtering/#engines.contentFilterEngine.special_techniques.interactive_filtering.InteractiveFilteringRecommender","title":"<code>InteractiveFilteringRecommender</code>","text":"Source code in <code>engines/contentFilterEngine/special_techniques/interactive_filtering.py</code> <pre><code>class InteractiveFilteringRecommender:\n    def __init__(self, base_recommender: Any):\n        \"\"\"\n        Initialize the InteractiveFilteringRecommender with a base recommender.\n\n        Parameters:\n        - base_recommender (Any): An instance of a base recommender (e.g., LSA, LDA).\n        \"\"\"\n        self.base_recommender = base_recommender\n        # Stores user feedback in the format {user_id: {item_id: feedback_score}}\n        self.user_feedback: Dict[int, Dict[int, float]] = {}\n        logger.info(\"InteractiveFilteringRecommender initialized with base recommender.\")\n\n    def collect_feedback(self, user_id: int, item_id: int, feedback_score: float):\n        \"\"\"\n        Collect feedback from the user for a specific item.\n\n        Parameters:\n        - user_id (int): The ID of the user.\n        - item_id (int): The ID of the item.\n        - feedback_score (float): The feedback score (e.g., 1.0 for positive, -1.0 for negative).\n        \"\"\"\n        if user_id not in self.user_feedback:\n            self.user_feedback[user_id] = {}\n        self.user_feedback[user_id][item_id] = feedback_score\n        logger.info(f\"Collected feedback from user {user_id} for item {item_id}: {feedback_score}\")\n\n        # Update the base recommender based on feedback\n        self.update_recommender(user_id, item_id, feedback_score)\n\n    def update_recommender(self, user_id: int, item_id: int, feedback_score: float):\n        \"\"\"\n        Update the base recommender system based on user feedback.\n\n        Parameters:\n        - user_id (int): The ID of the user.\n        - item_id (int): The ID of the item.\n        - feedback_score (float): The feedback score.\n        \"\"\"\n        try:\n            # Example: Adjust user profile or model based on feedback\n            # This is a placeholder for actual implementation\n            logger.info(f\"Updating base recommender for user {user_id} based on feedback.\")\n            if hasattr(self.base_recommender, 'update_user_profile'):\n                self.base_recommender.update_user_profile(user_id, item_id, feedback_score)\n            else:\n                logger.warning(\"Base recommender does not support updating user profiles.\")\n        except Exception as e:\n            logger.error(f\"Error updating recommender based on feedback: {e}\")\n\n    def recommend(self, user_id: int, query: str, top_n: int = 10) -&gt; List[int]:\n        \"\"\"\n        Generate top-N item recommendations for a user, considering their feedback.\n\n        Parameters:\n        - user_id (int): The ID of the user.\n        - query (str): The query text for generating recommendations.\n        - top_n (int): Number of top recommendations to return.\n\n        Returns:\n        - List[int]: List of recommended item IDs.\n        \"\"\"\n        logger.info(f\"Generating recommendations for user {user_id} with query '{query}' using InteractiveFilteringRecommender.\")\n        # Get base recommendations\n        base_recommendations = self.base_recommender.recommend(user_id, query, top_n=top_n * 2)\n\n        if user_id in self.user_feedback:\n            # Filter out items with negative feedback\n            filtered_recommendations = [\n                item_id for item_id in base_recommendations\n                if self.user_feedback[user_id].get(item_id, 0) &gt;= 0\n            ]\n            logger.info(f\"Filtered recommendations for user {user_id}: {filtered_recommendations}\")\n            return filtered_recommendations[:top_n]\n        else:\n            return base_recommendations[:top_n]\n</code></pre>"},{"location":"contentFilterEngine/special_techniques/interactive_filtering/#engines.contentFilterEngine.special_techniques.interactive_filtering.InteractiveFilteringRecommender.__init__","title":"<code>__init__(base_recommender)</code>","text":"<p>Initialize the InteractiveFilteringRecommender with a base recommender.</p> <p>Parameters: - base_recommender (Any): An instance of a base recommender (e.g., LSA, LDA).</p> Source code in <code>engines/contentFilterEngine/special_techniques/interactive_filtering.py</code> <pre><code>def __init__(self, base_recommender: Any):\n    \"\"\"\n    Initialize the InteractiveFilteringRecommender with a base recommender.\n\n    Parameters:\n    - base_recommender (Any): An instance of a base recommender (e.g., LSA, LDA).\n    \"\"\"\n    self.base_recommender = base_recommender\n    # Stores user feedback in the format {user_id: {item_id: feedback_score}}\n    self.user_feedback: Dict[int, Dict[int, float]] = {}\n    logger.info(\"InteractiveFilteringRecommender initialized with base recommender.\")\n</code></pre>"},{"location":"contentFilterEngine/special_techniques/interactive_filtering/#engines.contentFilterEngine.special_techniques.interactive_filtering.InteractiveFilteringRecommender.collect_feedback","title":"<code>collect_feedback(user_id, item_id, feedback_score)</code>","text":"<p>Collect feedback from the user for a specific item.</p> <p>Parameters: - user_id (int): The ID of the user. - item_id (int): The ID of the item. - feedback_score (float): The feedback score (e.g., 1.0 for positive, -1.0 for negative).</p> Source code in <code>engines/contentFilterEngine/special_techniques/interactive_filtering.py</code> <pre><code>def collect_feedback(self, user_id: int, item_id: int, feedback_score: float):\n    \"\"\"\n    Collect feedback from the user for a specific item.\n\n    Parameters:\n    - user_id (int): The ID of the user.\n    - item_id (int): The ID of the item.\n    - feedback_score (float): The feedback score (e.g., 1.0 for positive, -1.0 for negative).\n    \"\"\"\n    if user_id not in self.user_feedback:\n        self.user_feedback[user_id] = {}\n    self.user_feedback[user_id][item_id] = feedback_score\n    logger.info(f\"Collected feedback from user {user_id} for item {item_id}: {feedback_score}\")\n\n    # Update the base recommender based on feedback\n    self.update_recommender(user_id, item_id, feedback_score)\n</code></pre>"},{"location":"contentFilterEngine/special_techniques/interactive_filtering/#engines.contentFilterEngine.special_techniques.interactive_filtering.InteractiveFilteringRecommender.recommend","title":"<code>recommend(user_id, query, top_n=10)</code>","text":"<p>Generate top-N item recommendations for a user, considering their feedback.</p> <p>Parameters: - user_id (int): The ID of the user. - query (str): The query text for generating recommendations. - top_n (int): Number of top recommendations to return.</p> <p>Returns: - List[int]: List of recommended item IDs.</p> Source code in <code>engines/contentFilterEngine/special_techniques/interactive_filtering.py</code> <pre><code>def recommend(self, user_id: int, query: str, top_n: int = 10) -&gt; List[int]:\n    \"\"\"\n    Generate top-N item recommendations for a user, considering their feedback.\n\n    Parameters:\n    - user_id (int): The ID of the user.\n    - query (str): The query text for generating recommendations.\n    - top_n (int): Number of top recommendations to return.\n\n    Returns:\n    - List[int]: List of recommended item IDs.\n    \"\"\"\n    logger.info(f\"Generating recommendations for user {user_id} with query '{query}' using InteractiveFilteringRecommender.\")\n    # Get base recommendations\n    base_recommendations = self.base_recommender.recommend(user_id, query, top_n=top_n * 2)\n\n    if user_id in self.user_feedback:\n        # Filter out items with negative feedback\n        filtered_recommendations = [\n            item_id for item_id in base_recommendations\n            if self.user_feedback[user_id].get(item_id, 0) &gt;= 0\n        ]\n        logger.info(f\"Filtered recommendations for user {user_id}: {filtered_recommendations}\")\n        return filtered_recommendations[:top_n]\n    else:\n        return base_recommendations[:top_n]\n</code></pre>"},{"location":"contentFilterEngine/special_techniques/interactive_filtering/#engines.contentFilterEngine.special_techniques.interactive_filtering.InteractiveFilteringRecommender.update_recommender","title":"<code>update_recommender(user_id, item_id, feedback_score)</code>","text":"<p>Update the base recommender system based on user feedback.</p> <p>Parameters: - user_id (int): The ID of the user. - item_id (int): The ID of the item. - feedback_score (float): The feedback score.</p> Source code in <code>engines/contentFilterEngine/special_techniques/interactive_filtering.py</code> <pre><code>def update_recommender(self, user_id: int, item_id: int, feedback_score: float):\n    \"\"\"\n    Update the base recommender system based on user feedback.\n\n    Parameters:\n    - user_id (int): The ID of the user.\n    - item_id (int): The ID of the item.\n    - feedback_score (float): The feedback score.\n    \"\"\"\n    try:\n        # Example: Adjust user profile or model based on feedback\n        # This is a placeholder for actual implementation\n        logger.info(f\"Updating base recommender for user {user_id} based on feedback.\")\n        if hasattr(self.base_recommender, 'update_user_profile'):\n            self.base_recommender.update_user_profile(user_id, item_id, feedback_score)\n        else:\n            logger.warning(\"Base recommender does not support updating user profiles.\")\n    except Exception as e:\n        logger.error(f\"Error updating recommender based on feedback: {e}\")\n</code></pre>"},{"location":"contentFilterEngine/special_techniques/temporal_filtering/","title":"Temporal Filtering","text":""},{"location":"contentFilterEngine/special_techniques/temporal_filtering/#engines.contentFilterEngine.special_techniques.temporal_filtering.TemporalFilteringRecommender","title":"<code>TemporalFilteringRecommender</code>","text":"<p>TemporalFilteringRecommender is a class designed to provide recommendations based on temporal filtering techniques. This class is part of a content  filtering engine that utilizes time-based data to enhance the relevance of  recommendations.</p> <p>The primary goal of temporal filtering is to incorporate the dimension of  time into the recommendation process, allowing for more dynamic and  contextually relevant suggestions. This can be particularly useful in  scenarios where user preferences or item popularity change over time.</p> Usage <p>(Provide a brief example of how to use this class, if applicable)</p> Note <p>This class is currently a placeholder and does not contain any  implemented methods. Future versions will include methods for fitting  the model to data, making predictions, and updating recommendations  based on new temporal data.</p> Source code in <code>engines/contentFilterEngine/special_techniques/temporal_filtering.py</code> <pre><code>class TemporalFilteringRecommender:\n    \"\"\"\n    TemporalFilteringRecommender is a class designed to provide recommendations\n    based on temporal filtering techniques. This class is part of a content \n    filtering engine that utilizes time-based data to enhance the relevance of \n    recommendations.\n\n    The primary goal of temporal filtering is to incorporate the dimension of \n    time into the recommendation process, allowing for more dynamic and \n    contextually relevant suggestions. This can be particularly useful in \n    scenarios where user preferences or item popularity change over time.\n\n    Attributes:\n        (Define any attributes here, if applicable, e.g., time_window, data_source)\n\n    Methods:\n        (List any methods here, if applicable, e.g., fit, predict, update)\n\n    Usage:\n        (Provide a brief example of how to use this class, if applicable)\n\n    Note:\n        This class is currently a placeholder and does not contain any \n        implemented methods. Future versions will include methods for fitting \n        the model to data, making predictions, and updating recommendations \n        based on new temporal data.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"contentFilterEngine/traditional_ml_algorithms/LR/","title":"LR","text":"<p>LR.py</p> <p>This module implements the Logistic Regression algorithm, a popular supervised learning method used for binary classification tasks. Logistic Regression is a linear model that estimates the probability of a binary response based on one or more predictor variables.</p> Usage <p>To use this module, import the <code>LR</code> class and instantiate it with the desired parameters. Then, fit the model to your training data and use it to make predictions on new data.</p> <p>Example:     from engines.contentFilterEngine.traditional_ml_algorithms.LR import LR     model = LR()     model.fit(X_train, y_train)     predictions = model.predict(X_test)</p> Note <ul> <li>Ensure that your data is preprocessed appropriately before fitting the model, as Logistic Regression assumes a linear relationship between the input variables and the log-odds of the response.</li> <li>This implementation may require additional libraries such as NumPy or SciPy for matrix operations.</li> </ul>"},{"location":"contentFilterEngine/traditional_ml_algorithms/decision_tree/","title":"Decision Tree","text":"<p>Decision Tree is to be implemented here.</p>"},{"location":"contentFilterEngine/traditional_ml_algorithms/lightgbm/","title":"LightGBM","text":""},{"location":"contentFilterEngine/traditional_ml_algorithms/lightgbm/#engines.contentFilterEngine.traditional_ml_algorithms.lightgbm.endLine","title":"<code>endLine: 2</code>  <code>module-attribute</code>","text":"<p>lightgbm.py</p> <p>This module is intended to implement the LightGBM algorithm, a gradient boosting framework that uses tree-based learning algorithms. It is designed for distributed and efficient training of large datasets.</p> Usage <p>Import the <code>LIGHTGBM</code> class and configure it with the necessary parameters. Train the model on your dataset and use it for predictions.</p> <p>Example:     from engines.contentFilterEngine.traditional_ml_algorithms.lightgbm import LIGHTGBM     model = LIGHTGBM()     model.fit(X_train, y_train)     predictions = model.predict(X_test)</p> Note <ul> <li>This file currently contains a placeholder and requires a full implementation of the LightGBM algorithm.</li> <li>LightGBM is particularly useful for large datasets and can handle categorical features directly.</li> </ul>"},{"location":"contentFilterEngine/traditional_ml_algorithms/svm/","title":"SVM","text":""},{"location":"contentFilterEngine/traditional_ml_algorithms/svm/#engines.contentFilterEngine.traditional_ml_algorithms.svm.endLine","title":"<code>endLine: 2</code>  <code>module-attribute</code>","text":"<p>svm.py</p> <p>This module provides a basic implementation of the Support Vector Machine (SVM) algorithm, which is used for classification and regression tasks. SVMs are effective in high-dimensional spaces and are versatile due to the use of different kernel functions.</p> Usage <p>Import the <code>SVM</code> class and create an instance with the desired kernel and parameters. Fit the model to your data and use it for predictions.</p> <p>Example:     from engines.contentFilterEngine.traditional_ml_algorithms.svm import SVM     model = SVM(kernel='linear')     model.fit(X_train, y_train)     predictions = model.predict(X_test)</p> Note <ul> <li>The current implementation is a placeholder and needs to be completed with actual SVM logic.</li> <li>Consider using libraries like scikit-learn for a more comprehensive SVM implementation.</li> </ul>"},{"location":"contentFilterEngine/traditional_ml_algorithms/tfidf/","title":"TFIDF","text":""},{"location":"contentFilterEngine/traditional_ml_algorithms/tfidf/#engines.contentFilterEngine.traditional_ml_algorithms.tfidf.endLine","title":"<code>endLine: 2</code>  <code>module-attribute</code>","text":"<p>tfidf.py</p> <p>This module is intended to implement the TF-IDF (Term Frequency-Inverse Document Frequency) algorithm, a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents.</p> Usage <p>Import the <code>TFIDF</code> class, fit it to your corpus, and transform your documents into TF-IDF vectors.</p> <p>Example:     from engines.contentFilterEngine.traditional_ml_algorithms.tfidf import TFIDF     vectorizer = TFIDF()     vectorizer.fit(corpus)     tfidf_matrix = vectorizer.transform(documents)</p> Note <ul> <li>This file currently contains a placeholder and requires a full implementation of the TF-IDF algorithm.</li> <li>TF-IDF is commonly used in text mining and information retrieval applications.</li> </ul>"},{"location":"contentFilterEngine/traditional_ml_algorithms/vw/","title":"VW","text":""},{"location":"contentFilterEngine/traditional_ml_algorithms/vw/#engines.contentFilterEngine.traditional_ml_algorithms.vw.endLine","title":"<code>endLine: 2</code>  <code>module-attribute</code>","text":"<p>vw.py</p> <p>This module is intended to implement the Vowpal Wabbit (VW) algorithm, a fast and scalable machine learning system that supports online learning and large-scale data.</p> Usage <p>Import the <code>VW</code> class, configure it with the necessary parameters, and fit it to your data. Use the trained model for predictions.</p> <p>Example:     from engines.contentFilterEngine.traditional_ml_algorithms.vw import VW     model = VW()     model.fit(X_train, y_train)     predictions = model.predict(X_test)</p> Note <ul> <li>The current implementation is a placeholder and needs to be completed with actual Vowpal Wabbit logic.</li> <li>Vowpal Wabbit is particularly useful for large datasets and online learning scenarios.</li> </ul>"}]}